{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((148084, 81),\n",
       " (148084,),\n",
       " (16454, 81),\n",
       " (16454,),\n",
       " 0.428162348933,\n",
       " -0.400350158603,\n",
       " 0.425186135849,\n",
       " -0.399102826816)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#导入数据\n",
    "# housedata=fetch_california_housing()\n",
    "#划分测试集和训练集\n",
    "# df_train= pd.read_csv('/home/ysy/ysy/Fed-ReKD-dirs/all.csv')\n",
    "# x = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode'], axis=1).values\n",
    "# y = df_train['logerror'].values\n",
    "# train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.1,random_state=42)\n",
    "#划分测试集和训练集\n",
    "process_dataset_x= pd.read_csv('/home/ysy/ysy/Fed-ReKD-dirs/process_dataset_x.csv',index_col=0)\n",
    "process_dataset_y= pd.read_csv('/home/ysy/ysy/Fed-ReKD-dirs/process_dataset_y.csv',index_col=0)\n",
    "x = process_dataset_x\n",
    "y = process_dataset_y['logerror']\n",
    "train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.1,random_state=42)\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# #标准化处理\n",
    "scale=StandardScaler()\n",
    "train_x=scale.fit_transform(train_x)\n",
    "test_x=scale.fit_transform(test_x)\n",
    "\n",
    "\n",
    "del x,y\n",
    "train_x.shape,train_y.shape,test_x.shape,test_y.shape,max(train_y),min(train_y),max(test_y),min(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=np.array(train_x)\n",
    "train_y=np.array(train_y)\n",
    "test_x=np.array(test_x)\n",
    "test_y=np.array(test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16454 16455 16455\n"
     ]
    }
   ],
   "source": [
    "count=1\n",
    "count1=1\n",
    "for i in range(len(test_y)):\n",
    "    if -1<test_y[i]<1:\n",
    "        count+=1\n",
    "for i in range(len(test_y)):\n",
    "    if -0.5<test_y[i]<0.6:\n",
    "        count1+=1\n",
    "print(len(test_y),count,count1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "def split_list_n_list(origin_list, n):\n",
    "    if len(origin_list) % n == 0:\n",
    "        cnt = len(origin_list) // n\n",
    "    else:\n",
    "        cnt = len(origin_list) // n + 1\n",
    " \n",
    "    for i in range(0, n):\n",
    "        yield origin_list[i*cnt:(i+1)*cnt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (4937, 81) (4937,)\n",
      "1 (4937, 81) (4937,)\n",
      "2 (4937, 81) (4937,)\n",
      "3 (4937, 81) (4937,)\n",
      "4 (4937, 81) (4937,)\n",
      "5 (4937, 81) (4937,)\n",
      "6 (4937, 81) (4937,)\n",
      "7 (4937, 81) (4937,)\n",
      "8 (4937, 81) (4937,)\n",
      "9 (4937, 81) (4937,)\n",
      "10 (4937, 81) (4937,)\n",
      "11 (4937, 81) (4937,)\n",
      "12 (4937, 81) (4937,)\n",
      "13 (4937, 81) (4937,)\n",
      "14 (4937, 81) (4937,)\n",
      "15 (4937, 81) (4937,)\n",
      "16 (4937, 81) (4937,)\n",
      "17 (4937, 81) (4937,)\n",
      "18 (4937, 81) (4937,)\n",
      "19 (4937, 81) (4937,)\n",
      "20 (4937, 81) (4937,)\n",
      "21 (4937, 81) (4937,)\n",
      "22 (4937, 81) (4937,)\n",
      "23 (4937, 81) (4937,)\n",
      "24 (4937, 81) (4937,)\n",
      "25 (4937, 81) (4937,)\n",
      "26 (4937, 81) (4937,)\n",
      "27 (4937, 81) (4937,)\n",
      "28 (4937, 81) (4937,)\n",
      "29 (4911, 81) (4911,)\n"
     ]
    }
   ],
   "source": [
    "#preparing teacher's datasets\n",
    "n_teachers=30\n",
    "\n",
    "teacher_x,teacher_y = [],[]\n",
    "teacher_datasets = []\n",
    "teacher_data_loader = []\n",
    "\n",
    "\n",
    "\n",
    "teacher_x_loder = split_list_n_list(train_x,n_teachers)\n",
    "teacher_y_loder = split_list_n_list(train_y,n_teachers)\n",
    "\n",
    "teacher_x.extend(iter(teacher_x_loder))\n",
    "teacher_y.extend(iter(teacher_y_loder))\n",
    "\n",
    "for i in range(n_teachers):\n",
    "    print(i,teacher_x[i].shape,teacher_y[i].shape)\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "teacher_datasets.extend(TensorDataset(torch.tensor(teacher_x[u],device=device,dtype=torch.float),\n",
    "                                       torch.tensor(teacher_y[u],device=device,dtype=torch.float))\n",
    "                        for u in  range(n_teachers))\n",
    "\n",
    "teacher_data_loader.extend(DataLoader(teacher_datasets[i],batch_size=128,shuffle=True)\n",
    "                           for i in range(n_teachers))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (8227, 81) (8227,)\n",
      "1 (8227, 81) (8227,)\n",
      "8227\n"
     ]
    }
   ],
   "source": [
    "#preparing student's dataset\n",
    "from pickle import TRUE\n",
    "\n",
    "\n",
    "student_x,student_y = [] ,[]\n",
    "student_datasets= []\n",
    " \n",
    "\n",
    "student_x_loder =  split_list_n_list(test_x,2)\n",
    "student_y_loder =  split_list_n_list(test_y,2)\n",
    "student_x.extend(iter(student_x_loder))\n",
    "student_y.extend(iter(student_y_loder))\n",
    "\n",
    "for i in range(2):\n",
    "    print(i,student_x[i].shape,student_y[i].shape)\n",
    "\n",
    "student_datasets.extend(\n",
    "                        TensorDataset(torch.tensor(student_x[u],device=device,dtype=torch.float),\n",
    "                                      torch.tensor(student_y[u],device=device,dtype=torch.float))\n",
    "                        for u in range(2)\n",
    ")\n",
    "\n",
    "student_train_loader = DataLoader(student_datasets[0], batch_size=len(student_datasets[0]),shuffle=True)\n",
    "student_test_loader = DataLoader(student_datasets[1], batch_size=len(student_datasets[1]),shuffle=True)\n",
    "samples= student_x[i].shape[0]\n",
    "print(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#define metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_BBB(\n",
      "  (hidden): Linear_BBB()\n",
      "  (out): Linear_BBB()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Normal\n",
    "from pyro.distributions import Categorical\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI\n",
    "from pyro.infer import Trace_ELBO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "\n",
    "class Linear_BBB(nn.Module):\n",
    "    \"\"\"\n",
    "        Layer of our BNN.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_features, output_features, prior_var=1.):\n",
    "        \"\"\"\n",
    "            Initialization of our layer : our prior is a normal distribution\n",
    "            centered in 0 and of variance 20.\n",
    "        \"\"\"\n",
    "        # initialize layers\n",
    "        super().__init__()\n",
    "        # set input and output dimensions\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        # initialize mu and rho parameters for the weights of the layer\n",
    "        self.w_mu = nn.Parameter(torch.zeros(output_features, input_features))\n",
    "        self.w_rho = nn.Parameter(torch.zeros(output_features, input_features))\n",
    "\n",
    "        #initialize mu and rho parameters for the layer's bias\n",
    "        self.b_mu =  nn.Parameter(torch.zeros(output_features))\n",
    "        self.b_rho = nn.Parameter(torch.zeros(output_features))        \n",
    "\n",
    "        #initialize weight samples (these will be calculated whenever the layer makes a prediction)\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "        # initialize prior distribution for all of the weights and biases\n",
    "        self.prior = torch.distributions.Normal(0,prior_var)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "          Optimization process\n",
    "        \"\"\"\n",
    "        # sample weights\n",
    "        w_epsilon = Normal(0,1).sample(self.w_mu.shape).to(device)\n",
    "        self.w = self.w_mu + torch.log(1+torch.exp(self.w_rho)) * w_epsilon\n",
    "\n",
    "        # sample bias\n",
    "        b_epsilon = Normal(0,1).sample(self.b_mu.shape).to(device)\n",
    "        self.b = self.b_mu + torch.log(1+torch.exp(self.b_rho)) * b_epsilon\n",
    "\n",
    "        # record log prior by evaluating log pdf of prior at sampled weight and bias\n",
    "        w_log_prior = self.prior.log_prob(self.w)\n",
    "        b_log_prior = self.prior.log_prob(self.b)\n",
    "        self.log_prior = torch.sum(w_log_prior) + torch.sum(b_log_prior)\n",
    "\n",
    "        # record log variational posterior by evaluating log pdf of normal distribution defined by parameters with respect at the sampled values\n",
    "        self.w_post = Normal(self.w_mu.data, torch.log(1+torch.exp(self.w_rho)).to(device))\n",
    "        self.b_post = Normal(self.b_mu.data, torch.log(1+torch.exp(self.b_rho)).to(device))\n",
    "        self.log_post = self.w_post.log_prob(self.w).sum() + self.b_post.log_prob(self.b).sum()\n",
    "\n",
    "        return F.linear(input, self.w, self.b)\n",
    "\n",
    "class MLP_BBB(nn.Module):\n",
    "    def __init__(self, hidden_units, noise_tol=.1,  prior_var=1.):\n",
    "\n",
    "        # initialize the network like you would with a standard multilayer perceptron, but using the BBB layer\n",
    "        super().__init__()\n",
    "        self.hidden = Linear_BBB(81,hidden_units, prior_var=prior_var)\n",
    "        self.out = Linear_BBB(hidden_units, 1, prior_var=prior_var)\n",
    "        self.noise_tol = noise_tol # we will use the noise tolerance to calculate our likelihood\n",
    "\n",
    "    def forward(self, x):\n",
    "        # again, this is equivalent to a standard multilayer perceptron\n",
    "        x = torch.sigmoid(self.hidden(x)).to(device)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "    def log_prior(self):\n",
    "        # calculate the log prior over all the layers\n",
    "        return self.hidden.log_prior + self.out.log_prior\n",
    "\n",
    "    def log_post(self):\n",
    "        # calculate the log posterior over all the layers\n",
    "        return self.hidden.log_post + self.out.log_post\n",
    "\n",
    "    def sample_elbo(self, input, target, samples):\n",
    "        # we calculate the negative elbo, which will be our loss function\n",
    "        #initialize tensors\n",
    "        outputs = torch.zeros(samples, target.shape[0]).to(device)\n",
    "        log_priors = torch.zeros(samples).to(device)\n",
    "        log_posts = torch.zeros(samples).to(device)\n",
    "        log_likes = torch.zeros(samples).to(device)\n",
    "        # make predictions and calculate prior, posterior, and likelihood for a given number of samples\n",
    "        for i in range(samples):\n",
    "            outputs[i] = self(input).reshape(-1).to(device) # make predictions\n",
    "            log_priors[i] = self.log_prior() # get log prior\n",
    "            log_posts[i] = self.log_post() # get log variational posterior\n",
    "            log_likes[i] = Normal(outputs[i], self.noise_tol).log_prob(target.reshape(-1)).sum() # calculate the log likelihood\n",
    "        # calculate monte carlo estimate of prior posterior and likelihood\n",
    "        log_prior = log_priors.mean()\n",
    "        log_post = log_posts.mean()\n",
    "        log_like = log_likes.mean()\n",
    "        # calculate the negative elbo (which is our loss function)\n",
    "        loss = log_post - log_prior - log_like\n",
    "        return loss\n",
    "BNN = MLP_BBB(405, prior_var=1).to(device)\n",
    "print(BNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "#training configs\n",
    "num_epochs=1200  #10*3000epoch\n",
    "batch_size =128\n",
    "lr =1e-3\n",
    "# n= 30 600e  0.05\n",
    "#initializing  teachers model\n",
    "teachers_model = []\n",
    "\n",
    "for i in range(n_teachers):\n",
    "    net = BNN\n",
    "    teachers_model.append(net)\n",
    " \n",
    "teacher_optimizers = [torch.optim.Adam(teachers_model[i].parameters(), lr=lr) for i in range(n_teachers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#creating teachers folders\n",
    "def  mkdir_if_missing(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "for i in  range(n_teachers):\n",
    "    mkdir_if_missing(f'/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_44_gauss/teacher{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training number 0 techer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangyipeng/anaconda3/envs/ysy/lib/python3.9/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/wangyipeng/anaconda3/envs/ysy/lib/python3.9/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([73])) that is different to the input size (torch.Size([73, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number:0 epoch: 1/1200 loss_tea:84.71632800953142  loss_houyan:702365.4375\n",
      "Number:0 epoch: 11/1200 loss_tea:81.02520990439314  loss_houyan:189296.90625\n",
      "Number:0 epoch: 21/1200 loss_tea:61.022242098895795  loss_houyan:168064.40625\n",
      "Number:0 epoch: 31/1200 loss_tea:69.45231204296573  loss_houyan:1027040.375\n",
      "Number:0 epoch: 41/1200 loss_tea:67.45395820807754  loss_houyan:125599.1796875\n",
      "Number:0 epoch: 51/1200 loss_tea:49.33490046176057  loss_houyan:90795.484375\n",
      "Number:0 epoch: 61/1200 loss_tea:52.265118376214375  loss_houyan:119894.359375\n",
      "Number:0 epoch: 71/1200 loss_tea:58.91151873747154  loss_houyan:104937.234375\n",
      "Number:0 epoch: 81/1200 loss_tea:37.89518909558657  loss_houyan:121125.078125\n",
      "Number:0 epoch: 91/1200 loss_tea:36.04376152553176  loss_houyan:123577.9140625\n",
      "Number:0 epoch: 101/1200 loss_tea:36.40967997592882  loss_houyan:183863.671875\n",
      "Number:0 epoch: 111/1200 loss_tea:41.163383851680095  loss_houyan:225436.109375\n",
      "Number:0 epoch: 121/1200 loss_tea:28.706429014231045  loss_houyan:263800.4375\n",
      "Number:0 epoch: 131/1200 loss_tea:26.75699155814063  loss_houyan:34738.43359375\n",
      "Number:0 epoch: 141/1200 loss_tea:31.68550435932199  loss_houyan:51544.20703125\n",
      "Number:0 epoch: 151/1200 loss_tea:30.132788051012582  loss_houyan:204774.171875\n",
      "Number:0 epoch: 161/1200 loss_tea:26.541213089863927  loss_houyan:53947.06640625\n",
      "Number:0 epoch: 171/1200 loss_tea:24.952972230530712  loss_houyan:44897.4453125\n",
      "Number:0 epoch: 181/1200 loss_tea:24.629355495121402  loss_houyan:66803.921875\n",
      "Number:0 epoch: 191/1200 loss_tea:17.182928808551097  loss_houyan:24738.681640625\n",
      "Number:0 epoch: 201/1200 loss_tea:17.671132162850178  loss_houyan:64402.0546875\n",
      "Number:0 epoch: 211/1200 loss_tea:19.244267072903618  loss_houyan:82878.4375\n",
      "Number:0 epoch: 221/1200 loss_tea:15.837806934814886  loss_houyan:58313.3046875\n",
      "Number:0 epoch: 231/1200 loss_tea:18.097041694988043  loss_houyan:74901.734375\n",
      "Number:0 epoch: 241/1200 loss_tea:15.44255333939665  loss_houyan:50544.7109375\n",
      "Number:0 epoch: 251/1200 loss_tea:17.487361802005903  loss_houyan:33899.55859375\n",
      "Number:0 epoch: 261/1200 loss_tea:11.74653183131752  loss_houyan:37624.78125\n",
      "Number:0 epoch: 271/1200 loss_tea:15.116527888464352  loss_houyan:31770.087890625\n",
      "Number:0 epoch: 281/1200 loss_tea:9.681296927112303  loss_houyan:12772.8701171875\n",
      "Number:0 epoch: 291/1200 loss_tea:10.703698002376946  loss_houyan:35402.5234375\n",
      "Number:0 epoch: 301/1200 loss_tea:9.756921659627617  loss_houyan:32707.05859375\n",
      "Number:0 epoch: 311/1200 loss_tea:8.728537530244205  loss_houyan:26140.06640625\n",
      "Number:0 epoch: 321/1200 loss_tea:8.523956458385545  loss_houyan:21783.5390625\n",
      "Number:0 epoch: 331/1200 loss_tea:6.436637291233139  loss_houyan:27256.38671875\n",
      "Number:0 epoch: 341/1200 loss_tea:6.225490231153717  loss_houyan:15460.994140625\n",
      "Number:0 epoch: 351/1200 loss_tea:5.480704883295731  loss_houyan:10331.8828125\n",
      "Number:0 epoch: 361/1200 loss_tea:5.912339703911636  loss_houyan:13123.169921875\n",
      "Number:0 epoch: 371/1200 loss_tea:4.991895178249545  loss_houyan:12248.2822265625\n",
      "Number:0 epoch: 381/1200 loss_tea:4.130104335812342  loss_houyan:21345.62890625\n",
      "Number:0 epoch: 391/1200 loss_tea:4.148015995655365  loss_houyan:15839.279296875\n",
      "Number:0 epoch: 401/1200 loss_tea:3.7421571013897035  loss_houyan:26519.767578125\n",
      "Number:0 epoch: 411/1200 loss_tea:4.037311663626658  loss_houyan:10897.7490234375\n",
      "Number:0 epoch: 421/1200 loss_tea:4.029232524004884  loss_houyan:13362.546875\n",
      "Number:0 epoch: 431/1200 loss_tea:3.5522328880554226  loss_houyan:11936.755859375\n",
      "Number:0 epoch: 441/1200 loss_tea:2.7530863480648096  loss_houyan:11902.767578125\n",
      "Number:0 epoch: 451/1200 loss_tea:2.8508944257396616  loss_houyan:10098.74609375\n",
      "Number:0 epoch: 461/1200 loss_tea:2.830801537784459  loss_houyan:7968.53955078125\n",
      "Number:0 epoch: 471/1200 loss_tea:2.7206714380866774  loss_houyan:35797.30859375\n",
      "Number:0 epoch: 481/1200 loss_tea:2.6436016157326825  loss_houyan:16108.19140625\n",
      "Number:0 epoch: 491/1200 loss_tea:1.6519776339083758  loss_houyan:13356.791015625\n",
      "Number:0 epoch: 501/1200 loss_tea:2.126828671709014  loss_houyan:15160.2890625\n",
      "Number:0 epoch: 511/1200 loss_tea:2.0249194537721618  loss_houyan:6325.69921875\n",
      "Number:0 epoch: 521/1200 loss_tea:1.829680530701858  loss_houyan:10227.3046875\n",
      "Number:0 epoch: 531/1200 loss_tea:1.6386070770605967  loss_houyan:5185.06640625\n",
      "Number:0 epoch: 541/1200 loss_tea:1.3611710280186013  loss_houyan:12299.513671875\n",
      "Number:0 epoch: 551/1200 loss_tea:1.0995770474725681  loss_houyan:7835.07568359375\n",
      "Number:0 epoch: 561/1200 loss_tea:1.0733372704664512  loss_houyan:5621.93212890625\n",
      "Number:0 epoch: 571/1200 loss_tea:1.0597553491978744  loss_houyan:6946.1982421875\n",
      "Number:0 epoch: 581/1200 loss_tea:1.2636792821779843  loss_houyan:6522.6923828125\n",
      "Number:0 epoch: 591/1200 loss_tea:1.1632710053342687  loss_houyan:5796.92041015625\n",
      "Number:0 epoch: 601/1200 loss_tea:0.819173390840697  loss_houyan:8516.615234375\n",
      "Number:0 epoch: 611/1200 loss_tea:0.8189479520090539  loss_houyan:5397.5224609375\n",
      "Number:0 epoch: 621/1200 loss_tea:0.9074381889602977  loss_houyan:5070.11328125\n",
      "Number:0 epoch: 631/1200 loss_tea:1.098335918637454  loss_houyan:5819.6962890625\n",
      "Number:0 epoch: 641/1200 loss_tea:0.6686818097703534  loss_houyan:5421.779296875\n",
      "Number:0 epoch: 651/1200 loss_tea:0.5887836396923571  loss_houyan:4165.41796875\n",
      "Number:0 epoch: 661/1200 loss_tea:0.590712498394323  loss_houyan:5445.44482421875\n",
      "Number:0 epoch: 671/1200 loss_tea:0.46432078026532403  loss_houyan:3369.848388671875\n",
      "Number:0 epoch: 681/1200 loss_tea:0.5380477838630205  loss_houyan:5125.51953125\n",
      "Number:0 epoch: 691/1200 loss_tea:0.45053612222453226  loss_houyan:4542.94140625\n",
      "Number:0 epoch: 701/1200 loss_tea:0.5833310670992226  loss_houyan:3602.49072265625\n",
      "Number:0 epoch: 711/1200 loss_tea:0.3860760850294993  loss_houyan:4217.9697265625\n",
      "Number:0 epoch: 721/1200 loss_tea:0.5047630982068668  loss_houyan:2676.959228515625\n",
      "Number:0 epoch: 731/1200 loss_tea:0.370365355519213  loss_houyan:2946.69873046875\n",
      "Number:0 epoch: 741/1200 loss_tea:0.46863540667789244  loss_houyan:2740.462890625\n",
      "Number:0 epoch: 751/1200 loss_tea:0.3813714098599557  loss_houyan:2597.76123046875\n",
      "Number:0 epoch: 761/1200 loss_tea:0.29080298255034154  loss_houyan:3875.56396484375\n",
      "Number:0 epoch: 771/1200 loss_tea:0.31758822522739033  loss_houyan:2505.12060546875\n",
      "Number:0 epoch: 781/1200 loss_tea:0.2588414073062856  loss_houyan:2185.8115234375\n",
      "Number:0 epoch: 791/1200 loss_tea:0.2899520918394912  loss_houyan:2341.46923828125\n",
      "Number:0 epoch: 801/1200 loss_tea:0.28390526526462506  loss_houyan:2681.62060546875\n",
      "Number:0 epoch: 811/1200 loss_tea:0.2218291551804335  loss_houyan:2306.027587890625\n",
      "Number:0 epoch: 821/1200 loss_tea:0.27565835793901605  loss_houyan:2014.2933349609375\n",
      "Number:0 epoch: 831/1200 loss_tea:0.25108581650092915  loss_houyan:2004.56640625\n",
      "Number:0 epoch: 841/1200 loss_tea:0.27694188481838516  loss_houyan:1852.079833984375\n",
      "Number:0 epoch: 851/1200 loss_tea:0.2679660911076654  loss_houyan:1719.320068359375\n",
      "Number:0 epoch: 861/1200 loss_tea:0.3086364535044774  loss_houyan:2536.0986328125\n",
      "Number:0 epoch: 871/1200 loss_tea:0.22133899245736374  loss_houyan:1661.7843017578125\n",
      "Number:0 epoch: 881/1200 loss_tea:0.18538583964954533  loss_houyan:1873.2252197265625\n",
      "Number:0 epoch: 891/1200 loss_tea:0.15266254446013677  loss_houyan:2240.03662109375\n",
      "Number:0 epoch: 901/1200 loss_tea:0.1270702205468569  loss_houyan:1629.8253173828125\n",
      "Number:0 epoch: 911/1200 loss_tea:0.1560140092892458  loss_houyan:1742.2130126953125\n",
      "Number:0 epoch: 921/1200 loss_tea:0.18034033664740456  loss_houyan:1642.6318359375\n",
      "Number:0 epoch: 931/1200 loss_tea:0.12491844670840645  loss_houyan:2915.09228515625\n",
      "Number:0 epoch: 941/1200 loss_tea:0.20410787573747122  loss_houyan:1883.5662841796875\n",
      "Number:0 epoch: 951/1200 loss_tea:0.14239517293292453  loss_houyan:1494.56787109375\n",
      "Number:0 epoch: 961/1200 loss_tea:0.12853041227249715  loss_houyan:1630.8292236328125\n",
      "Number:0 epoch: 971/1200 loss_tea:0.13327129312193126  loss_houyan:1771.2916259765625\n",
      "Number:0 epoch: 981/1200 loss_tea:0.13644866638695974  loss_houyan:1654.1319580078125\n",
      "Number:0 epoch: 991/1200 loss_tea:0.09717742068301045  loss_houyan:1532.3255615234375\n",
      "Number:0 epoch: 1001/1200 loss_tea:0.14136485312362662  loss_houyan:1886.279052734375\n",
      "Number:0 epoch: 1011/1200 loss_tea:0.11173601220845754  loss_houyan:1916.151123046875\n",
      "Number:0 epoch: 1021/1200 loss_tea:0.10644036301909512  loss_houyan:1508.9747314453125\n",
      "Number:0 epoch: 1031/1200 loss_tea:0.10737528595058499  loss_houyan:1651.5635986328125\n",
      "Number:0 epoch: 1041/1200 loss_tea:0.09093440414917767  loss_houyan:1680.0767822265625\n",
      "Number:0 epoch: 1051/1200 loss_tea:0.1199633080526622  loss_houyan:1568.9949951171875\n",
      "Number:0 epoch: 1061/1200 loss_tea:0.08779880734368425  loss_houyan:1583.7230224609375\n",
      "Number:0 epoch: 1071/1200 loss_tea:0.10737015705360245  loss_houyan:1520.5330810546875\n",
      "Number:0 epoch: 1081/1200 loss_tea:0.07822961847514597  loss_houyan:1644.8804931640625\n",
      "Number:0 epoch: 1091/1200 loss_tea:0.12770090664606576  loss_houyan:1445.8973388671875\n",
      "Number:0 epoch: 1101/1200 loss_tea:0.06585382682689897  loss_houyan:1749.25341796875\n",
      "Number:0 epoch: 1111/1200 loss_tea:0.08142197162444133  loss_houyan:1705.3023681640625\n",
      "Number:0 epoch: 1121/1200 loss_tea:0.07990419219085954  loss_houyan:2005.986083984375\n",
      "Number:0 epoch: 1131/1200 loss_tea:0.09936200903571277  loss_houyan:1895.744873046875\n",
      "Number:0 epoch: 1141/1200 loss_tea:0.07344203343788863  loss_houyan:1620.94189453125\n",
      "Number:0 epoch: 1151/1200 loss_tea:0.06992437933919303  loss_houyan:1605.3511962890625\n",
      "Number:0 epoch: 1161/1200 loss_tea:0.07494284824089215  loss_houyan:1619.009033203125\n",
      "Number:0 epoch: 1171/1200 loss_tea:0.0843177527801962  loss_houyan:1737.7734375\n",
      "Number:0 epoch: 1181/1200 loss_tea:0.07672760724804048  loss_houyan:1580.8984375\n",
      "Number:0 epoch: 1191/1200 loss_tea:0.05651877088902727  loss_houyan:1670.560302734375\n",
      "finished training number 0 techer!\n",
      "start training number 1 techer!\n",
      "Number:1 epoch: 1/1200 loss_tea:0.06090958153420381  loss_houyan:1769.11767578125\n",
      "Number:1 epoch: 11/1200 loss_tea:0.08328569759681734  loss_houyan:1509.43359375\n",
      "Number:1 epoch: 21/1200 loss_tea:0.06288100685226318  loss_houyan:1491.116943359375\n",
      "Number:1 epoch: 31/1200 loss_tea:0.056836534571449646  loss_houyan:1508.566650390625\n",
      "Number:1 epoch: 41/1200 loss_tea:0.11327940146975463  loss_houyan:1987.936279296875\n",
      "Number:1 epoch: 51/1200 loss_tea:0.05597250495262177  loss_houyan:1538.0052490234375\n",
      "Number:1 epoch: 61/1200 loss_tea:0.08153567451841078  loss_houyan:1557.2420654296875\n",
      "Number:1 epoch: 71/1200 loss_tea:0.10083042986876621  loss_houyan:1625.902099609375\n",
      "Number:1 epoch: 81/1200 loss_tea:0.06391862440565313  loss_houyan:1538.2772216796875\n",
      "Number:1 epoch: 91/1200 loss_tea:0.05657090980643344  loss_houyan:1583.1922607421875\n",
      "Number:1 epoch: 101/1200 loss_tea:0.06680624055660506  loss_houyan:1598.640625\n",
      "Number:1 epoch: 111/1200 loss_tea:0.06529448980173892  loss_houyan:1579.1776123046875\n",
      "Number:1 epoch: 121/1200 loss_tea:0.05526285199005159  loss_houyan:1568.423095703125\n",
      "Number:1 epoch: 131/1200 loss_tea:0.058973243513256036  loss_houyan:1536.0301513671875\n",
      "Number:1 epoch: 141/1200 loss_tea:0.05988973192455606  loss_houyan:1687.0833740234375\n",
      "Number:1 epoch: 151/1200 loss_tea:0.052650164589829136  loss_houyan:1858.260009765625\n",
      "Number:1 epoch: 161/1200 loss_tea:0.04688022167987613  loss_houyan:1693.051513671875\n",
      "Number:1 epoch: 171/1200 loss_tea:0.058129337962421784  loss_houyan:1749.5972900390625\n",
      "Number:1 epoch: 181/1200 loss_tea:0.0614802481556424  loss_houyan:1557.8536376953125\n",
      "Number:1 epoch: 191/1200 loss_tea:0.05150133186635494  loss_houyan:1592.9368896484375\n",
      "Number:1 epoch: 201/1200 loss_tea:0.053901062395852996  loss_houyan:1893.103515625\n",
      "Number:1 epoch: 211/1200 loss_tea:0.06375983646239483  loss_houyan:2128.57275390625\n",
      "Number:1 epoch: 221/1200 loss_tea:0.05827622322438457  loss_houyan:1528.07275390625\n",
      "Number:1 epoch: 231/1200 loss_tea:0.04927168410148412  loss_houyan:1526.71875\n",
      "Number:1 epoch: 241/1200 loss_tea:0.0517157660421842  loss_houyan:1587.49169921875\n",
      "Number:1 epoch: 251/1200 loss_tea:0.06682737537840612  loss_houyan:1657.839599609375\n",
      "Number:1 epoch: 261/1200 loss_tea:0.04389828356752179  loss_houyan:1641.9140625\n",
      "Number:1 epoch: 271/1200 loss_tea:0.05316246716927753  loss_houyan:1537.48779296875\n",
      "Number:1 epoch: 281/1200 loss_tea:0.06875831888773941  loss_houyan:1987.249267578125\n",
      "Number:1 epoch: 291/1200 loss_tea:0.05930883549674455  loss_houyan:1717.03076171875\n",
      "Number:1 epoch: 301/1200 loss_tea:0.06272150225694396  loss_houyan:1601.1630859375\n",
      "Number:1 epoch: 311/1200 loss_tea:0.052815554286633426  loss_houyan:1649.9366455078125\n",
      "Number:1 epoch: 321/1200 loss_tea:0.06413411954823403  loss_houyan:1946.419921875\n",
      "Number:1 epoch: 331/1200 loss_tea:0.049881626216323846  loss_houyan:1627.2874755859375\n",
      "Number:1 epoch: 341/1200 loss_tea:0.044626285389732775  loss_houyan:1594.560546875\n",
      "Number:1 epoch: 351/1200 loss_tea:0.05596437724448661  loss_houyan:1504.5079345703125\n",
      "Number:1 epoch: 361/1200 loss_tea:0.046852369729941834  loss_houyan:1539.6805419921875\n",
      "Number:1 epoch: 371/1200 loss_tea:0.07986204193198346  loss_houyan:1722.94189453125\n",
      "Number:1 epoch: 381/1200 loss_tea:0.05704028600876283  loss_houyan:1534.238525390625\n",
      "Number:1 epoch: 391/1200 loss_tea:0.055324145254515095  loss_houyan:1745.9984130859375\n",
      "Number:1 epoch: 401/1200 loss_tea:0.054121755179736886  loss_houyan:1530.606689453125\n",
      "Number:1 epoch: 411/1200 loss_tea:0.0651802118186263  loss_houyan:1501.6761474609375\n",
      "Number:1 epoch: 421/1200 loss_tea:0.06847524285204791  loss_houyan:1542.992919921875\n",
      "Number:1 epoch: 431/1200 loss_tea:0.044788673110359084  loss_houyan:1683.57861328125\n",
      "Number:1 epoch: 441/1200 loss_tea:0.044611195290598135  loss_houyan:1536.5186767578125\n",
      "Number:1 epoch: 451/1200 loss_tea:0.05221803146868141  loss_houyan:1537.5987548828125\n",
      "Number:1 epoch: 461/1200 loss_tea:0.04797280833339411  loss_houyan:1835.392578125\n",
      "Number:1 epoch: 471/1200 loss_tea:0.04106734349723043  loss_houyan:1685.35400390625\n",
      "Number:1 epoch: 481/1200 loss_tea:0.06641537150308106  loss_houyan:2208.58447265625\n",
      "Number:1 epoch: 491/1200 loss_tea:0.0594080974390711  loss_houyan:2242.95556640625\n",
      "Number:1 epoch: 501/1200 loss_tea:0.06293058277371684  loss_houyan:1895.5491943359375\n",
      "Number:1 epoch: 511/1200 loss_tea:0.05811140532191282  loss_houyan:1531.267822265625\n",
      "Number:1 epoch: 521/1200 loss_tea:0.04430839119047826  loss_houyan:1601.3519287109375\n",
      "Number:1 epoch: 531/1200 loss_tea:0.0541975033888848  loss_houyan:1630.7552490234375\n",
      "Number:1 epoch: 541/1200 loss_tea:0.05496729009196008  loss_houyan:1527.967041015625\n",
      "Number:1 epoch: 551/1200 loss_tea:0.05140012186431851  loss_houyan:1535.1397705078125\n",
      "Number:1 epoch: 561/1200 loss_tea:0.05640829716565181  loss_houyan:1777.0252685546875\n",
      "Number:1 epoch: 571/1200 loss_tea:0.04953865117453988  loss_houyan:1854.9254150390625\n",
      "Number:1 epoch: 581/1200 loss_tea:0.04490584284648169  loss_houyan:1521.01416015625\n",
      "Number:1 epoch: 591/1200 loss_tea:0.0563296757552434  loss_houyan:1669.6365966796875\n",
      "Number:1 epoch: 601/1200 loss_tea:0.06933001587754058  loss_houyan:1543.411865234375\n",
      "Number:1 epoch: 611/1200 loss_tea:0.05022132288870576  loss_houyan:1732.18017578125\n",
      "Number:1 epoch: 621/1200 loss_tea:0.05765635879581663  loss_houyan:1599.2108154296875\n",
      "Number:1 epoch: 631/1200 loss_tea:0.06223419461839229  loss_houyan:1891.106689453125\n",
      "Number:1 epoch: 641/1200 loss_tea:0.06778402880350298  loss_houyan:2035.1119384765625\n",
      "Number:1 epoch: 651/1200 loss_tea:0.05548757407229476  loss_houyan:1586.2274169921875\n",
      "Number:1 epoch: 661/1200 loss_tea:0.054395337046363804  loss_houyan:1536.186279296875\n",
      "Number:1 epoch: 671/1200 loss_tea:0.06353883563012869  loss_houyan:1649.718505859375\n",
      "Number:1 epoch: 681/1200 loss_tea:0.054757035994049214  loss_houyan:1550.28173828125\n",
      "Number:1 epoch: 691/1200 loss_tea:0.06334521756023591  loss_houyan:1580.694091796875\n",
      "Number:1 epoch: 701/1200 loss_tea:0.04659348098624928  loss_houyan:1528.4664306640625\n",
      "Number:1 epoch: 711/1200 loss_tea:0.048444103714435575  loss_houyan:1582.6640625\n",
      "Number:1 epoch: 721/1200 loss_tea:0.04173479213064104  loss_houyan:1534.52392578125\n",
      "Number:1 epoch: 731/1200 loss_tea:0.061249587880302256  loss_houyan:1716.0098876953125\n",
      "Number:1 epoch: 741/1200 loss_tea:0.050706409364229446  loss_houyan:1665.6834716796875\n",
      "Number:1 epoch: 751/1200 loss_tea:0.06120231866999534  loss_houyan:1529.556640625\n",
      "Number:1 epoch: 761/1200 loss_tea:0.04724748802977051  loss_houyan:1540.4769287109375\n",
      "Number:1 epoch: 771/1200 loss_tea:0.04477138342537666  loss_houyan:1545.6773681640625\n",
      "Number:1 epoch: 781/1200 loss_tea:0.05249859129148672  loss_houyan:1649.1800537109375\n",
      "Number:1 epoch: 791/1200 loss_tea:0.06515313293660353  loss_houyan:1633.267578125\n",
      "Number:1 epoch: 801/1200 loss_tea:0.03771536551070329  loss_houyan:1503.5609130859375\n",
      "Number:1 epoch: 811/1200 loss_tea:0.041022952340418015  loss_houyan:2258.54541015625\n",
      "Number:1 epoch: 821/1200 loss_tea:0.05091730616186912  loss_houyan:1797.400634765625\n",
      "Number:1 epoch: 831/1200 loss_tea:0.046651419984840396  loss_houyan:1599.719970703125\n",
      "Number:1 epoch: 841/1200 loss_tea:0.057458913404587336  loss_houyan:1671.9779052734375\n",
      "Number:1 epoch: 851/1200 loss_tea:0.05198770737645596  loss_houyan:1534.524658203125\n",
      "Number:1 epoch: 861/1200 loss_tea:0.03705989272277655  loss_houyan:1573.250732421875\n",
      "Number:1 epoch: 871/1200 loss_tea:0.055097784113583355  loss_houyan:1746.1878662109375\n",
      "Number:1 epoch: 881/1200 loss_tea:0.056279000938421614  loss_houyan:1663.625\n",
      "Number:1 epoch: 891/1200 loss_tea:0.06736520020578321  loss_houyan:1573.0347900390625\n",
      "Number:1 epoch: 901/1200 loss_tea:0.058029301260447655  loss_houyan:1568.141845703125\n",
      "Number:1 epoch: 911/1200 loss_tea:0.07868547679496869  loss_houyan:1823.0848388671875\n",
      "Number:1 epoch: 921/1200 loss_tea:0.04792422554135951  loss_houyan:1529.345458984375\n",
      "Number:1 epoch: 931/1200 loss_tea:0.05639341910435153  loss_houyan:2043.30322265625\n",
      "Number:1 epoch: 941/1200 loss_tea:0.05418347136613833  loss_houyan:1531.0657958984375\n",
      "Number:1 epoch: 951/1200 loss_tea:0.07205064825642706  loss_houyan:1798.136962890625\n",
      "Number:1 epoch: 961/1200 loss_tea:0.055720012797601484  loss_houyan:1612.248291015625\n",
      "Number:1 epoch: 971/1200 loss_tea:0.06741148048771498  loss_houyan:1798.0677490234375\n",
      "Number:1 epoch: 981/1200 loss_tea:0.05116537801358462  loss_houyan:1618.2127685546875\n",
      "Number:1 epoch: 991/1200 loss_tea:0.051657644216501694  loss_houyan:1558.4166259765625\n",
      "Number:1 epoch: 1001/1200 loss_tea:0.06252067599951895  loss_houyan:1687.8963623046875\n",
      "Number:1 epoch: 1011/1200 loss_tea:0.05563598388912756  loss_houyan:1571.9306640625\n",
      "Number:1 epoch: 1021/1200 loss_tea:0.04921600781305674  loss_houyan:1550.795654296875\n",
      "Number:1 epoch: 1031/1200 loss_tea:0.04849523143359517  loss_houyan:1607.3094482421875\n",
      "Number:1 epoch: 1041/1200 loss_tea:0.04188288069758938  loss_houyan:1546.864501953125\n",
      "Number:1 epoch: 1051/1200 loss_tea:0.05033881638402069  loss_houyan:1523.560546875\n",
      "Number:1 epoch: 1061/1200 loss_tea:0.0523248339478958  loss_houyan:1815.28369140625\n",
      "Number:1 epoch: 1071/1200 loss_tea:0.056974053218378236  loss_houyan:1551.4459228515625\n",
      "Number:1 epoch: 1081/1200 loss_tea:0.05814287127353099  loss_houyan:1524.6790771484375\n",
      "Number:1 epoch: 1091/1200 loss_tea:0.04413227818292091  loss_houyan:1592.641845703125\n",
      "Number:1 epoch: 1101/1200 loss_tea:0.05485688181318975  loss_houyan:1554.6732177734375\n",
      "Number:1 epoch: 1111/1200 loss_tea:0.0563470343347048  loss_houyan:1543.2589111328125\n",
      "Number:1 epoch: 1121/1200 loss_tea:0.06290431169791878  loss_houyan:1543.898681640625\n",
      "Number:1 epoch: 1131/1200 loss_tea:0.05880983479748087  loss_houyan:1673.0103759765625\n",
      "Number:1 epoch: 1141/1200 loss_tea:0.06879978410406153  loss_houyan:2047.109375\n",
      "Number:1 epoch: 1151/1200 loss_tea:0.05176209440767608  loss_houyan:2017.0653076171875\n",
      "Number:1 epoch: 1161/1200 loss_tea:0.046949178864207915  loss_houyan:1612.81640625\n",
      "Number:1 epoch: 1171/1200 loss_tea:0.056664947059676306  loss_houyan:1632.7083740234375\n",
      "Number:1 epoch: 1181/1200 loss_tea:0.06767837041021238  loss_houyan:1539.7891845703125\n",
      "Number:1 epoch: 1191/1200 loss_tea:0.04105253247568693  loss_houyan:1626.5450439453125\n",
      "finished training number 1 techer!\n",
      "start training number 2 techer!\n",
      "Number:2 epoch: 1/1200 loss_tea:0.054621558529278615  loss_houyan:1607.5377197265625\n",
      "Number:2 epoch: 11/1200 loss_tea:0.05594462794356436  loss_houyan:1536.33349609375\n",
      "Number:2 epoch: 21/1200 loss_tea:0.06095210641829294  loss_houyan:1579.0404052734375\n",
      "Number:2 epoch: 31/1200 loss_tea:0.07484900214929487  loss_houyan:2262.508544921875\n",
      "Number:2 epoch: 41/1200 loss_tea:0.057708982180548214  loss_houyan:1556.8089599609375\n",
      "Number:2 epoch: 51/1200 loss_tea:0.06593787659118841  loss_houyan:1522.557373046875\n",
      "Number:2 epoch: 61/1200 loss_tea:0.07318867170450923  loss_houyan:1539.501708984375\n",
      "Number:2 epoch: 71/1200 loss_tea:0.049666210202394964  loss_houyan:1715.7713623046875\n",
      "Number:2 epoch: 81/1200 loss_tea:0.050299713705380195  loss_houyan:1670.642822265625\n",
      "Number:2 epoch: 91/1200 loss_tea:0.04332436823488151  loss_houyan:1546.3450927734375\n",
      "Number:2 epoch: 101/1200 loss_tea:0.06173030578138762  loss_houyan:1584.163330078125\n",
      "Number:2 epoch: 111/1200 loss_tea:0.0814425181646518  loss_houyan:1559.7030029296875\n",
      "Number:2 epoch: 121/1200 loss_tea:0.03577987165472298  loss_houyan:1618.372314453125\n",
      "Number:2 epoch: 131/1200 loss_tea:0.04435297758585412  loss_houyan:1679.465576171875\n",
      "Number:2 epoch: 141/1200 loss_tea:0.07253444226372271  loss_houyan:1955.557861328125\n",
      "Number:2 epoch: 151/1200 loss_tea:0.05420826099917114  loss_houyan:1636.7401123046875\n",
      "Number:2 epoch: 161/1200 loss_tea:0.06953610026639605  loss_houyan:1663.304443359375\n",
      "Number:2 epoch: 171/1200 loss_tea:0.06538311199352813  loss_houyan:2005.4052734375\n",
      "Number:2 epoch: 181/1200 loss_tea:0.053539436516526116  loss_houyan:1702.875\n",
      "Number:2 epoch: 191/1200 loss_tea:0.04881776859261305  loss_houyan:1587.8538818359375\n",
      "Number:2 epoch: 201/1200 loss_tea:0.05920132052727573  loss_houyan:1758.187255859375\n",
      "Number:2 epoch: 211/1200 loss_tea:0.049298186919842266  loss_houyan:1577.386962890625\n",
      "Number:2 epoch: 221/1200 loss_tea:0.04841471521618856  loss_houyan:1601.205078125\n",
      "Number:2 epoch: 231/1200 loss_tea:0.05502352524599325  loss_houyan:1562.164306640625\n",
      "Number:2 epoch: 241/1200 loss_tea:0.05183621501244102  loss_houyan:1506.8319091796875\n",
      "Number:2 epoch: 251/1200 loss_tea:0.05149475098296122  loss_houyan:1527.7880859375\n",
      "Number:2 epoch: 261/1200 loss_tea:0.05157760960963517  loss_houyan:1643.9246826171875\n",
      "Number:2 epoch: 271/1200 loss_tea:0.05666552647813952  loss_houyan:1549.1517333984375\n",
      "Number:2 epoch: 281/1200 loss_tea:0.046970999026050425  loss_houyan:1578.1595458984375\n",
      "Number:2 epoch: 291/1200 loss_tea:0.056157934830477055  loss_houyan:2171.93505859375\n",
      "Number:2 epoch: 301/1200 loss_tea:0.059438588510816195  loss_houyan:2004.874755859375\n",
      "Number:2 epoch: 311/1200 loss_tea:0.06827325486695571  loss_houyan:1532.8499755859375\n",
      "Number:2 epoch: 321/1200 loss_tea:0.06252575011730363  loss_houyan:1624.4471435546875\n",
      "Number:2 epoch: 331/1200 loss_tea:0.06378509272218161  loss_houyan:1797.7777099609375\n",
      "Number:2 epoch: 341/1200 loss_tea:0.07589033160925346  loss_houyan:1573.1534423828125\n",
      "Number:2 epoch: 351/1200 loss_tea:0.046762624356713835  loss_houyan:1583.800537109375\n",
      "Number:2 epoch: 361/1200 loss_tea:0.05370561299433961  loss_houyan:1522.905029296875\n",
      "Number:2 epoch: 371/1200 loss_tea:0.073199314664214  loss_houyan:1645.86328125\n",
      "Number:2 epoch: 381/1200 loss_tea:0.06501077439583927  loss_houyan:2490.739013671875\n",
      "Number:2 epoch: 391/1200 loss_tea:0.04500671720189956  loss_houyan:1599.3148193359375\n",
      "Number:2 epoch: 401/1200 loss_tea:0.04739199298214569  loss_houyan:1598.28857421875\n",
      "Number:2 epoch: 411/1200 loss_tea:0.05871015502804321  loss_houyan:1582.39697265625\n",
      "Number:2 epoch: 421/1200 loss_tea:0.046444423176410086  loss_houyan:1526.0885009765625\n",
      "Number:2 epoch: 431/1200 loss_tea:0.043701425994579816  loss_houyan:1790.3408203125\n",
      "Number:2 epoch: 441/1200 loss_tea:0.05973669524208928  loss_houyan:1561.847900390625\n",
      "Number:2 epoch: 451/1200 loss_tea:0.04519317062910822  loss_houyan:1566.557373046875\n",
      "Number:2 epoch: 461/1200 loss_tea:0.06727928114560347  loss_houyan:1835.156494140625\n",
      "Number:2 epoch: 471/1200 loss_tea:0.05607536137451134  loss_houyan:1600.9808349609375\n",
      "Number:2 epoch: 481/1200 loss_tea:0.054932522196047266  loss_houyan:1521.62060546875\n",
      "Number:2 epoch: 491/1200 loss_tea:0.04444877161756264  loss_houyan:1695.36376953125\n",
      "Number:2 epoch: 501/1200 loss_tea:0.05551306173923722  loss_houyan:1526.3316650390625\n",
      "Number:2 epoch: 511/1200 loss_tea:0.06693964829211513  loss_houyan:1527.75\n",
      "Number:2 epoch: 521/1200 loss_tea:0.058097779233946545  loss_houyan:1560.3675537109375\n",
      "Number:2 epoch: 531/1200 loss_tea:0.07210607385967989  loss_houyan:1588.3011474609375\n",
      "Number:2 epoch: 541/1200 loss_tea:0.059318496153914595  loss_houyan:1586.586669921875\n",
      "Number:2 epoch: 551/1200 loss_tea:0.05897175669458827  loss_houyan:1577.0350341796875\n",
      "Number:2 epoch: 561/1200 loss_tea:0.05619825599633998  loss_houyan:1845.003662109375\n",
      "Number:2 epoch: 571/1200 loss_tea:0.04308077311326261  loss_houyan:2107.3125\n",
      "Number:2 epoch: 581/1200 loss_tea:0.05221454520438735  loss_houyan:1594.467041015625\n",
      "Number:2 epoch: 591/1200 loss_tea:0.06113060494080859  loss_houyan:1544.924072265625\n",
      "Number:2 epoch: 601/1200 loss_tea:0.0610363383403741  loss_houyan:2285.835205078125\n",
      "Number:2 epoch: 611/1200 loss_tea:0.05931031877800886  loss_houyan:1647.2783203125\n",
      "Number:2 epoch: 621/1200 loss_tea:0.05605666255338824  loss_houyan:1717.2965087890625\n",
      "Number:2 epoch: 631/1200 loss_tea:0.05554955467168247  loss_houyan:1533.7879638671875\n",
      "Number:2 epoch: 641/1200 loss_tea:0.05176359707523943  loss_houyan:1580.7716064453125\n",
      "Number:2 epoch: 651/1200 loss_tea:0.05307605600255039  loss_houyan:1559.7027587890625\n",
      "Number:2 epoch: 661/1200 loss_tea:0.042479291110808295  loss_houyan:1946.53662109375\n",
      "Number:2 epoch: 671/1200 loss_tea:0.053808160480855086  loss_houyan:1645.515625\n",
      "Number:2 epoch: 681/1200 loss_tea:0.06378421207434534  loss_houyan:1887.5792236328125\n",
      "Number:2 epoch: 691/1200 loss_tea:0.06492423611310708  loss_houyan:2059.11083984375\n",
      "Number:2 epoch: 701/1200 loss_tea:0.040805736609091516  loss_houyan:1570.729248046875\n",
      "Number:2 epoch: 711/1200 loss_tea:0.07380488263629867  loss_houyan:1614.5531005859375\n",
      "Number:2 epoch: 721/1200 loss_tea:0.04369126472917191  loss_houyan:1613.214599609375\n",
      "Number:2 epoch: 731/1200 loss_tea:0.07816134537299478  loss_houyan:1654.41845703125\n",
      "Number:2 epoch: 741/1200 loss_tea:0.05903053486141353  loss_houyan:1764.31201171875\n",
      "Number:2 epoch: 751/1200 loss_tea:0.0375464156527334  loss_houyan:1533.956298828125\n",
      "Number:2 epoch: 761/1200 loss_tea:0.04862457871907167  loss_houyan:1748.6962890625\n",
      "Number:2 epoch: 771/1200 loss_tea:0.060531471910078516  loss_houyan:1604.054443359375\n",
      "Number:2 epoch: 781/1200 loss_tea:0.050048480152175306  loss_houyan:1572.5263671875\n",
      "Number:2 epoch: 791/1200 loss_tea:0.0811444759121399  loss_houyan:1622.3662109375\n",
      "Number:2 epoch: 801/1200 loss_tea:0.04659944156504351  loss_houyan:1587.136474609375\n",
      "Number:2 epoch: 811/1200 loss_tea:0.06577820671997332  loss_houyan:1667.580322265625\n",
      "Number:2 epoch: 821/1200 loss_tea:0.045806728612804354  loss_houyan:2361.03271484375\n",
      "Number:2 epoch: 831/1200 loss_tea:0.044987286359346394  loss_houyan:1505.592041015625\n",
      "Number:2 epoch: 841/1200 loss_tea:0.050266403405319134  loss_houyan:1538.01513671875\n",
      "Number:2 epoch: 851/1200 loss_tea:0.050027876922584195  loss_houyan:1554.2457275390625\n",
      "Number:2 epoch: 861/1200 loss_tea:0.053745937572725395  loss_houyan:1601.55712890625\n",
      "Number:2 epoch: 871/1200 loss_tea:0.07936459899564395  loss_houyan:1682.628173828125\n",
      "Number:2 epoch: 881/1200 loss_tea:0.04852701792090147  loss_houyan:1624.7822265625\n",
      "Number:2 epoch: 891/1200 loss_tea:0.03905499785201364  loss_houyan:1558.347900390625\n",
      "Number:2 epoch: 901/1200 loss_tea:0.04840661195409197  loss_houyan:1742.237060546875\n",
      "Number:2 epoch: 911/1200 loss_tea:0.05833087507043021  loss_houyan:1491.2442626953125\n",
      "Number:2 epoch: 921/1200 loss_tea:0.04813662806708054  loss_houyan:2021.8765869140625\n",
      "Number:2 epoch: 931/1200 loss_tea:0.03905128050653624  loss_houyan:1637.952392578125\n",
      "Number:2 epoch: 941/1200 loss_tea:0.061423907780456526  loss_houyan:1774.918701171875\n",
      "Number:2 epoch: 951/1200 loss_tea:0.06299188764566276  loss_houyan:1583.124267578125\n",
      "Number:2 epoch: 961/1200 loss_tea:0.06799957062313376  loss_houyan:1979.44287109375\n",
      "Number:2 epoch: 971/1200 loss_tea:0.04713771049312814  loss_houyan:1584.1826171875\n",
      "Number:2 epoch: 981/1200 loss_tea:0.05379812454879103  loss_houyan:1765.5615234375\n",
      "Number:2 epoch: 991/1200 loss_tea:0.040779134183548615  loss_houyan:1622.55126953125\n",
      "Number:2 epoch: 1001/1200 loss_tea:0.053504206187357566  loss_houyan:1683.5802001953125\n",
      "Number:2 epoch: 1011/1200 loss_tea:0.04380289619648556  loss_houyan:1725.2728271484375\n",
      "Number:2 epoch: 1021/1200 loss_tea:0.056033343532463545  loss_houyan:1558.0958251953125\n",
      "Number:2 epoch: 1031/1200 loss_tea:0.03996961762729096  loss_houyan:1565.3349609375\n",
      "Number:2 epoch: 1041/1200 loss_tea:0.05460094974118011  loss_houyan:1554.9365234375\n",
      "Number:2 epoch: 1051/1200 loss_tea:0.040408766962978696  loss_houyan:1643.5594482421875\n",
      "Number:2 epoch: 1061/1200 loss_tea:0.0687090903150847  loss_houyan:1967.0555419921875\n",
      "Number:2 epoch: 1071/1200 loss_tea:0.04304911163445967  loss_houyan:1570.571533203125\n",
      "Number:2 epoch: 1081/1200 loss_tea:0.04519518993490074  loss_houyan:1538.8441162109375\n",
      "Number:2 epoch: 1091/1200 loss_tea:0.062249425952208656  loss_houyan:1527.0667724609375\n",
      "Number:2 epoch: 1101/1200 loss_tea:0.06920091832629734  loss_houyan:1610.26513671875\n",
      "Number:2 epoch: 1111/1200 loss_tea:0.0682800848762142  loss_houyan:1670.2069091796875\n",
      "Number:2 epoch: 1121/1200 loss_tea:0.05823286503088578  loss_houyan:1536.717529296875\n",
      "Number:2 epoch: 1131/1200 loss_tea:0.04820226700076671  loss_houyan:1689.70263671875\n",
      "Number:2 epoch: 1141/1200 loss_tea:0.0554282520134632  loss_houyan:1510.592529296875\n",
      "Number:2 epoch: 1151/1200 loss_tea:0.05936007783668037  loss_houyan:1775.887451171875\n",
      "Number:2 epoch: 1161/1200 loss_tea:0.05823483029393645  loss_houyan:1655.612548828125\n",
      "Number:2 epoch: 1171/1200 loss_tea:0.05087542510596164  loss_houyan:1766.1064453125\n",
      "Number:2 epoch: 1181/1200 loss_tea:0.057138291671826866  loss_houyan:1557.6700439453125\n",
      "Number:2 epoch: 1191/1200 loss_tea:0.048369926622136294  loss_houyan:1604.5574951171875\n",
      "finished training number 2 techer!\n",
      "start training number 3 techer!\n",
      "Number:3 epoch: 1/1200 loss_tea:0.055655008310017094  loss_houyan:1536.08447265625\n",
      "Number:3 epoch: 11/1200 loss_tea:0.052247680410145145  loss_houyan:1833.4661865234375\n",
      "Number:3 epoch: 21/1200 loss_tea:0.06101115587596978  loss_houyan:1925.189697265625\n",
      "Number:3 epoch: 31/1200 loss_tea:0.05146109229393254  loss_houyan:1583.3170166015625\n",
      "Number:3 epoch: 41/1200 loss_tea:0.047411969125479105  loss_houyan:1647.9700927734375\n",
      "Number:3 epoch: 51/1200 loss_tea:0.04083817209071275  loss_houyan:1929.10107421875\n",
      "Number:3 epoch: 61/1200 loss_tea:0.044134391961079364  loss_houyan:1539.3160400390625\n",
      "Number:3 epoch: 71/1200 loss_tea:0.04964607860836419  loss_houyan:1566.218994140625\n",
      "Number:3 epoch: 81/1200 loss_tea:0.04926927654000581  loss_houyan:1539.96826171875\n",
      "Number:3 epoch: 91/1200 loss_tea:0.047624350888950756  loss_houyan:1636.1798095703125\n",
      "Number:3 epoch: 101/1200 loss_tea:0.05442704296077954  loss_houyan:1774.5084228515625\n",
      "Number:3 epoch: 111/1200 loss_tea:0.04101810160422943  loss_houyan:1735.41552734375\n",
      "Number:3 epoch: 121/1200 loss_tea:0.07827982225440064  loss_houyan:1680.925048828125\n",
      "Number:3 epoch: 131/1200 loss_tea:0.051349602901886685  loss_houyan:1597.8038330078125\n",
      "Number:3 epoch: 141/1200 loss_tea:0.04320165801039526  loss_houyan:1667.349853515625\n",
      "Number:3 epoch: 151/1200 loss_tea:0.042369712762125644  loss_houyan:1565.3641357421875\n",
      "Number:3 epoch: 161/1200 loss_tea:0.047876519325525266  loss_houyan:1504.0225830078125\n",
      "Number:3 epoch: 171/1200 loss_tea:0.040826275181466684  loss_houyan:1568.3623046875\n",
      "Number:3 epoch: 181/1200 loss_tea:0.06997617130206077  loss_houyan:1520.964111328125\n",
      "Number:3 epoch: 191/1200 loss_tea:0.05936409412029243  loss_houyan:1534.84619140625\n",
      "Number:3 epoch: 201/1200 loss_tea:0.05094013566931105  loss_houyan:1885.820068359375\n",
      "Number:3 epoch: 211/1200 loss_tea:0.06422865448049116  loss_houyan:1548.9683837890625\n",
      "Number:3 epoch: 221/1200 loss_tea:0.06853887809567638  loss_houyan:1609.7257080078125\n",
      "Number:3 epoch: 231/1200 loss_tea:0.04100144812820265  loss_houyan:1644.766845703125\n",
      "Number:3 epoch: 241/1200 loss_tea:0.061723722950837404  loss_houyan:1707.591552734375\n",
      "Number:3 epoch: 251/1200 loss_tea:0.057177861325076074  loss_houyan:1706.099609375\n",
      "Number:3 epoch: 261/1200 loss_tea:0.054833927393451574  loss_houyan:1784.214111328125\n",
      "Number:3 epoch: 271/1200 loss_tea:0.05203361352213317  loss_houyan:2216.7294921875\n",
      "Number:3 epoch: 281/1200 loss_tea:0.05861916228923596  loss_houyan:1538.5296630859375\n",
      "Number:3 epoch: 291/1200 loss_tea:0.06629022226606832  loss_houyan:1562.453857421875\n",
      "Number:3 epoch: 301/1200 loss_tea:0.05416883430214505  loss_houyan:1522.9781494140625\n",
      "Number:3 epoch: 311/1200 loss_tea:0.07127936328301938  loss_houyan:1583.781494140625\n",
      "Number:3 epoch: 321/1200 loss_tea:0.05429040970639563  loss_houyan:1620.623779296875\n",
      "Number:3 epoch: 331/1200 loss_tea:0.07995766659993704  loss_houyan:1550.3970947265625\n",
      "Number:3 epoch: 341/1200 loss_tea:0.08653684086299314  loss_houyan:1562.0556640625\n",
      "Number:3 epoch: 351/1200 loss_tea:0.05427295328430109  loss_houyan:1568.1051025390625\n",
      "Number:3 epoch: 361/1200 loss_tea:0.056732328759922485  loss_houyan:1779.978271484375\n",
      "Number:3 epoch: 371/1200 loss_tea:0.05822958203901495  loss_houyan:1563.1851806640625\n",
      "Number:3 epoch: 381/1200 loss_tea:0.05218201553034401  loss_houyan:1765.3231201171875\n",
      "Number:3 epoch: 391/1200 loss_tea:0.057756230301501565  loss_houyan:1701.5850830078125\n",
      "Number:3 epoch: 401/1200 loss_tea:0.06167086675168532  loss_houyan:1774.7470703125\n",
      "Number:3 epoch: 411/1200 loss_tea:0.04963159584699673  loss_houyan:1554.05712890625\n",
      "Number:3 epoch: 421/1200 loss_tea:0.04296509274133478  loss_houyan:1587.417724609375\n",
      "Number:3 epoch: 431/1200 loss_tea:0.049727205459480464  loss_houyan:1553.6650390625\n",
      "Number:3 epoch: 441/1200 loss_tea:0.04555621999603596  loss_houyan:1540.0654296875\n",
      "Number:3 epoch: 451/1200 loss_tea:0.05611981151496106  loss_houyan:1555.427734375\n",
      "Number:3 epoch: 461/1200 loss_tea:0.05516656836714005  loss_houyan:1591.450439453125\n",
      "Number:3 epoch: 471/1200 loss_tea:0.03892837494312932  loss_houyan:1560.8211669921875\n",
      "Number:3 epoch: 481/1200 loss_tea:0.055268775281610096  loss_houyan:1539.60888671875\n",
      "Number:3 epoch: 491/1200 loss_tea:0.0663307054628178  loss_houyan:1603.31005859375\n",
      "Number:3 epoch: 501/1200 loss_tea:0.06615608851471567  loss_houyan:1511.461181640625\n",
      "Number:3 epoch: 511/1200 loss_tea:0.046621323128340095  loss_houyan:1769.2451171875\n",
      "Number:3 epoch: 521/1200 loss_tea:0.05819275073672394  loss_houyan:1558.5408935546875\n",
      "Number:3 epoch: 531/1200 loss_tea:0.05504420297686294  loss_houyan:1532.1669921875\n",
      "Number:3 epoch: 541/1200 loss_tea:0.05074443152909678  loss_houyan:1576.012939453125\n",
      "Number:3 epoch: 551/1200 loss_tea:0.04165925529304607  loss_houyan:1531.573486328125\n",
      "Number:3 epoch: 561/1200 loss_tea:0.05038869215399238  loss_houyan:1600.57470703125\n",
      "Number:3 epoch: 571/1200 loss_tea:0.06040789985209432  loss_houyan:1530.2305908203125\n",
      "Number:3 epoch: 581/1200 loss_tea:0.05065085538748802  loss_houyan:1525.1026611328125\n",
      "Number:3 epoch: 591/1200 loss_tea:0.05593724062385976  loss_houyan:1548.2093505859375\n",
      "Number:3 epoch: 601/1200 loss_tea:0.06894934948413797  loss_houyan:1730.58642578125\n",
      "Number:3 epoch: 611/1200 loss_tea:0.04073531340286272  loss_houyan:1816.09521484375\n",
      "Number:3 epoch: 621/1200 loss_tea:0.05632521999565628  loss_houyan:1644.4720458984375\n",
      "Number:3 epoch: 631/1200 loss_tea:0.0559425826803384  loss_houyan:1590.7425537109375\n",
      "Number:3 epoch: 641/1200 loss_tea:0.06814744127531368  loss_houyan:1881.3421630859375\n",
      "Number:3 epoch: 651/1200 loss_tea:0.045422782278616874  loss_houyan:1584.6463623046875\n",
      "Number:3 epoch: 661/1200 loss_tea:0.06517780761119202  loss_houyan:1712.092041015625\n",
      "Number:3 epoch: 671/1200 loss_tea:0.05511445827828937  loss_houyan:1970.13818359375\n",
      "Number:3 epoch: 681/1200 loss_tea:0.055806632825064595  loss_houyan:1557.422607421875\n",
      "Number:3 epoch: 691/1200 loss_tea:0.06064506515736809  loss_houyan:1718.720703125\n",
      "Number:3 epoch: 701/1200 loss_tea:0.04586984201924676  loss_houyan:1510.7943115234375\n",
      "Number:3 epoch: 711/1200 loss_tea:0.04936682757504054  loss_houyan:1602.518310546875\n",
      "Number:3 epoch: 721/1200 loss_tea:0.052493009613651435  loss_houyan:1670.6064453125\n",
      "Number:3 epoch: 731/1200 loss_tea:0.037420851798531544  loss_houyan:1658.3135986328125\n",
      "Number:3 epoch: 741/1200 loss_tea:0.04732210316437023  loss_houyan:1784.22216796875\n",
      "Number:3 epoch: 751/1200 loss_tea:0.03964127394272075  loss_houyan:1602.1884765625\n",
      "Number:3 epoch: 761/1200 loss_tea:0.07332051236613309  loss_houyan:1744.4111328125\n",
      "Number:3 epoch: 771/1200 loss_tea:0.049332741208047017  loss_houyan:1585.177001953125\n",
      "Number:3 epoch: 781/1200 loss_tea:0.041331320882208776  loss_houyan:1632.1444091796875\n",
      "Number:3 epoch: 791/1200 loss_tea:0.06419089089619538  loss_houyan:1550.745361328125\n",
      "Number:3 epoch: 801/1200 loss_tea:0.045275137332095074  loss_houyan:1575.2742919921875\n",
      "Number:3 epoch: 811/1200 loss_tea:0.06691958134624551  loss_houyan:1535.60791015625\n",
      "Number:3 epoch: 821/1200 loss_tea:0.062066547240603905  loss_houyan:1724.63623046875\n",
      "Number:3 epoch: 831/1200 loss_tea:0.05243055509312423  loss_houyan:1574.1544189453125\n",
      "Number:3 epoch: 841/1200 loss_tea:0.051945596462515946  loss_houyan:1621.558349609375\n",
      "Number:3 epoch: 851/1200 loss_tea:0.05675553650770118  loss_houyan:1611.3115234375\n",
      "Number:3 epoch: 861/1200 loss_tea:0.06032848567131236  loss_houyan:1689.7764892578125\n",
      "Number:3 epoch: 871/1200 loss_tea:0.07413553712892948  loss_houyan:1779.186279296875\n",
      "Number:3 epoch: 881/1200 loss_tea:0.04707669953428736  loss_houyan:1766.9666748046875\n",
      "Number:3 epoch: 891/1200 loss_tea:0.07353363028944976  loss_houyan:1601.2435302734375\n",
      "Number:3 epoch: 901/1200 loss_tea:0.046926777210733246  loss_houyan:1775.105224609375\n",
      "Number:3 epoch: 911/1200 loss_tea:0.04977293535146439  loss_houyan:1524.63427734375\n",
      "Number:3 epoch: 921/1200 loss_tea:0.08008295717991169  loss_houyan:1562.65380859375\n",
      "Number:3 epoch: 931/1200 loss_tea:0.04960881824951701  loss_houyan:2172.59912109375\n",
      "Number:3 epoch: 941/1200 loss_tea:0.06977661421074663  loss_houyan:1685.4085693359375\n",
      "Number:3 epoch: 951/1200 loss_tea:0.0535588503043879  loss_houyan:1851.5765380859375\n",
      "Number:3 epoch: 961/1200 loss_tea:0.047563140439569454  loss_houyan:1647.1148681640625\n",
      "Number:3 epoch: 971/1200 loss_tea:0.043974125134919564  loss_houyan:1624.3077392578125\n",
      "Number:3 epoch: 981/1200 loss_tea:0.04875576311557525  loss_houyan:1535.91748046875\n",
      "Number:3 epoch: 991/1200 loss_tea:0.043613614194911575  loss_houyan:1570.1573486328125\n",
      "Number:3 epoch: 1001/1200 loss_tea:0.07040029328564702  loss_houyan:1526.9598388671875\n",
      "Number:3 epoch: 1011/1200 loss_tea:0.055601759146878614  loss_houyan:1595.221923828125\n",
      "Number:3 epoch: 1021/1200 loss_tea:0.051855412474649504  loss_houyan:1585.9390869140625\n",
      "Number:3 epoch: 1031/1200 loss_tea:0.05834528073104065  loss_houyan:1670.6744384765625\n",
      "Number:3 epoch: 1041/1200 loss_tea:0.0502992164108171  loss_houyan:1562.8260498046875\n",
      "Number:3 epoch: 1051/1200 loss_tea:0.056256809455797405  loss_houyan:1772.21337890625\n",
      "Number:3 epoch: 1061/1200 loss_tea:0.05784626874045754  loss_houyan:1567.9022216796875\n",
      "Number:3 epoch: 1071/1200 loss_tea:0.061018896375756186  loss_houyan:1534.8446044921875\n",
      "Number:3 epoch: 1081/1200 loss_tea:0.07656263263814238  loss_houyan:1803.806640625\n",
      "Number:3 epoch: 1091/1200 loss_tea:0.0666972180254605  loss_houyan:1625.6375732421875\n",
      "Number:3 epoch: 1101/1200 loss_tea:0.04900132457814176  loss_houyan:1573.9541015625\n",
      "Number:3 epoch: 1111/1200 loss_tea:0.05670768352433306  loss_houyan:1547.890380859375\n",
      "Number:3 epoch: 1121/1200 loss_tea:0.0435523939176908  loss_houyan:1535.153076171875\n",
      "Number:3 epoch: 1131/1200 loss_tea:0.06341230617570133  loss_houyan:1533.351806640625\n",
      "Number:3 epoch: 1141/1200 loss_tea:0.06359107717826391  loss_houyan:1638.43408203125\n",
      "Number:3 epoch: 1151/1200 loss_tea:0.061132690516700984  loss_houyan:1602.467529296875\n",
      "Number:3 epoch: 1161/1200 loss_tea:0.04546263655626885  loss_houyan:2278.9375\n",
      "Number:3 epoch: 1171/1200 loss_tea:0.06991143412518314  loss_houyan:1796.9266357421875\n",
      "Number:3 epoch: 1181/1200 loss_tea:0.054377864891576796  loss_houyan:1551.0555419921875\n",
      "Number:3 epoch: 1191/1200 loss_tea:0.04527619260074471  loss_houyan:1608.6026611328125\n",
      "finished training number 3 techer!\n",
      "start training number 4 techer!\n",
      "Number:4 epoch: 1/1200 loss_tea:0.05916388136354902  loss_houyan:1877.229736328125\n",
      "Number:4 epoch: 11/1200 loss_tea:0.07063032347985866  loss_houyan:1572.8695068359375\n",
      "Number:4 epoch: 21/1200 loss_tea:0.06395450671646176  loss_houyan:1826.1087646484375\n",
      "Number:4 epoch: 31/1200 loss_tea:0.05695033637991265  loss_houyan:2098.037841796875\n",
      "Number:4 epoch: 41/1200 loss_tea:0.04647291769364693  loss_houyan:1969.2501220703125\n",
      "Number:4 epoch: 51/1200 loss_tea:0.04865116691218713  loss_houyan:1670.67138671875\n",
      "Number:4 epoch: 61/1200 loss_tea:0.064379819901047  loss_houyan:1504.3018798828125\n",
      "Number:4 epoch: 71/1200 loss_tea:0.05515113868632731  loss_houyan:1547.2901611328125\n",
      "Number:4 epoch: 81/1200 loss_tea:0.07651421356043014  loss_houyan:1563.8614501953125\n",
      "Number:4 epoch: 91/1200 loss_tea:0.05086749698608323  loss_houyan:1750.3382568359375\n",
      "Number:4 epoch: 101/1200 loss_tea:0.04416694732657178  loss_houyan:1546.0284423828125\n",
      "Number:4 epoch: 111/1200 loss_tea:0.062153742222897504  loss_houyan:1576.089599609375\n",
      "Number:4 epoch: 121/1200 loss_tea:0.05960436930764125  loss_houyan:1540.146728515625\n",
      "Number:4 epoch: 131/1200 loss_tea:0.05668540647698965  loss_houyan:1556.6683349609375\n",
      "Number:4 epoch: 141/1200 loss_tea:0.04863504172337304  loss_houyan:1594.859375\n",
      "Number:4 epoch: 151/1200 loss_tea:0.07512899657987936  loss_houyan:1890.1181640625\n",
      "Number:4 epoch: 161/1200 loss_tea:0.05126916428023211  loss_houyan:1571.6700439453125\n",
      "Number:4 epoch: 171/1200 loss_tea:0.04586380248987798  loss_houyan:1521.4912109375\n",
      "Number:4 epoch: 181/1200 loss_tea:0.06281653056341867  loss_houyan:1681.9381103515625\n",
      "Number:4 epoch: 191/1200 loss_tea:0.044325105916102794  loss_houyan:1782.74755859375\n",
      "Number:4 epoch: 201/1200 loss_tea:0.043696552973051216  loss_houyan:1572.870361328125\n",
      "Number:4 epoch: 211/1200 loss_tea:0.04862798084232316  loss_houyan:1568.5484619140625\n",
      "Number:4 epoch: 221/1200 loss_tea:0.04689981119851726  loss_houyan:1601.18017578125\n",
      "Number:4 epoch: 231/1200 loss_tea:0.052730818394347435  loss_houyan:1757.370361328125\n",
      "Number:4 epoch: 241/1200 loss_tea:0.04833650291992243  loss_houyan:1698.00341796875\n",
      "Number:4 epoch: 251/1200 loss_tea:0.044502603971426344  loss_houyan:1596.9853515625\n",
      "Number:4 epoch: 261/1200 loss_tea:0.04828607603184711  loss_houyan:1574.0379638671875\n",
      "Number:4 epoch: 271/1200 loss_tea:0.06169097534542519  loss_houyan:1853.665283203125\n",
      "Number:4 epoch: 281/1200 loss_tea:0.06397890202974245  loss_houyan:1586.62255859375\n",
      "Number:4 epoch: 291/1200 loss_tea:0.05046328950838592  loss_houyan:2272.5341796875\n",
      "Number:4 epoch: 301/1200 loss_tea:0.06913333801042944  loss_houyan:1646.66064453125\n",
      "Number:4 epoch: 311/1200 loss_tea:0.05002650938639883  loss_houyan:1546.6990966796875\n",
      "Number:4 epoch: 321/1200 loss_tea:0.05211944088825746  loss_houyan:1989.4896240234375\n",
      "Number:4 epoch: 331/1200 loss_tea:0.049877559900486035  loss_houyan:1551.9151611328125\n",
      "Number:4 epoch: 341/1200 loss_tea:0.04964512223270298  loss_houyan:1617.0499267578125\n",
      "Number:4 epoch: 351/1200 loss_tea:0.0489346720938508  loss_houyan:1593.238525390625\n",
      "Number:4 epoch: 361/1200 loss_tea:0.07084324950884657  loss_houyan:1888.4798583984375\n",
      "Number:4 epoch: 371/1200 loss_tea:0.06059479185394944  loss_houyan:1559.239013671875\n",
      "Number:4 epoch: 381/1200 loss_tea:0.057685216193453175  loss_houyan:1702.2344970703125\n",
      "Number:4 epoch: 391/1200 loss_tea:0.04212837807037327  loss_houyan:1593.5762939453125\n",
      "Number:4 epoch: 401/1200 loss_tea:0.0564917585770442  loss_houyan:1767.8790283203125\n",
      "Number:4 epoch: 411/1200 loss_tea:0.05049152247696747  loss_houyan:1587.1767578125\n",
      "Number:4 epoch: 421/1200 loss_tea:0.0613976899426561  loss_houyan:1607.0904541015625\n",
      "Number:4 epoch: 431/1200 loss_tea:0.04916249963528284  loss_houyan:1768.598876953125\n",
      "Number:4 epoch: 441/1200 loss_tea:0.05740046180544751  loss_houyan:1635.53173828125\n",
      "Number:4 epoch: 451/1200 loss_tea:0.0491953837695285  loss_houyan:1922.83203125\n",
      "Number:4 epoch: 461/1200 loss_tea:0.04172796543807558  loss_houyan:1626.9013671875\n",
      "Number:4 epoch: 471/1200 loss_tea:0.04773869799857128  loss_houyan:1626.2962646484375\n",
      "Number:4 epoch: 481/1200 loss_tea:0.06772723070770073  loss_houyan:1904.915283203125\n",
      "Number:4 epoch: 491/1200 loss_tea:0.06584313449370419  loss_houyan:1758.2403564453125\n",
      "Number:4 epoch: 501/1200 loss_tea:0.05696660906290728  loss_houyan:1737.016357421875\n",
      "Number:4 epoch: 511/1200 loss_tea:0.041656078412715354  loss_houyan:1665.5150146484375\n",
      "Number:4 epoch: 521/1200 loss_tea:0.04375318712702955  loss_houyan:1954.1337890625\n",
      "Number:4 epoch: 531/1200 loss_tea:0.05594795934911776  loss_houyan:1564.0101318359375\n",
      "Number:4 epoch: 541/1200 loss_tea:0.05589621786102849  loss_houyan:1786.341552734375\n",
      "Number:4 epoch: 551/1200 loss_tea:0.048195452679681325  loss_houyan:1530.288330078125\n",
      "Number:4 epoch: 561/1200 loss_tea:0.05107711833698537  loss_houyan:1707.976806640625\n",
      "Number:4 epoch: 571/1200 loss_tea:0.04647278313196596  loss_houyan:1611.02978515625\n",
      "Number:4 epoch: 581/1200 loss_tea:0.06339370402197546  loss_houyan:1503.6236572265625\n",
      "Number:4 epoch: 591/1200 loss_tea:0.03664594391617054  loss_houyan:1513.10205078125\n",
      "Number:4 epoch: 601/1200 loss_tea:0.056972790695840055  loss_houyan:1511.19140625\n",
      "Number:4 epoch: 611/1200 loss_tea:0.0586485898528525  loss_houyan:1695.861572265625\n",
      "Number:4 epoch: 621/1200 loss_tea:0.06784339744732741  loss_houyan:1617.0574951171875\n",
      "Number:4 epoch: 631/1200 loss_tea:0.06493794396806396  loss_houyan:2983.222412109375\n",
      "Number:4 epoch: 641/1200 loss_tea:0.04936414386896752  loss_houyan:1527.1566162109375\n",
      "Number:4 epoch: 651/1200 loss_tea:0.08598967029186597  loss_houyan:1716.304443359375\n",
      "Number:4 epoch: 661/1200 loss_tea:0.058712555974245025  loss_houyan:1568.6153564453125\n",
      "Number:4 epoch: 671/1200 loss_tea:0.04895272801490793  loss_houyan:1878.74462890625\n",
      "Number:4 epoch: 681/1200 loss_tea:0.04797736860333316  loss_houyan:1695.0789794921875\n",
      "Number:4 epoch: 691/1200 loss_tea:0.04897010665336359  loss_houyan:1947.463134765625\n",
      "Number:4 epoch: 701/1200 loss_tea:0.047939131636657716  loss_houyan:1553.625244140625\n",
      "Number:4 epoch: 711/1200 loss_tea:0.049760359732733336  loss_houyan:2338.717041015625\n",
      "Number:4 epoch: 721/1200 loss_tea:0.053824408890983944  loss_houyan:1572.1085205078125\n",
      "Number:4 epoch: 731/1200 loss_tea:0.046209895711317464  loss_houyan:1647.1126708984375\n",
      "Number:4 epoch: 741/1200 loss_tea:0.05623065287136756  loss_houyan:1582.47705078125\n",
      "Number:4 epoch: 751/1200 loss_tea:0.05944586557000181  loss_houyan:1643.868408203125\n",
      "Number:4 epoch: 761/1200 loss_tea:0.04416189340171675  loss_houyan:1573.3028564453125\n",
      "Number:4 epoch: 771/1200 loss_tea:0.04830249659724629  loss_houyan:1620.076904296875\n",
      "Number:4 epoch: 781/1200 loss_tea:0.053826691455654656  loss_houyan:1649.8074951171875\n",
      "Number:4 epoch: 791/1200 loss_tea:0.047496651403993784  loss_houyan:1546.0142822265625\n",
      "Number:4 epoch: 801/1200 loss_tea:0.05304612658641733  loss_houyan:1508.151123046875\n",
      "Number:4 epoch: 811/1200 loss_tea:0.055536497917598994  loss_houyan:1583.12255859375\n",
      "Number:4 epoch: 821/1200 loss_tea:0.059591492971509896  loss_houyan:1560.133056640625\n",
      "Number:4 epoch: 831/1200 loss_tea:0.06035761828542794  loss_houyan:1531.67724609375\n",
      "Number:4 epoch: 841/1200 loss_tea:0.06261866424079544  loss_houyan:1834.06591796875\n",
      "Number:4 epoch: 851/1200 loss_tea:0.059985906384841546  loss_houyan:1605.837158203125\n",
      "Number:4 epoch: 861/1200 loss_tea:0.04884345928259482  loss_houyan:1570.4537353515625\n",
      "Number:4 epoch: 871/1200 loss_tea:0.056804001682579625  loss_houyan:1836.5655517578125\n",
      "Number:4 epoch: 881/1200 loss_tea:0.04656024811518127  loss_houyan:1749.2969970703125\n",
      "Number:4 epoch: 891/1200 loss_tea:0.0452897396992763  loss_houyan:1629.5223388671875\n",
      "Number:4 epoch: 901/1200 loss_tea:0.046258355138669834  loss_houyan:1610.2420654296875\n",
      "Number:4 epoch: 911/1200 loss_tea:0.04124907972674054  loss_houyan:1582.955322265625\n",
      "Number:4 epoch: 921/1200 loss_tea:0.05952871707810403  loss_houyan:1908.83642578125\n",
      "Number:4 epoch: 931/1200 loss_tea:0.05603500417210516  loss_houyan:1605.3421630859375\n",
      "Number:4 epoch: 941/1200 loss_tea:0.043580672737450185  loss_houyan:1724.9024658203125\n",
      "Number:4 epoch: 951/1200 loss_tea:0.049850131191293647  loss_houyan:1559.955078125\n",
      "Number:4 epoch: 961/1200 loss_tea:0.07297948761266845  loss_houyan:1716.6229248046875\n",
      "Number:4 epoch: 971/1200 loss_tea:0.044573042863564546  loss_houyan:2079.46240234375\n",
      "Number:4 epoch: 981/1200 loss_tea:0.04217605318678688  loss_houyan:1903.3282470703125\n",
      "Number:4 epoch: 991/1200 loss_tea:0.05094525158399268  loss_houyan:1554.6337890625\n",
      "Number:4 epoch: 1001/1200 loss_tea:0.06898976409691221  loss_houyan:1793.442626953125\n",
      "Number:4 epoch: 1011/1200 loss_tea:0.06269762989194716  loss_houyan:1494.319091796875\n",
      "Number:4 epoch: 1021/1200 loss_tea:0.046396898755421516  loss_houyan:1550.8231201171875\n",
      "Number:4 epoch: 1031/1200 loss_tea:0.06494999220053363  loss_houyan:1563.21875\n",
      "Number:4 epoch: 1041/1200 loss_tea:0.04875913345973115  loss_houyan:1558.5601806640625\n",
      "Number:4 epoch: 1051/1200 loss_tea:0.05317958611757294  loss_houyan:1656.2916259765625\n",
      "Number:4 epoch: 1061/1200 loss_tea:0.05243237616774619  loss_houyan:1629.1954345703125\n",
      "Number:4 epoch: 1071/1200 loss_tea:0.0700413118349907  loss_houyan:1515.831298828125\n",
      "Number:4 epoch: 1081/1200 loss_tea:0.05559529144681367  loss_houyan:2043.4815673828125\n",
      "Number:4 epoch: 1091/1200 loss_tea:0.046202323244986854  loss_houyan:1621.20166015625\n",
      "Number:4 epoch: 1101/1200 loss_tea:0.05673349255321297  loss_houyan:1778.33349609375\n",
      "Number:4 epoch: 1111/1200 loss_tea:0.038741785257221696  loss_houyan:1557.9337158203125\n",
      "Number:4 epoch: 1121/1200 loss_tea:0.04512674750068638  loss_houyan:1653.179931640625\n",
      "Number:4 epoch: 1131/1200 loss_tea:0.0680179105777688  loss_houyan:1612.221923828125\n",
      "Number:4 epoch: 1141/1200 loss_tea:0.06716885782985015  loss_houyan:1735.2518310546875\n",
      "Number:4 epoch: 1151/1200 loss_tea:0.07553007415423563  loss_houyan:1616.1531982421875\n",
      "Number:4 epoch: 1161/1200 loss_tea:0.07234667301184015  loss_houyan:1601.04638671875\n",
      "Number:4 epoch: 1171/1200 loss_tea:0.04670666006531628  loss_houyan:1675.67236328125\n",
      "Number:4 epoch: 1181/1200 loss_tea:0.05792191459089729  loss_houyan:1542.1954345703125\n",
      "Number:4 epoch: 1191/1200 loss_tea:0.0582586091033645  loss_houyan:1559.50390625\n",
      "finished training number 4 techer!\n",
      "start training number 5 techer!\n",
      "Number:5 epoch: 1/1200 loss_tea:0.046874332201910666  loss_houyan:1754.4088134765625\n",
      "Number:5 epoch: 11/1200 loss_tea:0.04991526495426114  loss_houyan:1586.7626953125\n",
      "Number:5 epoch: 21/1200 loss_tea:0.056116107129524875  loss_houyan:1762.62109375\n",
      "Number:5 epoch: 31/1200 loss_tea:0.06122139937298873  loss_houyan:1522.04931640625\n",
      "Number:5 epoch: 41/1200 loss_tea:0.04676977137029352  loss_houyan:1517.29443359375\n",
      "Number:5 epoch: 51/1200 loss_tea:0.04846548373270112  loss_houyan:1553.1632080078125\n",
      "Number:5 epoch: 61/1200 loss_tea:0.04930993077715495  loss_houyan:1508.6873779296875\n",
      "Number:5 epoch: 71/1200 loss_tea:0.06459316103920078  loss_houyan:2014.630126953125\n",
      "Number:5 epoch: 81/1200 loss_tea:0.0472124631258008  loss_houyan:1526.640869140625\n",
      "Number:5 epoch: 91/1200 loss_tea:0.042317094025895355  loss_houyan:1586.46337890625\n",
      "Number:5 epoch: 101/1200 loss_tea:0.05581385063840137  loss_houyan:1535.6082763671875\n",
      "Number:5 epoch: 111/1200 loss_tea:0.05075398999469493  loss_houyan:1570.17236328125\n",
      "Number:5 epoch: 121/1200 loss_tea:0.05447514193931982  loss_houyan:1637.0452880859375\n",
      "Number:5 epoch: 131/1200 loss_tea:0.054222352590720516  loss_houyan:1515.4805908203125\n",
      "Number:5 epoch: 141/1200 loss_tea:0.06508710449653911  loss_houyan:1534.6129150390625\n",
      "Number:5 epoch: 151/1200 loss_tea:0.057955170203817505  loss_houyan:1573.500732421875\n",
      "Number:5 epoch: 161/1200 loss_tea:0.06804557937426209  loss_houyan:1555.2161865234375\n",
      "Number:5 epoch: 171/1200 loss_tea:0.05563243325963444  loss_houyan:1818.583740234375\n",
      "Number:5 epoch: 181/1200 loss_tea:0.05114416698594131  loss_houyan:1576.9537353515625\n",
      "Number:5 epoch: 191/1200 loss_tea:0.04559121143351278  loss_houyan:1553.530029296875\n",
      "Number:5 epoch: 201/1200 loss_tea:0.05066488137220066  loss_houyan:1637.331787109375\n",
      "Number:5 epoch: 211/1200 loss_tea:0.05759812216826216  loss_houyan:1604.213623046875\n",
      "Number:5 epoch: 221/1200 loss_tea:0.07960450568975697  loss_houyan:1656.75830078125\n",
      "Number:5 epoch: 231/1200 loss_tea:0.055277201039378936  loss_houyan:1572.1947021484375\n",
      "Number:5 epoch: 241/1200 loss_tea:0.05313069305156488  loss_houyan:1619.5552978515625\n",
      "Number:5 epoch: 251/1200 loss_tea:0.0533855768316383  loss_houyan:1596.6875\n",
      "Number:5 epoch: 261/1200 loss_tea:0.07605223435699542  loss_houyan:1556.1761474609375\n",
      "Number:5 epoch: 271/1200 loss_tea:0.044088034865880074  loss_houyan:1756.6195068359375\n",
      "Number:5 epoch: 281/1200 loss_tea:0.06275779369547707  loss_houyan:1547.0950927734375\n",
      "Number:5 epoch: 291/1200 loss_tea:0.05047818398261954  loss_houyan:2263.9365234375\n",
      "Number:5 epoch: 301/1200 loss_tea:0.07414096644714532  loss_houyan:2007.6256103515625\n",
      "Number:5 epoch: 311/1200 loss_tea:0.04573306971087566  loss_houyan:1665.2083740234375\n",
      "Number:5 epoch: 321/1200 loss_tea:0.06051638965691684  loss_houyan:1718.119140625\n",
      "Number:5 epoch: 331/1200 loss_tea:0.062377195419350905  loss_houyan:1568.2576904296875\n",
      "Number:5 epoch: 341/1200 loss_tea:0.04987313343238041  loss_houyan:1899.954345703125\n",
      "Number:5 epoch: 351/1200 loss_tea:0.06047655525414512  loss_houyan:1562.3363037109375\n",
      "Number:5 epoch: 361/1200 loss_tea:0.055098901230578534  loss_houyan:1632.7572021484375\n",
      "Number:5 epoch: 371/1200 loss_tea:0.05667401595708569  loss_houyan:1699.36328125\n",
      "Number:5 epoch: 381/1200 loss_tea:0.04880337193057451  loss_houyan:1510.3990478515625\n",
      "Number:5 epoch: 391/1200 loss_tea:0.05507002355587828  loss_houyan:1863.5418701171875\n",
      "Number:5 epoch: 401/1200 loss_tea:0.06430627160937843  loss_houyan:1672.7166748046875\n",
      "Number:5 epoch: 411/1200 loss_tea:0.04437768999966311  loss_houyan:1523.939697265625\n",
      "Number:5 epoch: 421/1200 loss_tea:0.04911878214217534  loss_houyan:1532.327880859375\n",
      "Number:5 epoch: 431/1200 loss_tea:0.05975675266458664  loss_houyan:1598.0645751953125\n",
      "Number:5 epoch: 441/1200 loss_tea:0.05148835517337792  loss_houyan:1669.2320556640625\n",
      "Number:5 epoch: 451/1200 loss_tea:0.06766878049130234  loss_houyan:1800.2061767578125\n",
      "Number:5 epoch: 461/1200 loss_tea:0.043320817660471675  loss_houyan:1952.6337890625\n",
      "Number:5 epoch: 471/1200 loss_tea:0.04716113325889798  loss_houyan:1695.524658203125\n",
      "Number:5 epoch: 481/1200 loss_tea:0.05082164801233134  loss_houyan:1613.7535400390625\n",
      "Number:5 epoch: 491/1200 loss_tea:0.05495425495712386  loss_houyan:1661.975341796875\n",
      "Number:5 epoch: 501/1200 loss_tea:0.07482943012299315  loss_houyan:1521.2100830078125\n",
      "Number:5 epoch: 511/1200 loss_tea:0.04479710972293116  loss_houyan:1702.976318359375\n",
      "Number:5 epoch: 521/1200 loss_tea:0.057497539496208123  loss_houyan:1579.7064208984375\n",
      "Number:5 epoch: 531/1200 loss_tea:0.041699653206529325  loss_houyan:1547.474365234375\n",
      "Number:5 epoch: 541/1200 loss_tea:0.042874860839308186  loss_houyan:1556.007568359375\n",
      "Number:5 epoch: 551/1200 loss_tea:0.042733966153139936  loss_houyan:1598.1300048828125\n",
      "Number:5 epoch: 561/1200 loss_tea:0.06897806371012294  loss_houyan:1529.016357421875\n",
      "Number:5 epoch: 571/1200 loss_tea:0.052631159039681706  loss_houyan:1562.0443115234375\n",
      "Number:5 epoch: 581/1200 loss_tea:0.06174793488015729  loss_houyan:1555.7198486328125\n",
      "Number:5 epoch: 591/1200 loss_tea:0.05921886190364591  loss_houyan:1940.882080078125\n",
      "Number:5 epoch: 601/1200 loss_tea:0.07257550822015182  loss_houyan:1699.385986328125\n",
      "Number:5 epoch: 611/1200 loss_tea:0.08706654876950469  loss_houyan:1828.7711181640625\n",
      "Number:5 epoch: 621/1200 loss_tea:0.04829151204192786  loss_houyan:1587.8978271484375\n",
      "Number:5 epoch: 631/1200 loss_tea:0.05780343401853387  loss_houyan:1687.572998046875\n",
      "Number:5 epoch: 641/1200 loss_tea:0.04049496385118897  loss_houyan:1731.99365234375\n",
      "Number:5 epoch: 651/1200 loss_tea:0.05128488169010772  loss_houyan:1783.5902099609375\n",
      "Number:5 epoch: 661/1200 loss_tea:0.04580341655355802  loss_houyan:1785.4918212890625\n",
      "Number:5 epoch: 671/1200 loss_tea:0.06997245837645127  loss_houyan:1519.5643310546875\n",
      "Number:5 epoch: 681/1200 loss_tea:0.06751036038103496  loss_houyan:1558.6934814453125\n",
      "Number:5 epoch: 691/1200 loss_tea:0.046305129587867616  loss_houyan:1563.5694580078125\n",
      "Number:5 epoch: 701/1200 loss_tea:0.05104831198986083  loss_houyan:1535.0015869140625\n",
      "Number:5 epoch: 711/1200 loss_tea:0.05037343523323427  loss_houyan:1643.4422607421875\n",
      "Number:5 epoch: 721/1200 loss_tea:0.04273223183507351  loss_houyan:1512.58349609375\n",
      "Number:5 epoch: 731/1200 loss_tea:0.05370559626071589  loss_houyan:1488.900390625\n",
      "Number:5 epoch: 741/1200 loss_tea:0.04627558823236515  loss_houyan:2301.2744140625\n",
      "Number:5 epoch: 751/1200 loss_tea:0.04943953786729704  loss_houyan:1604.2977294921875\n",
      "Number:5 epoch: 761/1200 loss_tea:0.055879443670353815  loss_houyan:1538.198974609375\n",
      "Number:5 epoch: 771/1200 loss_tea:0.05429391085449587  loss_houyan:1541.1868896484375\n",
      "Number:5 epoch: 781/1200 loss_tea:0.04528347931308787  loss_houyan:1604.1219482421875\n",
      "Number:5 epoch: 791/1200 loss_tea:0.04664119320955744  loss_houyan:1591.5390625\n",
      "Number:5 epoch: 801/1200 loss_tea:0.05081679710787464  loss_houyan:1743.308837890625\n",
      "Number:5 epoch: 811/1200 loss_tea:0.04585280004456407  loss_houyan:1868.5382080078125\n",
      "Number:5 epoch: 821/1200 loss_tea:0.06169745563579422  loss_houyan:1615.8316650390625\n",
      "Number:5 epoch: 831/1200 loss_tea:0.06347378048006765  loss_houyan:1543.8043212890625\n",
      "Number:5 epoch: 841/1200 loss_tea:0.0538580509539145  loss_houyan:1636.372802734375\n",
      "Number:5 epoch: 851/1200 loss_tea:0.04850094397649462  loss_houyan:1643.879638671875\n",
      "Number:5 epoch: 861/1200 loss_tea:0.04524345444853015  loss_houyan:2036.763427734375\n",
      "Number:5 epoch: 871/1200 loss_tea:0.05283748272014867  loss_houyan:1566.1217041015625\n",
      "Number:5 epoch: 881/1200 loss_tea:0.06290617427477982  loss_houyan:1527.57080078125\n",
      "Number:5 epoch: 891/1200 loss_tea:0.05729918996525216  loss_houyan:1707.19775390625\n",
      "Number:5 epoch: 901/1200 loss_tea:0.0653357289449295  loss_houyan:1599.48828125\n",
      "Number:5 epoch: 911/1200 loss_tea:0.042673502217752624  loss_houyan:1735.6690673828125\n",
      "Number:5 epoch: 921/1200 loss_tea:0.048745315404997824  loss_houyan:1549.633544921875\n",
      "Number:5 epoch: 931/1200 loss_tea:0.03584218991655122  loss_houyan:1749.549560546875\n",
      "Number:5 epoch: 941/1200 loss_tea:0.0612517524222378  loss_houyan:1633.44970703125\n",
      "Number:5 epoch: 951/1200 loss_tea:0.05184089618642392  loss_houyan:1762.2618408203125\n",
      "Number:5 epoch: 961/1200 loss_tea:0.06118772360927269  loss_houyan:1588.5042724609375\n",
      "Number:5 epoch: 971/1200 loss_tea:0.06191117265853101  loss_houyan:1705.516357421875\n",
      "Number:5 epoch: 981/1200 loss_tea:0.047918720477586434  loss_houyan:1675.2744140625\n",
      "Number:5 epoch: 991/1200 loss_tea:0.07168576602063996  loss_houyan:1570.4549560546875\n",
      "Number:5 epoch: 1001/1200 loss_tea:0.058839450026086065  loss_houyan:1713.5511474609375\n",
      "Number:5 epoch: 1011/1200 loss_tea:0.04400703767478671  loss_houyan:1593.084228515625\n",
      "Number:5 epoch: 1021/1200 loss_tea:0.05000696862816316  loss_houyan:1630.88525390625\n",
      "Number:5 epoch: 1031/1200 loss_tea:0.07898389645704065  loss_houyan:1591.0550537109375\n",
      "Number:5 epoch: 1041/1200 loss_tea:0.05293938403782005  loss_houyan:1852.2762451171875\n",
      "Number:5 epoch: 1051/1200 loss_tea:0.04930134335295474  loss_houyan:1940.1064453125\n",
      "Number:5 epoch: 1061/1200 loss_tea:0.06131859016105765  loss_houyan:2066.4365234375\n",
      "Number:5 epoch: 1071/1200 loss_tea:0.0589761537618004  loss_houyan:1637.8519287109375\n",
      "Number:5 epoch: 1081/1200 loss_tea:0.06507809244734641  loss_houyan:2136.51611328125\n",
      "Number:5 epoch: 1091/1200 loss_tea:0.03603724283233197  loss_houyan:1608.34228515625\n",
      "Number:5 epoch: 1101/1200 loss_tea:0.04793555489919532  loss_houyan:1699.12353515625\n",
      "Number:5 epoch: 1111/1200 loss_tea:0.05439241561968485  loss_houyan:1628.752685546875\n",
      "Number:5 epoch: 1121/1200 loss_tea:0.05402771456867716  loss_houyan:1566.6348876953125\n",
      "Number:5 epoch: 1131/1200 loss_tea:0.04828337728086816  loss_houyan:1638.653564453125\n",
      "Number:5 epoch: 1141/1200 loss_tea:0.050290485304878965  loss_houyan:1564.0506591796875\n",
      "Number:5 epoch: 1151/1200 loss_tea:0.052676776571567906  loss_houyan:1527.854736328125\n",
      "Number:5 epoch: 1161/1200 loss_tea:0.05807309868619682  loss_houyan:2088.439697265625\n",
      "Number:5 epoch: 1171/1200 loss_tea:0.054736132783136535  loss_houyan:2207.18505859375\n",
      "Number:5 epoch: 1181/1200 loss_tea:0.050704744809524284  loss_houyan:1527.442626953125\n",
      "Number:5 epoch: 1191/1200 loss_tea:0.03958573543608297  loss_houyan:1636.984619140625\n",
      "finished training number 5 techer!\n",
      "start training number 6 techer!\n",
      "Number:6 epoch: 1/1200 loss_tea:0.08176413714449572  loss_houyan:1647.2261962890625\n",
      "Number:6 epoch: 11/1200 loss_tea:0.049356762115315964  loss_houyan:1968.0947265625\n",
      "Number:6 epoch: 21/1200 loss_tea:0.040776685072670876  loss_houyan:1579.8121337890625\n",
      "Number:6 epoch: 31/1200 loss_tea:0.07555088254115482  loss_houyan:1853.170166015625\n",
      "Number:6 epoch: 41/1200 loss_tea:0.04670970169957504  loss_houyan:1557.8092041015625\n",
      "Number:6 epoch: 51/1200 loss_tea:0.05310961686177355  loss_houyan:1614.151123046875\n",
      "Number:6 epoch: 61/1200 loss_tea:0.08178091907663604  loss_houyan:1551.4520263671875\n",
      "Number:6 epoch: 71/1200 loss_tea:0.04997984971605877  loss_houyan:1552.002197265625\n",
      "Number:6 epoch: 81/1200 loss_tea:0.04466559331662649  loss_houyan:1596.6690673828125\n",
      "Number:6 epoch: 91/1200 loss_tea:0.04576862499446842  loss_houyan:1589.2110595703125\n",
      "Number:6 epoch: 101/1200 loss_tea:0.05579302914656438  loss_houyan:1641.18017578125\n",
      "Number:6 epoch: 111/1200 loss_tea:0.05460485560543713  loss_houyan:1619.3863525390625\n",
      "Number:6 epoch: 121/1200 loss_tea:0.04523250933435491  loss_houyan:1552.427001953125\n",
      "Number:6 epoch: 131/1200 loss_tea:0.052572129752674834  loss_houyan:1531.8665771484375\n",
      "Number:6 epoch: 141/1200 loss_tea:0.05384148051632134  loss_houyan:1543.948974609375\n",
      "Number:6 epoch: 151/1200 loss_tea:0.05281281633408586  loss_houyan:2481.718994140625\n",
      "Number:6 epoch: 161/1200 loss_tea:0.057280771269057236  loss_houyan:1586.2406005859375\n",
      "Number:6 epoch: 171/1200 loss_tea:0.06611424853999967  loss_houyan:2012.72314453125\n",
      "Number:6 epoch: 181/1200 loss_tea:0.03843471586873277  loss_houyan:1533.48681640625\n",
      "Number:6 epoch: 191/1200 loss_tea:0.038188235340377734  loss_houyan:1709.826904296875\n",
      "Number:6 epoch: 201/1200 loss_tea:0.04140423303547359  loss_houyan:1507.692626953125\n",
      "Number:6 epoch: 211/1200 loss_tea:0.06285218931983569  loss_houyan:1534.9403076171875\n",
      "Number:6 epoch: 221/1200 loss_tea:0.0490431411809433  loss_houyan:1548.4073486328125\n",
      "Number:6 epoch: 231/1200 loss_tea:0.045341317717522436  loss_houyan:1888.02978515625\n",
      "Number:6 epoch: 241/1200 loss_tea:0.06019631895774865  loss_houyan:1579.7373046875\n",
      "Number:6 epoch: 251/1200 loss_tea:0.05465760832987854  loss_houyan:1591.21533203125\n",
      "Number:6 epoch: 261/1200 loss_tea:0.06152001376429236  loss_houyan:1863.4713134765625\n",
      "Number:6 epoch: 271/1200 loss_tea:0.05520730627353963  loss_houyan:1584.5279541015625\n",
      "Number:6 epoch: 281/1200 loss_tea:0.06467985671676857  loss_houyan:1544.5086669921875\n",
      "Number:6 epoch: 291/1200 loss_tea:0.05020047856748696  loss_houyan:1582.5758056640625\n",
      "Number:6 epoch: 301/1200 loss_tea:0.046750544052565776  loss_houyan:1550.8221435546875\n",
      "Number:6 epoch: 311/1200 loss_tea:0.04715979606719038  loss_houyan:1841.1483154296875\n",
      "Number:6 epoch: 321/1200 loss_tea:0.042853928871523735  loss_houyan:1547.932373046875\n",
      "Number:6 epoch: 331/1200 loss_tea:0.0604406417443132  loss_houyan:1739.3232421875\n",
      "Number:6 epoch: 341/1200 loss_tea:0.06018501026067315  loss_houyan:1541.3515625\n",
      "Number:6 epoch: 351/1200 loss_tea:0.039405087950392874  loss_houyan:1626.7008056640625\n",
      "Number:6 epoch: 361/1200 loss_tea:0.04837938755258679  loss_houyan:1752.42529296875\n",
      "Number:6 epoch: 371/1200 loss_tea:0.04584197912207494  loss_houyan:1550.49951171875\n",
      "Number:6 epoch: 381/1200 loss_tea:0.058345241614360666  loss_houyan:1687.5872802734375\n",
      "Number:6 epoch: 391/1200 loss_tea:0.060506463330031554  loss_houyan:1560.826171875\n",
      "Number:6 epoch: 401/1200 loss_tea:0.05897410490721953  loss_houyan:1863.11669921875\n",
      "Number:6 epoch: 411/1200 loss_tea:0.07250114683464022  loss_houyan:1567.4195556640625\n",
      "Number:6 epoch: 421/1200 loss_tea:0.05744192203258198  loss_houyan:1647.9097900390625\n",
      "Number:6 epoch: 431/1200 loss_tea:0.03693961946254851  loss_houyan:1653.0128173828125\n",
      "Number:6 epoch: 441/1200 loss_tea:0.04853198928269921  loss_houyan:1580.6265869140625\n",
      "Number:6 epoch: 451/1200 loss_tea:0.0627449292546949  loss_houyan:1663.840087890625\n",
      "Number:6 epoch: 461/1200 loss_tea:0.06003430075015354  loss_houyan:1692.062744140625\n",
      "Number:6 epoch: 471/1200 loss_tea:0.04334935009462922  loss_houyan:1666.4063720703125\n",
      "Number:6 epoch: 481/1200 loss_tea:0.04741977978439642  loss_houyan:1536.2891845703125\n",
      "Number:6 epoch: 491/1200 loss_tea:0.05841413729084463  loss_houyan:1665.85791015625\n",
      "Number:6 epoch: 501/1200 loss_tea:0.04590071970662934  loss_houyan:1604.3946533203125\n",
      "Number:6 epoch: 511/1200 loss_tea:0.06633736534092816  loss_houyan:1791.4296875\n",
      "Number:6 epoch: 521/1200 loss_tea:0.06879772435770372  loss_houyan:1607.1300048828125\n",
      "Number:6 epoch: 531/1200 loss_tea:0.0671109828452078  loss_houyan:2238.361328125\n",
      "Number:6 epoch: 541/1200 loss_tea:0.0615964037386396  loss_houyan:1718.6844482421875\n",
      "Number:6 epoch: 551/1200 loss_tea:0.06304668413476287  loss_houyan:1579.74365234375\n",
      "Number:6 epoch: 561/1200 loss_tea:0.08576073463427433  loss_houyan:1537.15771484375\n",
      "Number:6 epoch: 571/1200 loss_tea:0.061334904136144855  loss_houyan:2095.736328125\n",
      "Number:6 epoch: 581/1200 loss_tea:0.06513945413204444  loss_houyan:1844.1700439453125\n",
      "Number:6 epoch: 591/1200 loss_tea:0.05003272386028961  loss_houyan:1737.1680908203125\n",
      "Number:6 epoch: 601/1200 loss_tea:0.045739784776403856  loss_houyan:1573.449462890625\n",
      "Number:6 epoch: 611/1200 loss_tea:0.0599454754768462  loss_houyan:1949.427001953125\n",
      "Number:6 epoch: 621/1200 loss_tea:0.06498392056526625  loss_houyan:1777.6368408203125\n",
      "Number:6 epoch: 631/1200 loss_tea:0.050111308947797126  loss_houyan:1518.6376953125\n",
      "Number:6 epoch: 641/1200 loss_tea:0.04219761772652085  loss_houyan:1656.803955078125\n",
      "Number:6 epoch: 651/1200 loss_tea:0.04780855028312265  loss_houyan:1575.35205078125\n",
      "Number:6 epoch: 661/1200 loss_tea:0.04944057884862458  loss_houyan:1717.3369140625\n",
      "Number:6 epoch: 671/1200 loss_tea:0.0591897993377401  loss_houyan:1976.268310546875\n",
      "Number:6 epoch: 681/1200 loss_tea:0.06280691276557392  loss_houyan:1585.4234619140625\n",
      "Number:6 epoch: 691/1200 loss_tea:0.07223804441373285  loss_houyan:1566.31689453125\n",
      "Number:6 epoch: 701/1200 loss_tea:0.05260887420974884  loss_houyan:1714.043212890625\n",
      "Number:6 epoch: 711/1200 loss_tea:0.05410162969466909  loss_houyan:2294.74169921875\n",
      "Number:6 epoch: 721/1200 loss_tea:0.04771374598830631  loss_houyan:1602.7802734375\n",
      "Number:6 epoch: 731/1200 loss_tea:0.05416677607057764  loss_houyan:1688.670654296875\n",
      "Number:6 epoch: 741/1200 loss_tea:0.059706505780395575  loss_houyan:1554.8211669921875\n",
      "Number:6 epoch: 751/1200 loss_tea:0.0503870107494604  loss_houyan:1547.149169921875\n",
      "Number:6 epoch: 761/1200 loss_tea:0.04960067855850306  loss_houyan:1542.87890625\n",
      "Number:6 epoch: 771/1200 loss_tea:0.05502583747490084  loss_houyan:1494.4552001953125\n",
      "Number:6 epoch: 781/1200 loss_tea:0.05936397574819339  loss_houyan:1553.078857421875\n",
      "Number:6 epoch: 791/1200 loss_tea:0.04804743767080458  loss_houyan:1634.1771240234375\n",
      "Number:6 epoch: 801/1200 loss_tea:0.07015867463237775  loss_houyan:1626.334716796875\n",
      "Number:6 epoch: 811/1200 loss_tea:0.04838183638427526  loss_houyan:1629.37890625\n",
      "Number:6 epoch: 821/1200 loss_tea:0.05269562239263109  loss_houyan:1571.978759765625\n",
      "Number:6 epoch: 831/1200 loss_tea:0.07088427989316755  loss_houyan:1709.1226806640625\n",
      "Number:6 epoch: 841/1200 loss_tea:0.04860593001864713  loss_houyan:1588.22705078125\n",
      "Number:6 epoch: 851/1200 loss_tea:0.060069578407562066  loss_houyan:1799.9512939453125\n",
      "Number:6 epoch: 861/1200 loss_tea:0.060157588991120156  loss_houyan:1590.012939453125\n",
      "Number:6 epoch: 871/1200 loss_tea:0.042387049279975726  loss_houyan:1528.758544921875\n",
      "Number:6 epoch: 881/1200 loss_tea:0.06114363377313576  loss_houyan:1730.7412109375\n",
      "Number:6 epoch: 891/1200 loss_tea:0.053257541730622665  loss_houyan:1763.0\n",
      "Number:6 epoch: 901/1200 loss_tea:0.043547078539821656  loss_houyan:1788.944091796875\n",
      "Number:6 epoch: 911/1200 loss_tea:0.05076047095096809  loss_houyan:1676.6195068359375\n",
      "Number:6 epoch: 921/1200 loss_tea:0.060595977554165306  loss_houyan:1566.99560546875\n",
      "Number:6 epoch: 931/1200 loss_tea:0.06579962617787292  loss_houyan:1561.1739501953125\n",
      "Number:6 epoch: 941/1200 loss_tea:0.051663538249779595  loss_houyan:1873.625732421875\n",
      "Number:6 epoch: 951/1200 loss_tea:0.05155658228437935  loss_houyan:1625.747314453125\n",
      "Number:6 epoch: 961/1200 loss_tea:0.055456152979223405  loss_houyan:1610.449462890625\n",
      "Number:6 epoch: 971/1200 loss_tea:0.06935351856350175  loss_houyan:1734.3779296875\n",
      "Number:6 epoch: 981/1200 loss_tea:0.0726173339150914  loss_houyan:1538.5501708984375\n",
      "Number:6 epoch: 991/1200 loss_tea:0.05060423194686  loss_houyan:1634.200439453125\n",
      "Number:6 epoch: 1001/1200 loss_tea:0.06855275996402133  loss_houyan:1805.00048828125\n",
      "Number:6 epoch: 1011/1200 loss_tea:0.04778180923558136  loss_houyan:1566.8924560546875\n",
      "Number:6 epoch: 1021/1200 loss_tea:0.06538865115762917  loss_houyan:1601.570556640625\n",
      "Number:6 epoch: 1031/1200 loss_tea:0.05931855774834882  loss_houyan:1528.3447265625\n",
      "Number:6 epoch: 1041/1200 loss_tea:0.04504375373131596  loss_houyan:1928.3902587890625\n",
      "Number:6 epoch: 1051/1200 loss_tea:0.05493710662717638  loss_houyan:1746.4910888671875\n",
      "Number:6 epoch: 1061/1200 loss_tea:0.04067218961774796  loss_houyan:1515.6768798828125\n",
      "Number:6 epoch: 1071/1200 loss_tea:0.05572770057249485  loss_houyan:1931.614501953125\n",
      "Number:6 epoch: 1081/1200 loss_tea:0.047145133615837956  loss_houyan:1630.7730712890625\n",
      "Number:6 epoch: 1091/1200 loss_tea:0.05939513361596472  loss_houyan:1581.7686767578125\n",
      "Number:6 epoch: 1101/1200 loss_tea:0.0451264431277931  loss_houyan:1543.309814453125\n",
      "Number:6 epoch: 1111/1200 loss_tea:0.06631036699180398  loss_houyan:1568.3988037109375\n",
      "Number:6 epoch: 1121/1200 loss_tea:0.05317668835746631  loss_houyan:1766.63134765625\n",
      "Number:6 epoch: 1131/1200 loss_tea:0.057362735186942515  loss_houyan:1698.853271484375\n",
      "Number:6 epoch: 1141/1200 loss_tea:0.0704307758677747  loss_houyan:1592.9808349609375\n",
      "Number:6 epoch: 1151/1200 loss_tea:0.0751467426265055  loss_houyan:1677.788818359375\n",
      "Number:6 epoch: 1161/1200 loss_tea:0.06309763696410647  loss_houyan:2349.64599609375\n",
      "Number:6 epoch: 1171/1200 loss_tea:0.0726129925702841  loss_houyan:1572.6370849609375\n",
      "Number:6 epoch: 1181/1200 loss_tea:0.0629507897963873  loss_houyan:1556.0665283203125\n",
      "Number:6 epoch: 1191/1200 loss_tea:0.04970478996897072  loss_houyan:1732.611328125\n",
      "finished training number 6 techer!\n",
      "start training number 7 techer!\n",
      "Number:7 epoch: 1/1200 loss_tea:0.07314842401597937  loss_houyan:1694.7222900390625\n",
      "Number:7 epoch: 11/1200 loss_tea:0.06803741734508853  loss_houyan:1514.2161865234375\n",
      "Number:7 epoch: 21/1200 loss_tea:0.04721936875158581  loss_houyan:1676.759033203125\n",
      "Number:7 epoch: 31/1200 loss_tea:0.056796398567681323  loss_houyan:2033.2369384765625\n",
      "Number:7 epoch: 41/1200 loss_tea:0.05018468298801218  loss_houyan:1571.733642578125\n",
      "Number:7 epoch: 51/1200 loss_tea:0.045825654024691  loss_houyan:1695.989013671875\n",
      "Number:7 epoch: 61/1200 loss_tea:0.05776521465587355  loss_houyan:1653.0526123046875\n",
      "Number:7 epoch: 71/1200 loss_tea:0.08464442963353687  loss_houyan:1563.48291015625\n",
      "Number:7 epoch: 81/1200 loss_tea:0.05570983727656324  loss_houyan:2264.123046875\n",
      "Number:7 epoch: 91/1200 loss_tea:0.03698842235589398  loss_houyan:1825.3370361328125\n",
      "Number:7 epoch: 101/1200 loss_tea:0.052428113972341484  loss_houyan:1585.8154296875\n",
      "Number:7 epoch: 111/1200 loss_tea:0.04561045746031695  loss_houyan:1539.7869873046875\n",
      "Number:7 epoch: 121/1200 loss_tea:0.04619158632149333  loss_houyan:1584.0628662109375\n",
      "Number:7 epoch: 131/1200 loss_tea:0.07002410115717525  loss_houyan:1595.3330078125\n",
      "Number:7 epoch: 141/1200 loss_tea:0.06231734531024025  loss_houyan:1546.9757080078125\n",
      "Number:7 epoch: 151/1200 loss_tea:0.05033360952569908  loss_houyan:1847.0201416015625\n",
      "Number:7 epoch: 161/1200 loss_tea:0.04037870667876322  loss_houyan:1578.0947265625\n",
      "Number:7 epoch: 171/1200 loss_tea:0.06733430945875675  loss_houyan:1549.98046875\n",
      "Number:7 epoch: 181/1200 loss_tea:0.05454892352153976  loss_houyan:1638.1353759765625\n",
      "Number:7 epoch: 191/1200 loss_tea:0.046817629649580216  loss_houyan:1531.0242919921875\n",
      "Number:7 epoch: 201/1200 loss_tea:0.06792431004935487  loss_houyan:1984.304931640625\n",
      "Number:7 epoch: 211/1200 loss_tea:0.04216863833826601  loss_houyan:1649.3350830078125\n",
      "Number:7 epoch: 221/1200 loss_tea:0.060388656559081266  loss_houyan:1848.60205078125\n",
      "Number:7 epoch: 231/1200 loss_tea:0.0488725452158877  loss_houyan:1525.4503173828125\n",
      "Number:7 epoch: 241/1200 loss_tea:0.06491912081641467  loss_houyan:1652.2845458984375\n",
      "Number:7 epoch: 251/1200 loss_tea:0.06415947270933339  loss_houyan:1563.264404296875\n",
      "Number:7 epoch: 261/1200 loss_tea:0.0442965087504046  loss_houyan:1557.1790771484375\n",
      "Number:7 epoch: 271/1200 loss_tea:0.048437817062950944  loss_houyan:1599.4605712890625\n",
      "Number:7 epoch: 281/1200 loss_tea:0.04217851825446307  loss_houyan:1714.76416015625\n",
      "Number:7 epoch: 291/1200 loss_tea:0.05061164955897734  loss_houyan:1568.6680908203125\n",
      "Number:7 epoch: 301/1200 loss_tea:0.05815662105215182  loss_houyan:1559.2674560546875\n",
      "Number:7 epoch: 311/1200 loss_tea:0.04239822088833012  loss_houyan:1759.5780029296875\n",
      "Number:7 epoch: 321/1200 loss_tea:0.055458025856281984  loss_houyan:1614.856689453125\n",
      "Number:7 epoch: 331/1200 loss_tea:0.057589572823214245  loss_houyan:1528.5360107421875\n",
      "Number:7 epoch: 341/1200 loss_tea:0.04170582389448827  loss_houyan:1509.909912109375\n",
      "Number:7 epoch: 351/1200 loss_tea:0.054351980606475433  loss_houyan:1788.9896240234375\n",
      "Number:7 epoch: 361/1200 loss_tea:0.04641313619793958  loss_houyan:1875.3134765625\n",
      "Number:7 epoch: 371/1200 loss_tea:0.05579273900100522  loss_houyan:1661.051025390625\n",
      "Number:7 epoch: 381/1200 loss_tea:0.061215609358085996  loss_houyan:1868.3914794921875\n",
      "Number:7 epoch: 391/1200 loss_tea:0.08515550782128713  loss_houyan:1576.86181640625\n",
      "Number:7 epoch: 401/1200 loss_tea:0.06299667173861878  loss_houyan:2119.21826171875\n",
      "Number:7 epoch: 411/1200 loss_tea:0.04979936413788375  loss_houyan:1570.4793701171875\n",
      "Number:7 epoch: 421/1200 loss_tea:0.06693110663571325  loss_houyan:1930.022705078125\n",
      "Number:7 epoch: 431/1200 loss_tea:0.058864560378921016  loss_houyan:1558.2418212890625\n",
      "Number:7 epoch: 441/1200 loss_tea:0.0473859261989998  loss_houyan:1706.855224609375\n",
      "Number:7 epoch: 451/1200 loss_tea:0.04204751082641503  loss_houyan:1557.1817626953125\n",
      "Number:7 epoch: 461/1200 loss_tea:0.03842000867918674  loss_houyan:1565.490478515625\n",
      "Number:7 epoch: 471/1200 loss_tea:0.045129568202668946  loss_houyan:1759.210205078125\n",
      "Number:7 epoch: 481/1200 loss_tea:0.057317105149640855  loss_houyan:1535.7747802734375\n",
      "Number:7 epoch: 491/1200 loss_tea:0.03764646872387607  loss_houyan:1580.4007568359375\n",
      "Number:7 epoch: 501/1200 loss_tea:0.05518978355380357  loss_houyan:1642.5963134765625\n",
      "Number:7 epoch: 511/1200 loss_tea:0.0450918865887192  loss_houyan:1897.930908203125\n",
      "Number:7 epoch: 521/1200 loss_tea:0.05063638328379354  loss_houyan:1609.702392578125\n",
      "Number:7 epoch: 531/1200 loss_tea:0.0406226812683971  loss_houyan:1557.7562255859375\n",
      "Number:7 epoch: 541/1200 loss_tea:0.060531722316441085  loss_houyan:1544.9407958984375\n",
      "Number:7 epoch: 551/1200 loss_tea:0.04596315884741213  loss_houyan:1643.9681396484375\n",
      "Number:7 epoch: 561/1200 loss_tea:0.05389227018858924  loss_houyan:1673.9437255859375\n",
      "Number:7 epoch: 571/1200 loss_tea:0.05116021442167643  loss_houyan:1506.2337646484375\n",
      "Number:7 epoch: 581/1200 loss_tea:0.04711189156095219  loss_houyan:1504.802490234375\n",
      "Number:7 epoch: 591/1200 loss_tea:0.05026102535568475  loss_houyan:1622.738525390625\n",
      "Number:7 epoch: 601/1200 loss_tea:0.04801338475936273  loss_houyan:1702.4437255859375\n",
      "Number:7 epoch: 611/1200 loss_tea:0.06124451132904356  loss_houyan:1852.0908203125\n",
      "Number:7 epoch: 621/1200 loss_tea:0.05266031880721719  loss_houyan:1962.065185546875\n",
      "Number:7 epoch: 631/1200 loss_tea:0.05843019603864973  loss_houyan:1543.8795166015625\n",
      "Number:7 epoch: 641/1200 loss_tea:0.07028237904569314  loss_houyan:1741.03076171875\n",
      "Number:7 epoch: 651/1200 loss_tea:0.05676598371686978  loss_houyan:1624.032958984375\n",
      "Number:7 epoch: 661/1200 loss_tea:0.05476099228252881  loss_houyan:1737.2169189453125\n",
      "Number:7 epoch: 671/1200 loss_tea:0.043038984605578826  loss_houyan:1505.515625\n",
      "Number:7 epoch: 681/1200 loss_tea:0.06374978030797306  loss_houyan:1571.8795166015625\n",
      "Number:7 epoch: 691/1200 loss_tea:0.04706466706242908  loss_houyan:1549.49609375\n",
      "Number:7 epoch: 701/1200 loss_tea:0.06481353732130415  loss_houyan:1844.8505859375\n",
      "Number:7 epoch: 711/1200 loss_tea:0.04728803443044261  loss_houyan:1525.345947265625\n",
      "Number:7 epoch: 721/1200 loss_tea:0.06311895401689213  loss_houyan:1612.4337158203125\n",
      "Number:7 epoch: 731/1200 loss_tea:0.05048357509340693  loss_houyan:1531.8387451171875\n",
      "Number:7 epoch: 741/1200 loss_tea:0.051991261763685695  loss_houyan:1611.318359375\n",
      "Number:7 epoch: 751/1200 loss_tea:0.05852818323296648  loss_houyan:1762.6190185546875\n",
      "Number:7 epoch: 761/1200 loss_tea:0.05821623111168986  loss_houyan:1609.9573974609375\n",
      "Number:7 epoch: 771/1200 loss_tea:0.05161172724771299  loss_houyan:1656.2418212890625\n",
      "Number:7 epoch: 781/1200 loss_tea:0.05494330109432657  loss_houyan:1735.232666015625\n",
      "Number:7 epoch: 791/1200 loss_tea:0.0546577764934969  loss_houyan:1851.6197509765625\n",
      "Number:7 epoch: 801/1200 loss_tea:0.04700907882991918  loss_houyan:1708.659423828125\n",
      "Number:7 epoch: 811/1200 loss_tea:0.05146708291009633  loss_houyan:1602.6368408203125\n",
      "Number:7 epoch: 821/1200 loss_tea:0.05234204977977148  loss_houyan:1588.3214111328125\n",
      "Number:7 epoch: 831/1200 loss_tea:0.0700251251370083  loss_houyan:1564.5235595703125\n",
      "Number:7 epoch: 841/1200 loss_tea:0.04966291585733033  loss_houyan:1840.889892578125\n",
      "Number:7 epoch: 851/1200 loss_tea:0.07667711963139494  loss_houyan:1698.88427734375\n",
      "Number:7 epoch: 861/1200 loss_tea:0.07845259253037515  loss_houyan:1603.63330078125\n",
      "Number:7 epoch: 871/1200 loss_tea:0.07987936930072581  loss_houyan:1883.732177734375\n",
      "Number:7 epoch: 881/1200 loss_tea:0.05898807943546388  loss_houyan:1781.5035400390625\n",
      "Number:7 epoch: 891/1200 loss_tea:0.05947599351913165  loss_houyan:1607.485595703125\n",
      "Number:7 epoch: 901/1200 loss_tea:0.042214804601942164  loss_houyan:1568.368896484375\n",
      "Number:7 epoch: 911/1200 loss_tea:0.059185090747371195  loss_houyan:1947.05224609375\n",
      "Number:7 epoch: 921/1200 loss_tea:0.058936216726035925  loss_houyan:1514.18359375\n",
      "Number:7 epoch: 931/1200 loss_tea:0.06320558983364012  loss_houyan:1627.1259765625\n",
      "Number:7 epoch: 941/1200 loss_tea:0.09455808716455838  loss_houyan:1612.851318359375\n",
      "Number:7 epoch: 951/1200 loss_tea:0.036759008746084856  loss_houyan:1560.8310546875\n",
      "Number:7 epoch: 961/1200 loss_tea:0.05230673296003176  loss_houyan:1566.5927734375\n",
      "Number:7 epoch: 971/1200 loss_tea:0.04592243987740185  loss_houyan:1577.0595703125\n",
      "Number:7 epoch: 981/1200 loss_tea:0.049883918969893665  loss_houyan:1519.5086669921875\n",
      "Number:7 epoch: 991/1200 loss_tea:0.06898947315528636  loss_houyan:1594.2513427734375\n",
      "Number:7 epoch: 1001/1200 loss_tea:0.05372003097587422  loss_houyan:1709.1231689453125\n",
      "Number:7 epoch: 1011/1200 loss_tea:0.04555289484771768  loss_houyan:1546.1578369140625\n",
      "Number:7 epoch: 1021/1200 loss_tea:0.049157558019969634  loss_houyan:1556.9110107421875\n",
      "Number:7 epoch: 1031/1200 loss_tea:0.05068267686211413  loss_houyan:1786.9462890625\n",
      "Number:7 epoch: 1041/1200 loss_tea:0.04855311516742548  loss_houyan:1555.39208984375\n",
      "Number:7 epoch: 1051/1200 loss_tea:0.048584890044608385  loss_houyan:1582.8802490234375\n",
      "Number:7 epoch: 1061/1200 loss_tea:0.05385991725116117  loss_houyan:1551.3941650390625\n",
      "Number:7 epoch: 1071/1200 loss_tea:0.05670975001386624  loss_houyan:1537.8292236328125\n",
      "Number:7 epoch: 1081/1200 loss_tea:0.06233142599756234  loss_houyan:1637.7730712890625\n",
      "Number:7 epoch: 1091/1200 loss_tea:0.04278939814794056  loss_houyan:1535.992919921875\n",
      "Number:7 epoch: 1101/1200 loss_tea:0.058857154977183995  loss_houyan:1504.04296875\n",
      "Number:7 epoch: 1111/1200 loss_tea:0.04359300876462993  loss_houyan:1862.5394287109375\n",
      "Number:7 epoch: 1121/1200 loss_tea:0.058318698673486616  loss_houyan:1587.2574462890625\n",
      "Number:7 epoch: 1131/1200 loss_tea:0.042932080772360354  loss_houyan:1753.5174560546875\n",
      "Number:7 epoch: 1141/1200 loss_tea:0.0796566646921899  loss_houyan:1930.366455078125\n",
      "Number:7 epoch: 1151/1200 loss_tea:0.06448764032998354  loss_houyan:1638.77490234375\n",
      "Number:7 epoch: 1161/1200 loss_tea:0.06544234674723495  loss_houyan:1584.4306640625\n",
      "Number:7 epoch: 1171/1200 loss_tea:0.048497821952327436  loss_houyan:1958.144775390625\n",
      "Number:7 epoch: 1181/1200 loss_tea:0.047020077539378875  loss_houyan:1544.7724609375\n",
      "Number:7 epoch: 1191/1200 loss_tea:0.057570044804686644  loss_houyan:1843.256103515625\n",
      "finished training number 7 techer!\n",
      "start training number 8 techer!\n",
      "Number:8 epoch: 1/1200 loss_tea:0.05828522871942059  loss_houyan:1588.0916748046875\n",
      "Number:8 epoch: 11/1200 loss_tea:0.05480847615014452  loss_houyan:1608.7362060546875\n",
      "Number:8 epoch: 21/1200 loss_tea:0.05609127293373497  loss_houyan:1568.77685546875\n",
      "Number:8 epoch: 31/1200 loss_tea:0.07964360631258352  loss_houyan:2072.302734375\n",
      "Number:8 epoch: 41/1200 loss_tea:0.05244055214695103  loss_houyan:1615.76953125\n",
      "Number:8 epoch: 51/1200 loss_tea:0.07723370969638074  loss_houyan:1765.024658203125\n",
      "Number:8 epoch: 61/1200 loss_tea:0.05320645056714745  loss_houyan:1578.0120849609375\n",
      "Number:8 epoch: 71/1200 loss_tea:0.06230321203271508  loss_houyan:1702.2979736328125\n",
      "Number:8 epoch: 81/1200 loss_tea:0.060466069109769566  loss_houyan:1691.80029296875\n",
      "Number:8 epoch: 91/1200 loss_tea:0.05146584016509944  loss_houyan:1638.3455810546875\n",
      "Number:8 epoch: 101/1200 loss_tea:0.04446166003579815  loss_houyan:1542.9490966796875\n",
      "Number:8 epoch: 111/1200 loss_tea:0.05624421907549979  loss_houyan:1622.444091796875\n",
      "Number:8 epoch: 121/1200 loss_tea:0.05265913492399525  loss_houyan:1552.4844970703125\n",
      "Number:8 epoch: 131/1200 loss_tea:0.0598796764190607  loss_houyan:1568.4686279296875\n",
      "Number:8 epoch: 141/1200 loss_tea:0.07615100356573055  loss_houyan:3247.64453125\n",
      "Number:8 epoch: 151/1200 loss_tea:0.06366999167326445  loss_houyan:1663.681396484375\n",
      "Number:8 epoch: 161/1200 loss_tea:0.04551936602678338  loss_houyan:1621.19775390625\n",
      "Number:8 epoch: 171/1200 loss_tea:0.06416107420951991  loss_houyan:1601.7359619140625\n",
      "Number:8 epoch: 181/1200 loss_tea:0.06420677392642604  loss_houyan:1924.61669921875\n",
      "Number:8 epoch: 191/1200 loss_tea:0.04292589211894936  loss_houyan:1522.4278564453125\n",
      "Number:8 epoch: 201/1200 loss_tea:0.06456092761515218  loss_houyan:1660.174560546875\n",
      "Number:8 epoch: 211/1200 loss_tea:0.04438239900806135  loss_houyan:1503.0953369140625\n",
      "Number:8 epoch: 221/1200 loss_tea:0.048877244277979674  loss_houyan:2098.7177734375\n",
      "Number:8 epoch: 231/1200 loss_tea:0.051607341893999736  loss_houyan:1616.170654296875\n",
      "Number:8 epoch: 241/1200 loss_tea:0.04446261231190453  loss_houyan:1638.5665283203125\n",
      "Number:8 epoch: 251/1200 loss_tea:0.048779428631885224  loss_houyan:1558.5501708984375\n",
      "Number:8 epoch: 261/1200 loss_tea:0.049976405545583205  loss_houyan:1560.9417724609375\n",
      "Number:8 epoch: 271/1200 loss_tea:0.0518524839407173  loss_houyan:1591.21484375\n",
      "Number:8 epoch: 281/1200 loss_tea:0.052963319363014794  loss_houyan:1632.373779296875\n",
      "Number:8 epoch: 291/1200 loss_tea:0.06371990646974553  loss_houyan:1551.6466064453125\n",
      "Number:8 epoch: 301/1200 loss_tea:0.06418664689978076  loss_houyan:1539.49267578125\n",
      "Number:8 epoch: 311/1200 loss_tea:0.04655690915416266  loss_houyan:1757.528564453125\n",
      "Number:8 epoch: 321/1200 loss_tea:0.055913237289668745  loss_houyan:1717.137939453125\n",
      "Number:8 epoch: 331/1200 loss_tea:0.0464312598186211  loss_houyan:1526.048095703125\n",
      "Number:8 epoch: 341/1200 loss_tea:0.05712578935499239  loss_houyan:1667.5479736328125\n",
      "Number:8 epoch: 351/1200 loss_tea:0.050196020949027524  loss_houyan:1831.9090576171875\n",
      "Number:8 epoch: 361/1200 loss_tea:0.07468240542862853  loss_houyan:1655.266845703125\n",
      "Number:8 epoch: 371/1200 loss_tea:0.057830705091806954  loss_houyan:1795.2025146484375\n",
      "Number:8 epoch: 381/1200 loss_tea:0.05075335676363238  loss_houyan:1579.9576416015625\n",
      "Number:8 epoch: 391/1200 loss_tea:0.04637469096399401  loss_houyan:1559.8665771484375\n",
      "Number:8 epoch: 401/1200 loss_tea:0.04858250204071551  loss_houyan:1630.6051025390625\n",
      "Number:8 epoch: 411/1200 loss_tea:0.05776807021878619  loss_houyan:1635.103759765625\n",
      "Number:8 epoch: 421/1200 loss_tea:0.061497891936371965  loss_houyan:1569.867919921875\n",
      "Number:8 epoch: 431/1200 loss_tea:0.04877841853379527  loss_houyan:1832.3687744140625\n",
      "Number:8 epoch: 441/1200 loss_tea:0.04896312786716306  loss_houyan:1691.188232421875\n",
      "Number:8 epoch: 451/1200 loss_tea:0.04716921721392492  loss_houyan:1967.0400390625\n",
      "Number:8 epoch: 461/1200 loss_tea:0.06283479136072818  loss_houyan:1512.776611328125\n",
      "Number:8 epoch: 471/1200 loss_tea:0.0608452049238505  loss_houyan:1639.7203369140625\n",
      "Number:8 epoch: 481/1200 loss_tea:0.06826965694657361  loss_houyan:1572.2913818359375\n",
      "Number:8 epoch: 491/1200 loss_tea:0.04250316504228325  loss_houyan:1862.18115234375\n",
      "Number:8 epoch: 501/1200 loss_tea:0.04991091509455849  loss_houyan:1572.6851806640625\n",
      "Number:8 epoch: 511/1200 loss_tea:0.04148241026745421  loss_houyan:1679.91162109375\n",
      "Number:8 epoch: 521/1200 loss_tea:0.04980774513369018  loss_houyan:1635.9493408203125\n",
      "Number:8 epoch: 531/1200 loss_tea:0.05865641068520033  loss_houyan:1523.0135498046875\n",
      "Number:8 epoch: 541/1200 loss_tea:0.049306618070491345  loss_houyan:2243.953857421875\n",
      "Number:8 epoch: 551/1200 loss_tea:0.04562187576558949  loss_houyan:1709.457763671875\n",
      "Number:8 epoch: 561/1200 loss_tea:0.04817529811063758  loss_houyan:1673.954345703125\n",
      "Number:8 epoch: 571/1200 loss_tea:0.051764833779561516  loss_houyan:1548.171630859375\n",
      "Number:8 epoch: 581/1200 loss_tea:0.05504564024886791  loss_houyan:1673.0355224609375\n",
      "Number:8 epoch: 591/1200 loss_tea:0.0513965710767439  loss_houyan:2064.60107421875\n",
      "Number:8 epoch: 601/1200 loss_tea:0.06636961047281638  loss_houyan:1636.2799072265625\n",
      "Number:8 epoch: 611/1200 loss_tea:0.060777324127515894  loss_houyan:1576.76904296875\n",
      "Number:8 epoch: 621/1200 loss_tea:0.0603129844391952  loss_houyan:1545.98193359375\n",
      "Number:8 epoch: 631/1200 loss_tea:0.04632066569525159  loss_houyan:1616.249755859375\n",
      "Number:8 epoch: 641/1200 loss_tea:0.05823657995798048  loss_houyan:1628.8243408203125\n",
      "Number:8 epoch: 651/1200 loss_tea:0.04310953562088116  loss_houyan:1746.3204345703125\n",
      "Number:8 epoch: 661/1200 loss_tea:0.07271120219318061  loss_houyan:1759.213134765625\n",
      "Number:8 epoch: 671/1200 loss_tea:0.05401773988017433  loss_houyan:1592.569091796875\n",
      "Number:8 epoch: 681/1200 loss_tea:0.05147178470560213  loss_houyan:1600.9932861328125\n",
      "Number:8 epoch: 691/1200 loss_tea:0.07553931478699957  loss_houyan:1702.043701171875\n",
      "Number:8 epoch: 701/1200 loss_tea:0.0457816282956917  loss_houyan:2015.8580322265625\n",
      "Number:8 epoch: 711/1200 loss_tea:0.06814161559058655  loss_houyan:1535.3677978515625\n",
      "Number:8 epoch: 721/1200 loss_tea:0.051579282301198334  loss_houyan:1658.2486572265625\n",
      "Number:8 epoch: 731/1200 loss_tea:0.050796978956238206  loss_houyan:1600.728271484375\n",
      "Number:8 epoch: 741/1200 loss_tea:0.04903442083246944  loss_houyan:1591.8814697265625\n",
      "Number:8 epoch: 751/1200 loss_tea:0.04619533419971261  loss_houyan:1527.82421875\n",
      "Number:8 epoch: 761/1200 loss_tea:0.065158867710137  loss_houyan:1931.01123046875\n",
      "Number:8 epoch: 771/1200 loss_tea:0.06099059235652449  loss_houyan:1605.873291015625\n",
      "Number:8 epoch: 781/1200 loss_tea:0.0772320957383164  loss_houyan:1590.650634765625\n",
      "Number:8 epoch: 791/1200 loss_tea:0.06849429058278889  loss_houyan:1527.759765625\n",
      "Number:8 epoch: 801/1200 loss_tea:0.06385290368724406  loss_houyan:1552.87841796875\n",
      "Number:8 epoch: 811/1200 loss_tea:0.046750353427793614  loss_houyan:1505.120849609375\n",
      "Number:8 epoch: 821/1200 loss_tea:0.057302022439967  loss_houyan:1717.8870849609375\n",
      "Number:8 epoch: 831/1200 loss_tea:0.054437987295111255  loss_houyan:1545.09912109375\n",
      "Number:8 epoch: 841/1200 loss_tea:0.05007862315639772  loss_houyan:1557.984375\n",
      "Number:8 epoch: 851/1200 loss_tea:0.04683691947061643  loss_houyan:1590.3052978515625\n",
      "Number:8 epoch: 861/1200 loss_tea:0.05092948152845341  loss_houyan:1702.740478515625\n",
      "Number:8 epoch: 871/1200 loss_tea:0.05331183540890797  loss_houyan:1703.571044921875\n",
      "Number:8 epoch: 881/1200 loss_tea:0.0547794990327548  loss_houyan:1536.4617919921875\n",
      "Number:8 epoch: 891/1200 loss_tea:0.05471848078782872  loss_houyan:1760.0616455078125\n",
      "Number:8 epoch: 901/1200 loss_tea:0.06743209302751357  loss_houyan:2661.757568359375\n",
      "Number:8 epoch: 911/1200 loss_tea:0.045743614125824034  loss_houyan:1542.264404296875\n",
      "Number:8 epoch: 921/1200 loss_tea:0.06729426269368746  loss_houyan:1921.065673828125\n",
      "Number:8 epoch: 931/1200 loss_tea:0.05303186125236893  loss_houyan:1588.3048095703125\n",
      "Number:8 epoch: 941/1200 loss_tea:0.03774084388705697  loss_houyan:1704.885498046875\n",
      "Number:8 epoch: 951/1200 loss_tea:0.05020224876967439  loss_houyan:1595.9649658203125\n",
      "Number:8 epoch: 961/1200 loss_tea:0.06179373899234411  loss_houyan:1565.364990234375\n",
      "Number:8 epoch: 971/1200 loss_tea:0.047634018586972283  loss_houyan:1606.9556884765625\n",
      "Number:8 epoch: 981/1200 loss_tea:0.05412200936571847  loss_houyan:1767.456787109375\n",
      "Number:8 epoch: 991/1200 loss_tea:0.050737467898346045  loss_houyan:1552.307373046875\n",
      "Number:8 epoch: 1001/1200 loss_tea:0.05962369780242745  loss_houyan:1642.6141357421875\n",
      "Number:8 epoch: 1011/1200 loss_tea:0.05952023347005541  loss_houyan:1855.1650390625\n",
      "Number:8 epoch: 1021/1200 loss_tea:0.07755596959216043  loss_houyan:1725.30078125\n",
      "Number:8 epoch: 1031/1200 loss_tea:0.043165379553785785  loss_houyan:1556.072509765625\n",
      "Number:8 epoch: 1041/1200 loss_tea:0.04856808033200418  loss_houyan:1767.64501953125\n",
      "Number:8 epoch: 1051/1200 loss_tea:0.04412520345974263  loss_houyan:1610.5767822265625\n",
      "Number:8 epoch: 1061/1200 loss_tea:0.06684049340453567  loss_houyan:1533.6585693359375\n",
      "Number:8 epoch: 1071/1200 loss_tea:0.044179127711017216  loss_houyan:1883.234619140625\n",
      "Number:8 epoch: 1081/1200 loss_tea:0.06294761119564704  loss_houyan:1538.3162841796875\n",
      "Number:8 epoch: 1091/1200 loss_tea:0.06976226067628145  loss_houyan:2125.271728515625\n",
      "Number:8 epoch: 1101/1200 loss_tea:0.04177661120879498  loss_houyan:1588.380126953125\n",
      "Number:8 epoch: 1111/1200 loss_tea:0.04268552730090468  loss_houyan:1862.08203125\n",
      "Number:8 epoch: 1121/1200 loss_tea:0.04310947369104253  loss_houyan:1585.8441162109375\n",
      "Number:8 epoch: 1131/1200 loss_tea:0.0593150782713722  loss_houyan:1582.0443115234375\n",
      "Number:8 epoch: 1141/1200 loss_tea:0.04656994819840318  loss_houyan:1488.05810546875\n",
      "Number:8 epoch: 1151/1200 loss_tea:0.05471757974597107  loss_houyan:1515.3946533203125\n",
      "Number:8 epoch: 1161/1200 loss_tea:0.050142554788483715  loss_houyan:1573.8992919921875\n",
      "Number:8 epoch: 1171/1200 loss_tea:0.05379801626636573  loss_houyan:1588.268798828125\n",
      "Number:8 epoch: 1181/1200 loss_tea:0.03653311814168868  loss_houyan:1563.1456298828125\n",
      "Number:8 epoch: 1191/1200 loss_tea:0.06445215648186939  loss_houyan:1575.0267333984375\n",
      "finished training number 8 techer!\n",
      "start training number 9 techer!\n",
      "Number:9 epoch: 1/1200 loss_tea:0.05377281680084419  loss_houyan:1789.5762939453125\n",
      "Number:9 epoch: 11/1200 loss_tea:0.06630245961681995  loss_houyan:1515.198974609375\n",
      "Number:9 epoch: 21/1200 loss_tea:0.05319786725231141  loss_houyan:1576.5601806640625\n",
      "Number:9 epoch: 31/1200 loss_tea:0.07132893875512923  loss_houyan:1690.7852783203125\n",
      "Number:9 epoch: 41/1200 loss_tea:0.05432962275496711  loss_houyan:1576.1827392578125\n",
      "Number:9 epoch: 51/1200 loss_tea:0.053888416523293056  loss_houyan:1725.109375\n",
      "Number:9 epoch: 61/1200 loss_tea:0.06699281839017374  loss_houyan:1533.4451904296875\n",
      "Number:9 epoch: 71/1200 loss_tea:0.05030442430893008  loss_houyan:1572.281494140625\n",
      "Number:9 epoch: 81/1200 loss_tea:0.061270910409724925  loss_houyan:1785.597900390625\n",
      "Number:9 epoch: 91/1200 loss_tea:0.0390228227351343  loss_houyan:1631.369140625\n",
      "Number:9 epoch: 101/1200 loss_tea:0.06008128385861827  loss_houyan:1688.3468017578125\n",
      "Number:9 epoch: 111/1200 loss_tea:0.05311705335312185  loss_houyan:1938.0179443359375\n",
      "Number:9 epoch: 121/1200 loss_tea:0.04977842078672287  loss_houyan:1793.2471923828125\n",
      "Number:9 epoch: 131/1200 loss_tea:0.06082141474840103  loss_houyan:1756.013427734375\n",
      "Number:9 epoch: 141/1200 loss_tea:0.04342990809723701  loss_houyan:1638.21240234375\n",
      "Number:9 epoch: 151/1200 loss_tea:0.04320876630381918  loss_houyan:1648.6417236328125\n",
      "Number:9 epoch: 161/1200 loss_tea:0.06020671822559273  loss_houyan:1529.883544921875\n",
      "Number:9 epoch: 171/1200 loss_tea:0.064297265542759  loss_houyan:1561.3712158203125\n",
      "Number:9 epoch: 181/1200 loss_tea:0.05835863094756496  loss_houyan:1653.8946533203125\n",
      "Number:9 epoch: 191/1200 loss_tea:0.04426267478522234  loss_houyan:1595.8050537109375\n",
      "Number:9 epoch: 201/1200 loss_tea:0.05277891586111556  loss_houyan:1547.135498046875\n",
      "Number:9 epoch: 211/1200 loss_tea:0.04363523594128088  loss_houyan:1577.8995361328125\n",
      "Number:9 epoch: 221/1200 loss_tea:0.05484664618534334  loss_houyan:1593.7830810546875\n",
      "Number:9 epoch: 231/1200 loss_tea:0.05220750167645953  loss_houyan:1545.57861328125\n",
      "Number:9 epoch: 241/1200 loss_tea:0.052511515991876596  loss_houyan:1689.0751953125\n",
      "Number:9 epoch: 251/1200 loss_tea:0.05466600668974823  loss_houyan:1591.2041015625\n",
      "Number:9 epoch: 261/1200 loss_tea:0.05524846090312088  loss_houyan:1739.2989501953125\n",
      "Number:9 epoch: 271/1200 loss_tea:0.05846497340888805  loss_houyan:1581.0841064453125\n",
      "Number:9 epoch: 281/1200 loss_tea:0.04901609514553778  loss_houyan:1613.170654296875\n",
      "Number:9 epoch: 291/1200 loss_tea:0.04363905195526346  loss_houyan:1633.3785400390625\n",
      "Number:9 epoch: 301/1200 loss_tea:0.04719881292946406  loss_houyan:1573.06640625\n",
      "Number:9 epoch: 311/1200 loss_tea:0.048441227837108226  loss_houyan:1799.7154541015625\n",
      "Number:9 epoch: 321/1200 loss_tea:0.06433052199136059  loss_houyan:1666.9093017578125\n",
      "Number:9 epoch: 331/1200 loss_tea:0.053452106265523595  loss_houyan:1579.7213134765625\n",
      "Number:9 epoch: 341/1200 loss_tea:0.0792380251188087  loss_houyan:1629.701904296875\n",
      "Number:9 epoch: 351/1200 loss_tea:0.0747371818191197  loss_houyan:1562.4569091796875\n",
      "Number:9 epoch: 361/1200 loss_tea:0.04679942740580561  loss_houyan:1615.0701904296875\n",
      "Number:9 epoch: 371/1200 loss_tea:0.06894082823156683  loss_houyan:1559.377685546875\n",
      "Number:9 epoch: 381/1200 loss_tea:0.05007745557216973  loss_houyan:1576.9373779296875\n",
      "Number:9 epoch: 391/1200 loss_tea:0.06547259849643138  loss_houyan:1580.43701171875\n",
      "Number:9 epoch: 401/1200 loss_tea:0.05229432094922934  loss_houyan:1543.3446044921875\n",
      "Number:9 epoch: 411/1200 loss_tea:0.06525173358481882  loss_houyan:1678.4083251953125\n",
      "Number:9 epoch: 421/1200 loss_tea:0.06622644551724637  loss_houyan:1703.379638671875\n",
      "Number:9 epoch: 431/1200 loss_tea:0.05565277168009236  loss_houyan:1795.1405029296875\n",
      "Number:9 epoch: 441/1200 loss_tea:0.07588437345223749  loss_houyan:1531.74658203125\n",
      "Number:9 epoch: 451/1200 loss_tea:0.07689952795786704  loss_houyan:1786.7032470703125\n",
      "Number:9 epoch: 461/1200 loss_tea:0.04376671977638474  loss_houyan:1562.5140380859375\n",
      "Number:9 epoch: 471/1200 loss_tea:0.05543258357080028  loss_houyan:1647.994140625\n",
      "Number:9 epoch: 481/1200 loss_tea:0.06897634937257595  loss_houyan:1957.022705078125\n",
      "Number:9 epoch: 491/1200 loss_tea:0.04893729617830131  loss_houyan:1551.6683349609375\n",
      "Number:9 epoch: 501/1200 loss_tea:0.05636951637676739  loss_houyan:1621.61865234375\n",
      "Number:9 epoch: 511/1200 loss_tea:0.06843919629796642  loss_houyan:1524.712158203125\n",
      "Number:9 epoch: 521/1200 loss_tea:0.06619454115472247  loss_houyan:1542.2158203125\n",
      "Number:9 epoch: 531/1200 loss_tea:0.0650685468735013  loss_houyan:1647.7828369140625\n",
      "Number:9 epoch: 541/1200 loss_tea:0.07235698465877381  loss_houyan:1530.349853515625\n",
      "Number:9 epoch: 551/1200 loss_tea:0.04829912605501692  loss_houyan:1688.5035400390625\n",
      "Number:9 epoch: 561/1200 loss_tea:0.04898242110185399  loss_houyan:1585.7681884765625\n",
      "Number:9 epoch: 571/1200 loss_tea:0.03872587377345463  loss_houyan:1597.324462890625\n",
      "Number:9 epoch: 581/1200 loss_tea:0.053849243601387584  loss_houyan:1509.0855712890625\n",
      "Number:9 epoch: 591/1200 loss_tea:0.04698570114265201  loss_houyan:1498.1087646484375\n",
      "Number:9 epoch: 601/1200 loss_tea:0.05228825964460688  loss_houyan:1570.78271484375\n",
      "Number:9 epoch: 611/1200 loss_tea:0.05720041779182883  loss_houyan:1530.67431640625\n",
      "Number:9 epoch: 621/1200 loss_tea:0.045962774268535796  loss_houyan:1520.5499267578125\n",
      "Number:9 epoch: 631/1200 loss_tea:0.04455046494809636  loss_houyan:1624.2095947265625\n",
      "Number:9 epoch: 641/1200 loss_tea:0.048528967943991  loss_houyan:1968.7681884765625\n",
      "Number:9 epoch: 651/1200 loss_tea:0.053685662039914595  loss_houyan:1585.9078369140625\n",
      "Number:9 epoch: 661/1200 loss_tea:0.04276067505354627  loss_houyan:1605.784912109375\n",
      "Number:9 epoch: 671/1200 loss_tea:0.04938002237670934  loss_houyan:1773.2386474609375\n",
      "Number:9 epoch: 681/1200 loss_tea:0.06779157037867306  loss_houyan:1590.4537353515625\n",
      "Number:9 epoch: 691/1200 loss_tea:0.04681944159622228  loss_houyan:1574.9112548828125\n",
      "Number:9 epoch: 701/1200 loss_tea:0.04372430637671997  loss_houyan:1993.3037109375\n",
      "Number:9 epoch: 711/1200 loss_tea:0.07117724024077791  loss_houyan:1589.90576171875\n",
      "Number:9 epoch: 721/1200 loss_tea:0.05334838631575905  loss_houyan:1528.8697509765625\n",
      "Number:9 epoch: 731/1200 loss_tea:0.04591922421089087  loss_houyan:1908.11572265625\n",
      "Number:9 epoch: 741/1200 loss_tea:0.05040880580143526  loss_houyan:1554.993408203125\n",
      "Number:9 epoch: 751/1200 loss_tea:0.06742351465395027  loss_houyan:1559.4378662109375\n",
      "Number:9 epoch: 761/1200 loss_tea:0.042788501249025265  loss_houyan:1625.976806640625\n",
      "Number:9 epoch: 771/1200 loss_tea:0.058108593850153795  loss_houyan:1747.84033203125\n",
      "Number:9 epoch: 781/1200 loss_tea:0.07003218557324177  loss_houyan:1640.084228515625\n",
      "Number:9 epoch: 791/1200 loss_tea:0.05770490671786638  loss_houyan:1596.9227294921875\n",
      "Number:9 epoch: 801/1200 loss_tea:0.09973220269714311  loss_houyan:1527.6539306640625\n",
      "Number:9 epoch: 811/1200 loss_tea:0.03913595727370388  loss_houyan:1653.5286865234375\n",
      "Number:9 epoch: 821/1200 loss_tea:0.055734142627139216  loss_houyan:1524.762939453125\n",
      "Number:9 epoch: 831/1200 loss_tea:0.05437022107729719  loss_houyan:1555.735595703125\n",
      "Number:9 epoch: 841/1200 loss_tea:0.057515025335796596  loss_houyan:1670.1409912109375\n",
      "Number:9 epoch: 851/1200 loss_tea:0.05112830283116879  loss_houyan:1716.71044921875\n",
      "Number:9 epoch: 861/1200 loss_tea:0.0672626772181259  loss_houyan:1590.124755859375\n",
      "Number:9 epoch: 871/1200 loss_tea:0.04223958585212859  loss_houyan:1587.55078125\n",
      "Number:9 epoch: 881/1200 loss_tea:0.08585908865385966  loss_houyan:1927.7994384765625\n",
      "Number:9 epoch: 891/1200 loss_tea:0.05255398497887896  loss_houyan:1550.798828125\n",
      "Number:9 epoch: 901/1200 loss_tea:0.06579665288688298  loss_houyan:1665.32470703125\n",
      "Number:9 epoch: 911/1200 loss_tea:0.05721083293686469  loss_houyan:3041.125\n",
      "Number:9 epoch: 921/1200 loss_tea:0.05855053555652732  loss_houyan:1572.3897705078125\n",
      "Number:9 epoch: 931/1200 loss_tea:0.05230488917072196  loss_houyan:1691.6036376953125\n",
      "Number:9 epoch: 941/1200 loss_tea:0.05672076268807123  loss_houyan:1630.7562255859375\n",
      "Number:9 epoch: 951/1200 loss_tea:0.04426536109907402  loss_houyan:1640.6983642578125\n",
      "Number:9 epoch: 961/1200 loss_tea:0.035264645464128275  loss_houyan:1665.7108154296875\n",
      "Number:9 epoch: 971/1200 loss_tea:0.04724064117332617  loss_houyan:1831.80712890625\n",
      "Number:9 epoch: 981/1200 loss_tea:0.06280667061378158  loss_houyan:1590.9453125\n",
      "Number:9 epoch: 991/1200 loss_tea:0.05473077393870545  loss_houyan:1585.790771484375\n",
      "Number:9 epoch: 1001/1200 loss_tea:0.04347463353264059  loss_houyan:1547.3795166015625\n",
      "Number:9 epoch: 1011/1200 loss_tea:0.043564663337102565  loss_houyan:1721.22119140625\n",
      "Number:9 epoch: 1021/1200 loss_tea:0.05384861890785736  loss_houyan:1725.407958984375\n",
      "Number:9 epoch: 1031/1200 loss_tea:0.048304025419561976  loss_houyan:1698.081787109375\n",
      "Number:9 epoch: 1041/1200 loss_tea:0.05081335133885188  loss_houyan:1679.1204833984375\n",
      "Number:9 epoch: 1051/1200 loss_tea:0.06371738209774168  loss_houyan:1535.1766357421875\n",
      "Number:9 epoch: 1061/1200 loss_tea:0.04008649082328835  loss_houyan:1562.21337890625\n",
      "Number:9 epoch: 1071/1200 loss_tea:0.04922824293670805  loss_houyan:1602.695556640625\n",
      "Number:9 epoch: 1081/1200 loss_tea:0.056541655569550285  loss_houyan:1534.891845703125\n",
      "Number:9 epoch: 1091/1200 loss_tea:0.07115024362111806  loss_houyan:1576.6766357421875\n",
      "Number:9 epoch: 1101/1200 loss_tea:0.045998308856658225  loss_houyan:1536.1912841796875\n",
      "Number:9 epoch: 1111/1200 loss_tea:0.05499497692332544  loss_houyan:1613.254150390625\n",
      "Number:9 epoch: 1121/1200 loss_tea:0.04874856641083346  loss_houyan:1590.669677734375\n",
      "Number:9 epoch: 1131/1200 loss_tea:0.05494822263310186  loss_houyan:1596.9512939453125\n",
      "Number:9 epoch: 1141/1200 loss_tea:0.04897050303496618  loss_houyan:1733.9708251953125\n",
      "Number:9 epoch: 1151/1200 loss_tea:0.04985758330554513  loss_houyan:1593.6055908203125\n",
      "Number:9 epoch: 1161/1200 loss_tea:0.055431020986956576  loss_houyan:1689.8831787109375\n",
      "Number:9 epoch: 1171/1200 loss_tea:0.05259204730738602  loss_houyan:1569.25732421875\n",
      "Number:9 epoch: 1181/1200 loss_tea:0.056542753414185826  loss_houyan:1507.7989501953125\n",
      "Number:9 epoch: 1191/1200 loss_tea:0.04973541137305667  loss_houyan:1783.86328125\n",
      "finished training number 9 techer!\n",
      "start training number 10 techer!\n",
      "Number:10 epoch: 1/1200 loss_tea:0.04671254412323713  loss_houyan:1579.4072265625\n",
      "Number:10 epoch: 11/1200 loss_tea:0.039782380450917615  loss_houyan:1502.4732666015625\n",
      "Number:10 epoch: 21/1200 loss_tea:0.061651727166166424  loss_houyan:2325.56591796875\n",
      "Number:10 epoch: 31/1200 loss_tea:0.05006827818181125  loss_houyan:1507.88916015625\n",
      "Number:10 epoch: 41/1200 loss_tea:0.0699566707934853  loss_houyan:1585.701171875\n",
      "Number:10 epoch: 51/1200 loss_tea:0.061517494572381246  loss_houyan:1590.7037353515625\n",
      "Number:10 epoch: 61/1200 loss_tea:0.05838438718213179  loss_houyan:1608.862548828125\n",
      "Number:10 epoch: 71/1200 loss_tea:0.048218160629248316  loss_houyan:1600.4122314453125\n",
      "Number:10 epoch: 81/1200 loss_tea:0.06137988666713491  loss_houyan:1595.56201171875\n",
      "Number:10 epoch: 91/1200 loss_tea:0.05480104459554156  loss_houyan:1874.161865234375\n",
      "Number:10 epoch: 101/1200 loss_tea:0.05310796225563836  loss_houyan:1625.3729248046875\n",
      "Number:10 epoch: 111/1200 loss_tea:0.047305324466675305  loss_houyan:1664.8133544921875\n",
      "Number:10 epoch: 121/1200 loss_tea:0.053450802793465625  loss_houyan:2296.45654296875\n",
      "Number:10 epoch: 131/1200 loss_tea:0.06903508090121147  loss_houyan:1924.69287109375\n",
      "Number:10 epoch: 141/1200 loss_tea:0.039313503970966085  loss_houyan:1618.7822265625\n",
      "Number:10 epoch: 151/1200 loss_tea:0.04743098197265303  loss_houyan:1691.6431884765625\n",
      "Number:10 epoch: 161/1200 loss_tea:0.05036511010621841  loss_houyan:1539.5758056640625\n",
      "Number:10 epoch: 171/1200 loss_tea:0.05721756463075471  loss_houyan:1589.651123046875\n",
      "Number:10 epoch: 181/1200 loss_tea:0.04299969703761918  loss_houyan:1543.1307373046875\n",
      "Number:10 epoch: 191/1200 loss_tea:0.05735866373666317  loss_houyan:1622.9725341796875\n",
      "Number:10 epoch: 201/1200 loss_tea:0.05515021331406826  loss_houyan:1548.2862548828125\n",
      "Number:10 epoch: 211/1200 loss_tea:0.06066772103889138  loss_houyan:1569.5096435546875\n",
      "Number:10 epoch: 221/1200 loss_tea:0.044347700809862584  loss_houyan:1533.46630859375\n",
      "Number:10 epoch: 231/1200 loss_tea:0.04482451195366256  loss_houyan:1670.5565185546875\n",
      "Number:10 epoch: 241/1200 loss_tea:0.07620016596260608  loss_houyan:1496.55126953125\n",
      "Number:10 epoch: 251/1200 loss_tea:0.042454894511370564  loss_houyan:1659.05078125\n",
      "Number:10 epoch: 261/1200 loss_tea:0.05841667247954616  loss_houyan:1613.2872314453125\n",
      "Number:10 epoch: 271/1200 loss_tea:0.05210405152312942  loss_houyan:1544.107177734375\n",
      "Number:10 epoch: 281/1200 loss_tea:0.044071945478184785  loss_houyan:1605.2998046875\n",
      "Number:10 epoch: 291/1200 loss_tea:0.041897820061684285  loss_houyan:1599.43798828125\n",
      "Number:10 epoch: 301/1200 loss_tea:0.047305133808702264  loss_houyan:1536.633056640625\n",
      "Number:10 epoch: 311/1200 loss_tea:0.049340230465034905  loss_houyan:1532.7352294921875\n",
      "Number:10 epoch: 321/1200 loss_tea:0.050745635207592295  loss_houyan:1766.2647705078125\n",
      "Number:10 epoch: 331/1200 loss_tea:0.04407097980853925  loss_houyan:1542.367431640625\n",
      "Number:10 epoch: 341/1200 loss_tea:0.05393001857329313  loss_houyan:1575.0286865234375\n",
      "Number:10 epoch: 351/1200 loss_tea:0.08523218441624039  loss_houyan:1511.6849365234375\n",
      "Number:10 epoch: 361/1200 loss_tea:0.040552928787972106  loss_houyan:1613.2220458984375\n",
      "Number:10 epoch: 371/1200 loss_tea:0.058539006339545695  loss_houyan:1529.8173828125\n",
      "Number:10 epoch: 381/1200 loss_tea:0.05842604686962416  loss_houyan:1763.01025390625\n",
      "Number:10 epoch: 391/1200 loss_tea:0.0680901473088136  loss_houyan:1711.5928955078125\n",
      "Number:10 epoch: 401/1200 loss_tea:0.05826985444107687  loss_houyan:1600.860595703125\n",
      "Number:10 epoch: 411/1200 loss_tea:0.05094684443497117  loss_houyan:1715.057373046875\n",
      "Number:10 epoch: 421/1200 loss_tea:0.05320744216555293  loss_houyan:1546.1505126953125\n",
      "Number:10 epoch: 431/1200 loss_tea:0.05073402312912269  loss_houyan:1561.80126953125\n",
      "Number:10 epoch: 441/1200 loss_tea:0.056404800029576777  loss_houyan:1757.66650390625\n",
      "Number:10 epoch: 451/1200 loss_tea:0.05080315914681886  loss_houyan:1604.5389404296875\n",
      "Number:10 epoch: 461/1200 loss_tea:0.059729727300128065  loss_houyan:1523.13525390625\n",
      "Number:10 epoch: 471/1200 loss_tea:0.05546375526900424  loss_houyan:1574.5169677734375\n",
      "Number:10 epoch: 481/1200 loss_tea:0.09495267874440709  loss_houyan:2351.978515625\n",
      "Number:10 epoch: 491/1200 loss_tea:0.04351567711091812  loss_houyan:1688.5546875\n",
      "Number:10 epoch: 501/1200 loss_tea:0.05283606817150903  loss_houyan:1684.93017578125\n",
      "Number:10 epoch: 511/1200 loss_tea:0.07267177192781385  loss_houyan:1633.3001708984375\n",
      "Number:10 epoch: 521/1200 loss_tea:0.062177680974574566  loss_houyan:2117.900146484375\n",
      "Number:10 epoch: 531/1200 loss_tea:0.05612329186949745  loss_houyan:1756.3070068359375\n",
      "Number:10 epoch: 541/1200 loss_tea:0.04968811714512775  loss_houyan:1615.5806884765625\n",
      "Number:10 epoch: 551/1200 loss_tea:0.06048293834137535  loss_houyan:1560.1136474609375\n",
      "Number:10 epoch: 561/1200 loss_tea:0.06296710075132575  loss_houyan:1541.458984375\n",
      "Number:10 epoch: 571/1200 loss_tea:0.05107276956687542  loss_houyan:1528.76953125\n",
      "Number:10 epoch: 581/1200 loss_tea:0.05615460172601924  loss_houyan:1666.0357666015625\n",
      "Number:10 epoch: 591/1200 loss_tea:0.05556674459932158  loss_houyan:1656.248779296875\n",
      "Number:10 epoch: 601/1200 loss_tea:0.04739817428835157  loss_houyan:1532.29296875\n",
      "Number:10 epoch: 611/1200 loss_tea:0.04134926305854033  loss_houyan:1537.203125\n",
      "Number:10 epoch: 621/1200 loss_tea:0.0537948487332162  loss_houyan:1558.8447265625\n",
      "Number:10 epoch: 631/1200 loss_tea:0.05576441609650603  loss_houyan:1551.041748046875\n",
      "Number:10 epoch: 641/1200 loss_tea:0.060969188658859  loss_houyan:1818.62939453125\n",
      "Number:10 epoch: 651/1200 loss_tea:0.03792978392818485  loss_houyan:1522.0594482421875\n",
      "Number:10 epoch: 661/1200 loss_tea:0.03934644684965175  loss_houyan:1601.687255859375\n",
      "Number:10 epoch: 671/1200 loss_tea:0.044171631196617524  loss_houyan:1587.044677734375\n",
      "Number:10 epoch: 681/1200 loss_tea:0.06659169597877576  loss_houyan:1698.7646484375\n",
      "Number:10 epoch: 691/1200 loss_tea:0.039898476703177345  loss_houyan:1983.134765625\n",
      "Number:10 epoch: 701/1200 loss_tea:0.046153890993314066  loss_houyan:2596.107177734375\n",
      "Number:10 epoch: 711/1200 loss_tea:0.04735306757548475  loss_houyan:1765.723388671875\n",
      "Number:10 epoch: 721/1200 loss_tea:0.06473973451843623  loss_houyan:1547.43896484375\n",
      "Number:10 epoch: 731/1200 loss_tea:0.046439904334008124  loss_houyan:1524.8892822265625\n",
      "Number:10 epoch: 741/1200 loss_tea:0.06665687283837943  loss_houyan:1908.73193359375\n",
      "Number:10 epoch: 751/1200 loss_tea:0.0830647363060516  loss_houyan:1801.440185546875\n",
      "Number:10 epoch: 761/1200 loss_tea:0.044086730074086894  loss_houyan:1584.1298828125\n",
      "Number:10 epoch: 771/1200 loss_tea:0.04947989719757817  loss_houyan:1642.3515625\n",
      "Number:10 epoch: 781/1200 loss_tea:0.06034452471778729  loss_houyan:1511.0958251953125\n",
      "Number:10 epoch: 791/1200 loss_tea:0.044371080251872023  loss_houyan:1590.0367431640625\n",
      "Number:10 epoch: 801/1200 loss_tea:0.07674393839402215  loss_houyan:1554.10400390625\n",
      "Number:10 epoch: 811/1200 loss_tea:0.05980990584717121  loss_houyan:1947.8736572265625\n",
      "Number:10 epoch: 821/1200 loss_tea:0.05874513471919756  loss_houyan:2301.48388671875\n",
      "Number:10 epoch: 831/1200 loss_tea:0.05152231041950955  loss_houyan:1608.9825439453125\n",
      "Number:10 epoch: 841/1200 loss_tea:0.03729291596175061  loss_houyan:1576.9405517578125\n",
      "Number:10 epoch: 851/1200 loss_tea:0.04592924978729682  loss_houyan:1569.1309814453125\n",
      "Number:10 epoch: 861/1200 loss_tea:0.058931375213279784  loss_houyan:1663.6263427734375\n",
      "Number:10 epoch: 871/1200 loss_tea:0.052235551469717666  loss_houyan:1772.669189453125\n",
      "Number:10 epoch: 881/1200 loss_tea:0.05207424020896092  loss_houyan:1542.3121337890625\n",
      "Number:10 epoch: 891/1200 loss_tea:0.0618655444269048  loss_houyan:1769.5006103515625\n",
      "Number:10 epoch: 901/1200 loss_tea:0.04876229297071665  loss_houyan:1629.606689453125\n",
      "Number:10 epoch: 911/1200 loss_tea:0.05056341939883264  loss_houyan:1584.94970703125\n",
      "Number:10 epoch: 921/1200 loss_tea:0.07032279178865385  loss_houyan:1674.4786376953125\n",
      "Number:10 epoch: 931/1200 loss_tea:0.0594097436363274  loss_houyan:1546.8438720703125\n",
      "Number:10 epoch: 941/1200 loss_tea:0.06061369973878666  loss_houyan:1593.962158203125\n",
      "Number:10 epoch: 951/1200 loss_tea:0.04128804683824143  loss_houyan:1635.24560546875\n",
      "Number:10 epoch: 961/1200 loss_tea:0.04856164391849213  loss_houyan:1562.32763671875\n",
      "Number:10 epoch: 971/1200 loss_tea:0.04075477130036497  loss_houyan:1720.337646484375\n",
      "Number:10 epoch: 981/1200 loss_tea:0.05101289212287185  loss_houyan:1602.890869140625\n",
      "Number:10 epoch: 991/1200 loss_tea:0.05128847631832016  loss_houyan:1614.063232421875\n",
      "Number:10 epoch: 1001/1200 loss_tea:0.04649013399410784  loss_houyan:1625.6768798828125\n",
      "Number:10 epoch: 1011/1200 loss_tea:0.049626137340674885  loss_houyan:1555.7603759765625\n",
      "Number:10 epoch: 1021/1200 loss_tea:0.05118370714548546  loss_houyan:1547.4527587890625\n",
      "Number:10 epoch: 1031/1200 loss_tea:0.08266880636378679  loss_houyan:1565.01123046875\n",
      "Number:10 epoch: 1041/1200 loss_tea:0.06831973088923703  loss_houyan:1787.392333984375\n",
      "Number:10 epoch: 1051/1200 loss_tea:0.05040876471873464  loss_houyan:1556.6492919921875\n",
      "Number:10 epoch: 1061/1200 loss_tea:0.055919305946400995  loss_houyan:1555.26953125\n",
      "Number:10 epoch: 1071/1200 loss_tea:0.06550026219470227  loss_houyan:1549.6485595703125\n",
      "Number:10 epoch: 1081/1200 loss_tea:0.039415933761850576  loss_houyan:1572.10791015625\n",
      "Number:10 epoch: 1091/1200 loss_tea:0.045888830855389175  loss_houyan:1668.5562744140625\n",
      "Number:10 epoch: 1101/1200 loss_tea:0.05745283044355527  loss_houyan:1618.088623046875\n",
      "Number:10 epoch: 1111/1200 loss_tea:0.04385130494683589  loss_houyan:1517.403564453125\n",
      "Number:10 epoch: 1121/1200 loss_tea:0.07913215151284904  loss_houyan:1790.118896484375\n",
      "Number:10 epoch: 1131/1200 loss_tea:0.05210544784599189  loss_houyan:1522.2691650390625\n",
      "Number:10 epoch: 1141/1200 loss_tea:0.07206935123671566  loss_houyan:1529.125244140625\n",
      "Number:10 epoch: 1151/1200 loss_tea:0.06100351301168826  loss_houyan:1564.3509521484375\n",
      "Number:10 epoch: 1161/1200 loss_tea:0.0700312531927662  loss_houyan:1533.3671875\n",
      "Number:10 epoch: 1171/1200 loss_tea:0.04933727146438532  loss_houyan:2696.542236328125\n",
      "Number:10 epoch: 1181/1200 loss_tea:0.053505877959105275  loss_houyan:1609.7406005859375\n",
      "Number:10 epoch: 1191/1200 loss_tea:0.045104690090030077  loss_houyan:1595.92578125\n",
      "finished training number 10 techer!\n",
      "start training number 11 techer!\n",
      "Number:11 epoch: 1/1200 loss_tea:0.05456623241172548  loss_houyan:1538.9482421875\n",
      "Number:11 epoch: 11/1200 loss_tea:0.06881558203287626  loss_houyan:1548.1859130859375\n",
      "Number:11 epoch: 21/1200 loss_tea:0.05235690452009047  loss_houyan:1573.6573486328125\n",
      "Number:11 epoch: 31/1200 loss_tea:0.07576872089057889  loss_houyan:1593.346923828125\n",
      "Number:11 epoch: 41/1200 loss_tea:0.052703987174709484  loss_houyan:1779.191650390625\n",
      "Number:11 epoch: 51/1200 loss_tea:0.059317131070067385  loss_houyan:1548.652099609375\n",
      "Number:11 epoch: 61/1200 loss_tea:0.054665108113810916  loss_houyan:1676.85546875\n",
      "Number:11 epoch: 71/1200 loss_tea:0.08311066793869062  loss_houyan:1980.283935546875\n",
      "Number:11 epoch: 81/1200 loss_tea:0.06045937293554538  loss_houyan:1893.15869140625\n",
      "Number:11 epoch: 91/1200 loss_tea:0.06029382037934152  loss_houyan:1767.013671875\n",
      "Number:11 epoch: 101/1200 loss_tea:0.04796612184326466  loss_houyan:1840.7647705078125\n",
      "Number:11 epoch: 111/1200 loss_tea:0.0579941119136225  loss_houyan:1666.0040283203125\n",
      "Number:11 epoch: 121/1200 loss_tea:0.04476872438378896  loss_houyan:2283.22216796875\n",
      "Number:11 epoch: 131/1200 loss_tea:0.04800885040271504  loss_houyan:1724.4757080078125\n",
      "Number:11 epoch: 141/1200 loss_tea:0.05821306529857259  loss_houyan:1783.8424072265625\n",
      "Number:11 epoch: 151/1200 loss_tea:0.05155903700960233  loss_houyan:1533.564208984375\n",
      "Number:11 epoch: 161/1200 loss_tea:0.0542995709264136  loss_houyan:1645.00244140625\n",
      "Number:11 epoch: 171/1200 loss_tea:0.05700944460278005  loss_houyan:1556.1732177734375\n",
      "Number:11 epoch: 181/1200 loss_tea:0.05801550872851799  loss_houyan:1949.0831298828125\n",
      "Number:11 epoch: 191/1200 loss_tea:0.04895484702597873  loss_houyan:1616.620849609375\n",
      "Number:11 epoch: 201/1200 loss_tea:0.06394069116801802  loss_houyan:1633.9197998046875\n",
      "Number:11 epoch: 211/1200 loss_tea:0.057752675989728386  loss_houyan:1887.9161376953125\n",
      "Number:11 epoch: 221/1200 loss_tea:0.05087856526875365  loss_houyan:1553.6514892578125\n",
      "Number:11 epoch: 231/1200 loss_tea:0.0518982071740372  loss_houyan:1756.550537109375\n",
      "Number:11 epoch: 241/1200 loss_tea:0.062446996883532765  loss_houyan:3169.1611328125\n",
      "Number:11 epoch: 251/1200 loss_tea:0.052717855959641735  loss_houyan:1922.450439453125\n",
      "Number:11 epoch: 261/1200 loss_tea:0.046575653771380315  loss_houyan:1521.2391357421875\n",
      "Number:11 epoch: 271/1200 loss_tea:0.054326730186271724  loss_houyan:1567.91552734375\n",
      "Number:11 epoch: 281/1200 loss_tea:0.04931764568886777  loss_houyan:1716.064208984375\n",
      "Number:11 epoch: 291/1200 loss_tea:0.0717952776370513  loss_houyan:1930.33544921875\n",
      "Number:11 epoch: 301/1200 loss_tea:0.0529473073243286  loss_houyan:2440.352783203125\n",
      "Number:11 epoch: 311/1200 loss_tea:0.044319595892570125  loss_houyan:1548.0595703125\n",
      "Number:11 epoch: 321/1200 loss_tea:0.05184661138237597  loss_houyan:1877.283447265625\n",
      "Number:11 epoch: 331/1200 loss_tea:0.04922182794606755  loss_houyan:1510.7645263671875\n",
      "Number:11 epoch: 341/1200 loss_tea:0.05114930708407269  loss_houyan:1921.8961181640625\n",
      "Number:11 epoch: 351/1200 loss_tea:0.04629810389711569  loss_houyan:1881.55712890625\n",
      "Number:11 epoch: 361/1200 loss_tea:0.051710856756610815  loss_houyan:1605.7239990234375\n",
      "Number:11 epoch: 371/1200 loss_tea:0.04480906547395618  loss_houyan:1556.3421630859375\n",
      "Number:11 epoch: 381/1200 loss_tea:0.05953690845195596  loss_houyan:1999.6126708984375\n",
      "Number:11 epoch: 391/1200 loss_tea:0.05511188767971129  loss_houyan:1508.08056640625\n",
      "Number:11 epoch: 401/1200 loss_tea:0.053122825411250046  loss_houyan:1738.7750244140625\n",
      "Number:11 epoch: 411/1200 loss_tea:0.060431988103587256  loss_houyan:1596.0928955078125\n",
      "Number:11 epoch: 421/1200 loss_tea:0.055834866365795495  loss_houyan:1834.93310546875\n",
      "Number:11 epoch: 431/1200 loss_tea:0.04588801557217408  loss_houyan:1664.2864990234375\n",
      "Number:11 epoch: 441/1200 loss_tea:0.06138252529545982  loss_houyan:1686.834716796875\n",
      "Number:11 epoch: 451/1200 loss_tea:0.07891093148078927  loss_houyan:1583.04541015625\n",
      "Number:11 epoch: 461/1200 loss_tea:0.06670177114475877  loss_houyan:1752.01171875\n",
      "Number:11 epoch: 471/1200 loss_tea:0.04382905795648075  loss_houyan:1525.473876953125\n",
      "Number:11 epoch: 481/1200 loss_tea:0.052721207012573865  loss_houyan:1653.45947265625\n",
      "Number:11 epoch: 491/1200 loss_tea:0.045957005892687655  loss_houyan:1541.3631591796875\n",
      "Number:11 epoch: 501/1200 loss_tea:0.049041524967105135  loss_houyan:1725.813232421875\n",
      "Number:11 epoch: 511/1200 loss_tea:0.06183213970427912  loss_houyan:1573.3587646484375\n",
      "Number:11 epoch: 521/1200 loss_tea:0.059982653653691695  loss_houyan:1551.968994140625\n",
      "Number:11 epoch: 531/1200 loss_tea:0.05328844506870778  loss_houyan:1577.9683837890625\n",
      "Number:11 epoch: 541/1200 loss_tea:0.06977078386733158  loss_houyan:1504.3948974609375\n",
      "Number:11 epoch: 551/1200 loss_tea:0.05736080008086005  loss_houyan:1709.906005859375\n",
      "Number:11 epoch: 561/1200 loss_tea:0.05235409626315812  loss_houyan:1533.8917236328125\n",
      "Number:11 epoch: 571/1200 loss_tea:0.05647368192127933  loss_houyan:1505.4002685546875\n",
      "Number:11 epoch: 581/1200 loss_tea:0.057531425812018894  loss_houyan:1607.265869140625\n",
      "Number:11 epoch: 591/1200 loss_tea:0.05846453181145185  loss_houyan:1725.6629638671875\n",
      "Number:11 epoch: 601/1200 loss_tea:0.055282702405780634  loss_houyan:1570.20751953125\n",
      "Number:11 epoch: 611/1200 loss_tea:0.05144270271038705  loss_houyan:1966.908935546875\n",
      "Number:11 epoch: 621/1200 loss_tea:0.06358296264182563  loss_houyan:1628.1710205078125\n",
      "Number:11 epoch: 631/1200 loss_tea:0.04247216501001382  loss_houyan:1644.1121826171875\n",
      "Number:11 epoch: 641/1200 loss_tea:0.04575306063350792  loss_houyan:1797.1790771484375\n",
      "Number:11 epoch: 651/1200 loss_tea:0.05895893147423885  loss_houyan:1537.8455810546875\n",
      "Number:11 epoch: 661/1200 loss_tea:0.06400276382780277  loss_houyan:1701.5576171875\n",
      "Number:11 epoch: 671/1200 loss_tea:0.057862974215862555  loss_houyan:2063.3447265625\n",
      "Number:11 epoch: 681/1200 loss_tea:0.050224120899392016  loss_houyan:1555.0059814453125\n",
      "Number:11 epoch: 691/1200 loss_tea:0.03599404370012688  loss_houyan:1608.4342041015625\n",
      "Number:11 epoch: 701/1200 loss_tea:0.04348264954922314  loss_houyan:1717.677001953125\n",
      "Number:11 epoch: 711/1200 loss_tea:0.059350644377699165  loss_houyan:1568.84423828125\n",
      "Number:11 epoch: 721/1200 loss_tea:0.043129124381938444  loss_houyan:1515.9266357421875\n",
      "Number:11 epoch: 731/1200 loss_tea:0.045709075494477905  loss_houyan:1586.309814453125\n",
      "Number:11 epoch: 741/1200 loss_tea:0.03954584602434818  loss_houyan:1548.9969482421875\n",
      "Number:11 epoch: 751/1200 loss_tea:0.05000595987772734  loss_houyan:1748.1112060546875\n",
      "Number:11 epoch: 761/1200 loss_tea:0.04382754224549669  loss_houyan:1663.3099365234375\n",
      "Number:11 epoch: 771/1200 loss_tea:0.058170128053068176  loss_houyan:1634.6851806640625\n",
      "Number:11 epoch: 781/1200 loss_tea:0.04424700674883673  loss_houyan:1593.11279296875\n",
      "Number:11 epoch: 791/1200 loss_tea:0.04304715598268815  loss_houyan:2212.477783203125\n",
      "Number:11 epoch: 801/1200 loss_tea:0.05262688115195378  loss_houyan:1530.841064453125\n",
      "Number:11 epoch: 811/1200 loss_tea:0.056113464376367196  loss_houyan:1531.6201171875\n",
      "Number:11 epoch: 821/1200 loss_tea:0.05302053390431856  loss_houyan:1532.2677001953125\n",
      "Number:11 epoch: 831/1200 loss_tea:0.04596199376287129  loss_houyan:1703.10205078125\n",
      "Number:11 epoch: 841/1200 loss_tea:0.051814222473297276  loss_houyan:1551.40869140625\n",
      "Number:11 epoch: 851/1200 loss_tea:0.04236552300199894  loss_houyan:1625.6153564453125\n",
      "Number:11 epoch: 861/1200 loss_tea:0.06338767492598504  loss_houyan:1612.254150390625\n",
      "Number:11 epoch: 871/1200 loss_tea:0.054111309185041596  loss_houyan:1678.4759521484375\n",
      "Number:11 epoch: 881/1200 loss_tea:0.05033439171442214  loss_houyan:1622.9117431640625\n",
      "Number:11 epoch: 891/1200 loss_tea:0.05955632778134741  loss_houyan:1666.4871826171875\n",
      "Number:11 epoch: 901/1200 loss_tea:0.051039707614344725  loss_houyan:1647.128662109375\n",
      "Number:11 epoch: 911/1200 loss_tea:0.05240558269826995  loss_houyan:1608.2911376953125\n",
      "Number:11 epoch: 921/1200 loss_tea:0.06030925905533061  loss_houyan:1551.3094482421875\n",
      "Number:11 epoch: 931/1200 loss_tea:0.054952849239165276  loss_houyan:1722.814697265625\n",
      "Number:11 epoch: 941/1200 loss_tea:0.055961774407471146  loss_houyan:1806.7900390625\n",
      "Number:11 epoch: 951/1200 loss_tea:0.06434896311249018  loss_houyan:1821.158447265625\n",
      "Number:11 epoch: 961/1200 loss_tea:0.04726115006599586  loss_houyan:1605.7808837890625\n",
      "Number:11 epoch: 971/1200 loss_tea:0.044732139455462026  loss_houyan:1530.973876953125\n",
      "Number:11 epoch: 981/1200 loss_tea:0.05744179314674335  loss_houyan:1502.98095703125\n",
      "Number:11 epoch: 991/1200 loss_tea:0.05464386141747508  loss_houyan:1627.239501953125\n",
      "Number:11 epoch: 1001/1200 loss_tea:0.05033618522514052  loss_houyan:1537.0897216796875\n",
      "Number:11 epoch: 1011/1200 loss_tea:0.05242859826158941  loss_houyan:1869.166259765625\n",
      "Number:11 epoch: 1021/1200 loss_tea:0.06207854536312757  loss_houyan:1562.887451171875\n",
      "Number:11 epoch: 1031/1200 loss_tea:0.05301094915003267  loss_houyan:1622.593017578125\n",
      "Number:11 epoch: 1041/1200 loss_tea:0.0473625467430309  loss_houyan:1540.6612548828125\n",
      "Number:11 epoch: 1051/1200 loss_tea:0.06017937645291978  loss_houyan:1560.0189208984375\n",
      "Number:11 epoch: 1061/1200 loss_tea:0.05257326796053271  loss_houyan:1728.418212890625\n",
      "Number:11 epoch: 1071/1200 loss_tea:0.06543343789167724  loss_houyan:1787.4423828125\n",
      "Number:11 epoch: 1081/1200 loss_tea:0.05822626245788157  loss_houyan:1506.2210693359375\n",
      "Number:11 epoch: 1091/1200 loss_tea:0.045199583273427026  loss_houyan:1592.5482177734375\n",
      "Number:11 epoch: 1101/1200 loss_tea:0.04976858839533011  loss_houyan:1612.1002197265625\n",
      "Number:11 epoch: 1111/1200 loss_tea:0.0712032658265855  loss_houyan:1507.55859375\n",
      "Number:11 epoch: 1121/1200 loss_tea:0.04788516996308017  loss_houyan:1564.986328125\n",
      "Number:11 epoch: 1131/1200 loss_tea:0.046238540904360644  loss_houyan:1775.951416015625\n",
      "Number:11 epoch: 1141/1200 loss_tea:0.05524465370774631  loss_houyan:1574.9652099609375\n",
      "Number:11 epoch: 1151/1200 loss_tea:0.04714308628472501  loss_houyan:1702.910400390625\n",
      "Number:11 epoch: 1161/1200 loss_tea:0.046551686857429164  loss_houyan:1575.953125\n",
      "Number:11 epoch: 1171/1200 loss_tea:0.0635286219386357  loss_houyan:1557.9873046875\n",
      "Number:11 epoch: 1181/1200 loss_tea:0.05768109563000897  loss_houyan:1587.5494384765625\n",
      "Number:11 epoch: 1191/1200 loss_tea:0.05356317760220225  loss_houyan:1539.551513671875\n",
      "finished training number 11 techer!\n",
      "start training number 12 techer!\n",
      "Number:12 epoch: 1/1200 loss_tea:0.04510171029053467  loss_houyan:1659.4852294921875\n",
      "Number:12 epoch: 11/1200 loss_tea:0.04993104906924369  loss_houyan:2119.885009765625\n",
      "Number:12 epoch: 21/1200 loss_tea:0.09327087351244569  loss_houyan:1651.656494140625\n",
      "Number:12 epoch: 31/1200 loss_tea:0.051514814672880334  loss_houyan:1573.3668212890625\n",
      "Number:12 epoch: 41/1200 loss_tea:0.06203392928238699  loss_houyan:2603.694580078125\n",
      "Number:12 epoch: 51/1200 loss_tea:0.04531863017216346  loss_houyan:1534.7581787109375\n",
      "Number:12 epoch: 61/1200 loss_tea:0.05090163239719886  loss_houyan:1745.1658935546875\n",
      "Number:12 epoch: 71/1200 loss_tea:0.06114283138205099  loss_houyan:1718.4481201171875\n",
      "Number:12 epoch: 81/1200 loss_tea:0.04543070906215963  loss_houyan:1696.888427734375\n",
      "Number:12 epoch: 91/1200 loss_tea:0.053263059465061054  loss_houyan:1822.41845703125\n",
      "Number:12 epoch: 101/1200 loss_tea:0.059454583929440794  loss_houyan:1545.881591796875\n",
      "Number:12 epoch: 111/1200 loss_tea:0.06011763692252279  loss_houyan:1543.6044921875\n",
      "Number:12 epoch: 121/1200 loss_tea:0.05613461836903883  loss_houyan:1578.1224365234375\n",
      "Number:12 epoch: 131/1200 loss_tea:0.061381659135660085  loss_houyan:1667.7122802734375\n",
      "Number:12 epoch: 141/1200 loss_tea:0.053858363175044444  loss_houyan:1581.9658203125\n",
      "Number:12 epoch: 151/1200 loss_tea:0.04278182291839078  loss_houyan:1583.290283203125\n",
      "Number:12 epoch: 161/1200 loss_tea:0.05219406426573164  loss_houyan:1588.133544921875\n",
      "Number:12 epoch: 171/1200 loss_tea:0.04382810418256169  loss_houyan:1538.638916015625\n",
      "Number:12 epoch: 181/1200 loss_tea:0.04546399561868268  loss_houyan:1641.29150390625\n",
      "Number:12 epoch: 191/1200 loss_tea:0.05366688875146836  loss_houyan:1600.015625\n",
      "Number:12 epoch: 201/1200 loss_tea:0.055394435252753595  loss_houyan:1570.35693359375\n",
      "Number:12 epoch: 211/1200 loss_tea:0.05126725772276131  loss_houyan:1623.1663818359375\n",
      "Number:12 epoch: 221/1200 loss_tea:0.05478821963174444  loss_houyan:1537.9925537109375\n",
      "Number:12 epoch: 231/1200 loss_tea:0.07689838434807414  loss_houyan:1736.804443359375\n",
      "Number:12 epoch: 241/1200 loss_tea:0.05669186677174793  loss_houyan:1626.7587890625\n",
      "Number:12 epoch: 251/1200 loss_tea:0.041746906631324174  loss_houyan:1605.503662109375\n",
      "Number:12 epoch: 261/1200 loss_tea:0.06420102508137626  loss_houyan:1599.0980224609375\n",
      "Number:12 epoch: 271/1200 loss_tea:0.040225307398481555  loss_houyan:1669.1962890625\n",
      "Number:12 epoch: 281/1200 loss_tea:0.051194136410329855  loss_houyan:1961.7264404296875\n",
      "Number:12 epoch: 291/1200 loss_tea:0.0476445990060176  loss_houyan:1544.3023681640625\n",
      "Number:12 epoch: 301/1200 loss_tea:0.05216618691070591  loss_houyan:1529.8880615234375\n",
      "Number:12 epoch: 311/1200 loss_tea:0.0681869874366725  loss_houyan:1510.6671142578125\n",
      "Number:12 epoch: 321/1200 loss_tea:0.050736073719204404  loss_houyan:1947.326904296875\n",
      "Number:12 epoch: 331/1200 loss_tea:0.05535268401702557  loss_houyan:1714.848876953125\n",
      "Number:12 epoch: 341/1200 loss_tea:0.0626368966298425  loss_houyan:1659.080322265625\n",
      "Number:12 epoch: 351/1200 loss_tea:0.047014156011617424  loss_houyan:1577.3133544921875\n",
      "Number:12 epoch: 361/1200 loss_tea:0.06523044887875072  loss_houyan:2357.50439453125\n",
      "Number:12 epoch: 371/1200 loss_tea:0.06659684765005681  loss_houyan:1584.4754638671875\n",
      "Number:12 epoch: 381/1200 loss_tea:0.0465155822344325  loss_houyan:1595.708251953125\n",
      "Number:12 epoch: 391/1200 loss_tea:0.05312495192949386  loss_houyan:1505.1929931640625\n",
      "Number:12 epoch: 401/1200 loss_tea:0.04632726999885198  loss_houyan:1568.7869873046875\n",
      "Number:12 epoch: 411/1200 loss_tea:0.07386614172761863  loss_houyan:1575.8433837890625\n",
      "Number:12 epoch: 421/1200 loss_tea:0.060425771148684825  loss_houyan:1552.4219970703125\n",
      "Number:12 epoch: 431/1200 loss_tea:0.045264778853802874  loss_houyan:1785.739990234375\n",
      "Number:12 epoch: 441/1200 loss_tea:0.04846095235847391  loss_houyan:1729.867431640625\n",
      "Number:12 epoch: 451/1200 loss_tea:0.0612631163753966  loss_houyan:1585.7740478515625\n",
      "Number:12 epoch: 461/1200 loss_tea:0.05393930615890067  loss_houyan:1552.2972412109375\n",
      "Number:12 epoch: 471/1200 loss_tea:0.04376226117661276  loss_houyan:1692.6630859375\n",
      "Number:12 epoch: 481/1200 loss_tea:0.06200381306675201  loss_houyan:1802.1824951171875\n",
      "Number:12 epoch: 491/1200 loss_tea:0.06761616150229258  loss_houyan:1702.6759033203125\n",
      "Number:12 epoch: 501/1200 loss_tea:0.04286958468583327  loss_houyan:1842.947265625\n",
      "Number:12 epoch: 511/1200 loss_tea:0.05200547743071809  loss_houyan:1768.2677001953125\n",
      "Number:12 epoch: 521/1200 loss_tea:0.06700361471986128  loss_houyan:1974.415771484375\n",
      "Number:12 epoch: 531/1200 loss_tea:0.04994148800120509  loss_houyan:2365.7373046875\n",
      "Number:12 epoch: 541/1200 loss_tea:0.059902602871957276  loss_houyan:1660.3734130859375\n",
      "Number:12 epoch: 551/1200 loss_tea:0.07330095420535129  loss_houyan:1713.1190185546875\n",
      "Number:12 epoch: 561/1200 loss_tea:0.06220848470132482  loss_houyan:1566.7923583984375\n",
      "Number:12 epoch: 571/1200 loss_tea:0.04425891859734954  loss_houyan:1680.47509765625\n",
      "Number:12 epoch: 581/1200 loss_tea:0.07280204457121266  loss_houyan:1679.9747314453125\n",
      "Number:12 epoch: 591/1200 loss_tea:0.05174342091467872  loss_houyan:1562.2064208984375\n",
      "Number:12 epoch: 601/1200 loss_tea:0.05525266012399091  loss_houyan:2018.7265625\n",
      "Number:12 epoch: 611/1200 loss_tea:0.0478808573717491  loss_houyan:1484.4263916015625\n",
      "Number:12 epoch: 621/1200 loss_tea:0.05633117363584266  loss_houyan:1761.284912109375\n",
      "Number:12 epoch: 631/1200 loss_tea:0.04099842606133866  loss_houyan:1581.561767578125\n",
      "Number:12 epoch: 641/1200 loss_tea:0.05658948870444795  loss_houyan:1618.3221435546875\n",
      "Number:12 epoch: 651/1200 loss_tea:0.05060745505942418  loss_houyan:1959.90478515625\n",
      "Number:12 epoch: 661/1200 loss_tea:0.07111737806804996  loss_houyan:1624.5560302734375\n",
      "Number:12 epoch: 671/1200 loss_tea:0.07579977539795624  loss_houyan:1617.6943359375\n",
      "Number:12 epoch: 681/1200 loss_tea:0.0645233883756555  loss_houyan:1536.650634765625\n",
      "Number:12 epoch: 691/1200 loss_tea:0.05204411554143681  loss_houyan:1622.9207763671875\n",
      "Number:12 epoch: 701/1200 loss_tea:0.05048106226361556  loss_houyan:1585.4556884765625\n",
      "Number:12 epoch: 711/1200 loss_tea:0.04575426430332593  loss_houyan:1641.9033203125\n",
      "Number:12 epoch: 721/1200 loss_tea:0.0434458945154033  loss_houyan:1858.3795166015625\n",
      "Number:12 epoch: 731/1200 loss_tea:0.05074380275034847  loss_houyan:1512.4849853515625\n",
      "Number:12 epoch: 741/1200 loss_tea:0.04398849941163815  loss_houyan:1992.4208984375\n",
      "Number:12 epoch: 751/1200 loss_tea:0.05231053562499455  loss_houyan:1753.5740966796875\n",
      "Number:12 epoch: 761/1200 loss_tea:0.06681326600325502  loss_houyan:1596.2056884765625\n",
      "Number:12 epoch: 771/1200 loss_tea:0.0566298703925074  loss_houyan:1568.63330078125\n",
      "Number:12 epoch: 781/1200 loss_tea:0.044823626086873423  loss_houyan:1621.7508544921875\n",
      "Number:12 epoch: 791/1200 loss_tea:0.05172790140321024  loss_houyan:1605.1568603515625\n",
      "Number:12 epoch: 801/1200 loss_tea:0.05734705903256304  loss_houyan:2304.54638671875\n",
      "Number:12 epoch: 811/1200 loss_tea:0.052226325180490946  loss_houyan:1673.6136474609375\n",
      "Number:12 epoch: 821/1200 loss_tea:0.047708200380215164  loss_houyan:1520.0361328125\n",
      "Number:12 epoch: 831/1200 loss_tea:0.06032919903950385  loss_houyan:1581.096435546875\n",
      "Number:12 epoch: 841/1200 loss_tea:0.06892646301718093  loss_houyan:1560.5914306640625\n",
      "Number:12 epoch: 851/1200 loss_tea:0.0608493001210634  loss_houyan:1919.16064453125\n",
      "Number:12 epoch: 861/1200 loss_tea:0.04042508621587489  loss_houyan:1585.0687255859375\n",
      "Number:12 epoch: 871/1200 loss_tea:0.054972328515397  loss_houyan:1528.243408203125\n",
      "Number:12 epoch: 881/1200 loss_tea:0.048031363851306456  loss_houyan:1531.16162109375\n",
      "Number:12 epoch: 891/1200 loss_tea:0.06538038131989748  loss_houyan:2026.8984375\n",
      "Number:12 epoch: 901/1200 loss_tea:0.050206384469326966  loss_houyan:1734.6370849609375\n",
      "Number:12 epoch: 911/1200 loss_tea:0.06037534946975303  loss_houyan:1833.6029052734375\n",
      "Number:12 epoch: 921/1200 loss_tea:0.04841618338199504  loss_houyan:1557.5009765625\n",
      "Number:12 epoch: 931/1200 loss_tea:0.052389698840280836  loss_houyan:1539.529541015625\n",
      "Number:12 epoch: 941/1200 loss_tea:0.056737828666994346  loss_houyan:2109.89013671875\n",
      "Number:12 epoch: 951/1200 loss_tea:0.05482582635128694  loss_houyan:1663.322021484375\n",
      "Number:12 epoch: 961/1200 loss_tea:0.06423222490073047  loss_houyan:1634.6878662109375\n",
      "Number:12 epoch: 971/1200 loss_tea:0.06732870508283408  loss_houyan:1740.2745361328125\n",
      "Number:12 epoch: 981/1200 loss_tea:0.05076106557204145  loss_houyan:1577.310546875\n",
      "Number:12 epoch: 991/1200 loss_tea:0.047470804712104485  loss_houyan:1684.6915283203125\n",
      "Number:12 epoch: 1001/1200 loss_tea:0.050290279435995545  loss_houyan:1581.7596435546875\n",
      "Number:12 epoch: 1011/1200 loss_tea:0.06338585544953299  loss_houyan:1596.361083984375\n",
      "Number:12 epoch: 1021/1200 loss_tea:0.06705121232012952  loss_houyan:1561.8203125\n",
      "Number:12 epoch: 1031/1200 loss_tea:0.06674599011196813  loss_houyan:1518.970703125\n",
      "Number:12 epoch: 1041/1200 loss_tea:0.060850702328782875  loss_houyan:1502.7987060546875\n",
      "Number:12 epoch: 1051/1200 loss_tea:0.062100526374018285  loss_houyan:1671.8377685546875\n",
      "Number:12 epoch: 1061/1200 loss_tea:0.05599649214250478  loss_houyan:1532.625732421875\n",
      "Number:12 epoch: 1071/1200 loss_tea:0.048105692666371824  loss_houyan:1579.0693359375\n",
      "Number:12 epoch: 1081/1200 loss_tea:0.05754415407257692  loss_houyan:1558.64013671875\n",
      "Number:12 epoch: 1091/1200 loss_tea:0.05452120760167106  loss_houyan:1699.84912109375\n",
      "Number:12 epoch: 1101/1200 loss_tea:0.06940938956720133  loss_houyan:1791.7672119140625\n",
      "Number:12 epoch: 1111/1200 loss_tea:0.05536194560841459  loss_houyan:1677.7764892578125\n",
      "Number:12 epoch: 1121/1200 loss_tea:0.04814010060809029  loss_houyan:1527.427001953125\n",
      "Number:12 epoch: 1131/1200 loss_tea:0.0722047133691547  loss_houyan:1971.4320068359375\n",
      "Number:12 epoch: 1141/1200 loss_tea:0.059948079076727466  loss_houyan:1562.463623046875\n",
      "Number:12 epoch: 1151/1200 loss_tea:0.058740543883315746  loss_houyan:1667.312255859375\n",
      "Number:12 epoch: 1161/1200 loss_tea:0.05644934983078541  loss_houyan:1880.18115234375\n",
      "Number:12 epoch: 1171/1200 loss_tea:0.05415978155545747  loss_houyan:1537.3023681640625\n",
      "Number:12 epoch: 1181/1200 loss_tea:0.0688481942326259  loss_houyan:2307.92529296875\n",
      "Number:12 epoch: 1191/1200 loss_tea:0.053803369635291635  loss_houyan:1724.0885009765625\n",
      "finished training number 12 techer!\n",
      "start training number 13 techer!\n",
      "Number:13 epoch: 1/1200 loss_tea:0.0776807326300559  loss_houyan:1552.777587890625\n",
      "Number:13 epoch: 11/1200 loss_tea:0.05348036924808318  loss_houyan:1567.4130859375\n",
      "Number:13 epoch: 21/1200 loss_tea:0.06294965271056846  loss_houyan:1525.3314208984375\n",
      "Number:13 epoch: 31/1200 loss_tea:0.051680719573861895  loss_houyan:1787.1453857421875\n",
      "Number:13 epoch: 41/1200 loss_tea:0.06389147322455145  loss_houyan:1532.380615234375\n",
      "Number:13 epoch: 51/1200 loss_tea:0.050671471699737264  loss_houyan:1765.873291015625\n",
      "Number:13 epoch: 61/1200 loss_tea:0.04450403805161889  loss_houyan:1571.667236328125\n",
      "Number:13 epoch: 71/1200 loss_tea:0.05319997818953044  loss_houyan:1622.305419921875\n",
      "Number:13 epoch: 81/1200 loss_tea:0.04851937360408119  loss_houyan:1771.322509765625\n",
      "Number:13 epoch: 91/1200 loss_tea:0.06690458749496964  loss_houyan:1663.4873046875\n",
      "Number:13 epoch: 101/1200 loss_tea:0.05443447912841921  loss_houyan:1600.8460693359375\n",
      "Number:13 epoch: 111/1200 loss_tea:0.04178908837029923  loss_houyan:1529.5206298828125\n",
      "Number:13 epoch: 121/1200 loss_tea:0.052757741448390044  loss_houyan:1600.7042236328125\n",
      "Number:13 epoch: 131/1200 loss_tea:0.060158761735444904  loss_houyan:1597.6605224609375\n",
      "Number:13 epoch: 141/1200 loss_tea:0.051666829925945755  loss_houyan:1746.9052734375\n",
      "Number:13 epoch: 151/1200 loss_tea:0.04470056714933363  loss_houyan:1556.7802734375\n",
      "Number:13 epoch: 161/1200 loss_tea:0.05348281341844274  loss_houyan:1581.181884765625\n",
      "Number:13 epoch: 171/1200 loss_tea:0.04846465475487521  loss_houyan:1585.3118896484375\n",
      "Number:13 epoch: 181/1200 loss_tea:0.05011156673343384  loss_houyan:1534.1116943359375\n",
      "Number:13 epoch: 191/1200 loss_tea:0.0688998687575468  loss_houyan:1624.1756591796875\n",
      "Number:13 epoch: 201/1200 loss_tea:0.050217876486602475  loss_houyan:1543.96826171875\n",
      "Number:13 epoch: 211/1200 loss_tea:0.057458514049996336  loss_houyan:1675.9576416015625\n",
      "Number:13 epoch: 221/1200 loss_tea:0.053080354403521046  loss_houyan:1583.4097900390625\n",
      "Number:13 epoch: 231/1200 loss_tea:0.060449481837440894  loss_houyan:1816.888427734375\n",
      "Number:13 epoch: 241/1200 loss_tea:0.056057771342243046  loss_houyan:1559.1058349609375\n",
      "Number:13 epoch: 251/1200 loss_tea:0.05971663315298105  loss_houyan:1608.5623779296875\n",
      "Number:13 epoch: 261/1200 loss_tea:0.05523768846863903  loss_houyan:1548.0880126953125\n",
      "Number:13 epoch: 271/1200 loss_tea:0.05826512169064975  loss_houyan:1509.839111328125\n",
      "Number:13 epoch: 281/1200 loss_tea:0.04902676330999684  loss_houyan:1850.17431640625\n",
      "Number:13 epoch: 291/1200 loss_tea:0.062036286614697934  loss_houyan:1775.524169921875\n",
      "Number:13 epoch: 301/1200 loss_tea:0.055986167145796774  loss_houyan:1649.0888671875\n",
      "Number:13 epoch: 311/1200 loss_tea:0.05913567796412909  loss_houyan:1540.5064697265625\n",
      "Number:13 epoch: 321/1200 loss_tea:0.051033462145917925  loss_houyan:1568.3070068359375\n",
      "Number:13 epoch: 331/1200 loss_tea:0.07128601214824501  loss_houyan:1651.3436279296875\n",
      "Number:13 epoch: 341/1200 loss_tea:0.04609680214018861  loss_houyan:1557.4898681640625\n",
      "Number:13 epoch: 351/1200 loss_tea:0.05288279098502919  loss_houyan:1575.8798828125\n",
      "Number:13 epoch: 361/1200 loss_tea:0.05612261134792321  loss_houyan:1544.40966796875\n",
      "Number:13 epoch: 371/1200 loss_tea:0.05154549525886569  loss_houyan:1746.5792236328125\n",
      "Number:13 epoch: 381/1200 loss_tea:0.03940116501673045  loss_houyan:1691.6044921875\n",
      "Number:13 epoch: 391/1200 loss_tea:0.04929939998551095  loss_houyan:1561.224853515625\n",
      "Number:13 epoch: 401/1200 loss_tea:0.04976651623297286  loss_houyan:1537.393798828125\n",
      "Number:13 epoch: 411/1200 loss_tea:0.05007524651350559  loss_houyan:1505.38525390625\n",
      "Number:13 epoch: 421/1200 loss_tea:0.04781138263333416  loss_houyan:1567.2440185546875\n",
      "Number:13 epoch: 431/1200 loss_tea:0.08183110039775517  loss_houyan:2310.244384765625\n",
      "Number:13 epoch: 441/1200 loss_tea:0.04873602323766805  loss_houyan:1518.6893310546875\n",
      "Number:13 epoch: 451/1200 loss_tea:0.04985083945824449  loss_houyan:1598.122802734375\n",
      "Number:13 epoch: 461/1200 loss_tea:0.061005293533691926  loss_houyan:1529.6209716796875\n",
      "Number:13 epoch: 471/1200 loss_tea:0.046620020975640046  loss_houyan:1593.282958984375\n",
      "Number:13 epoch: 481/1200 loss_tea:0.05733024424776897  loss_houyan:1689.2039794921875\n",
      "Number:13 epoch: 491/1200 loss_tea:0.046712717482416836  loss_houyan:1562.9061279296875\n",
      "Number:13 epoch: 501/1200 loss_tea:0.05327037755903238  loss_houyan:1633.774658203125\n",
      "Number:13 epoch: 511/1200 loss_tea:0.0417027336367777  loss_houyan:1551.9837646484375\n",
      "Number:13 epoch: 521/1200 loss_tea:0.06936932727920882  loss_houyan:1807.1328125\n",
      "Number:13 epoch: 531/1200 loss_tea:0.05053573422449331  loss_houyan:1923.7232666015625\n",
      "Number:13 epoch: 541/1200 loss_tea:0.05657546679476676  loss_houyan:1774.0347900390625\n",
      "Number:13 epoch: 551/1200 loss_tea:0.05732962792538935  loss_houyan:1858.7998046875\n",
      "Number:13 epoch: 561/1200 loss_tea:0.0496017501883983  loss_houyan:2037.862060546875\n",
      "Number:13 epoch: 571/1200 loss_tea:0.08330893395810034  loss_houyan:1502.1151123046875\n",
      "Number:13 epoch: 581/1200 loss_tea:0.057244739517288847  loss_houyan:1521.000732421875\n",
      "Number:13 epoch: 591/1200 loss_tea:0.054113848830207485  loss_houyan:1571.3280029296875\n",
      "Number:13 epoch: 601/1200 loss_tea:0.04668475612952735  loss_houyan:1659.7401123046875\n",
      "Number:13 epoch: 611/1200 loss_tea:0.04382504116429149  loss_houyan:1582.1226806640625\n",
      "Number:13 epoch: 621/1200 loss_tea:0.0484821486739747  loss_houyan:1547.2623291015625\n",
      "Number:13 epoch: 631/1200 loss_tea:0.04596203127798581  loss_houyan:1490.64990234375\n",
      "Number:13 epoch: 641/1200 loss_tea:0.061655479428411764  loss_houyan:1614.6373291015625\n",
      "Number:13 epoch: 651/1200 loss_tea:0.03987882552671848  loss_houyan:1706.1549072265625\n",
      "Number:13 epoch: 661/1200 loss_tea:0.04536796028519995  loss_houyan:1696.660400390625\n",
      "Number:13 epoch: 671/1200 loss_tea:0.047856590526277706  loss_houyan:1601.98681640625\n",
      "Number:13 epoch: 681/1200 loss_tea:0.04311638160863186  loss_houyan:1620.9456787109375\n",
      "Number:13 epoch: 691/1200 loss_tea:0.08128303158986791  loss_houyan:1571.020263671875\n",
      "Number:13 epoch: 701/1200 loss_tea:0.04666406572912419  loss_houyan:1945.8746337890625\n",
      "Number:13 epoch: 711/1200 loss_tea:0.05166870480260314  loss_houyan:1572.4962158203125\n",
      "Number:13 epoch: 721/1200 loss_tea:0.04684660698730308  loss_houyan:1592.046142578125\n",
      "Number:13 epoch: 731/1200 loss_tea:0.06248627971885705  loss_houyan:1703.1876220703125\n",
      "Number:13 epoch: 741/1200 loss_tea:0.04362862321899225  loss_houyan:1519.658447265625\n",
      "Number:13 epoch: 751/1200 loss_tea:0.058348889214586384  loss_houyan:1715.158935546875\n",
      "Number:13 epoch: 761/1200 loss_tea:0.05618394130917486  loss_houyan:1562.1209716796875\n",
      "Number:13 epoch: 771/1200 loss_tea:0.06361858681687911  loss_houyan:1568.717041015625\n",
      "Number:13 epoch: 781/1200 loss_tea:0.04256383878802732  loss_houyan:1572.5509033203125\n",
      "Number:13 epoch: 791/1200 loss_tea:0.0445664600292415  loss_houyan:1663.20458984375\n",
      "Number:13 epoch: 801/1200 loss_tea:0.04913192452832299  loss_houyan:1795.01318359375\n",
      "Number:13 epoch: 811/1200 loss_tea:0.041500242251932246  loss_houyan:1652.7227783203125\n",
      "Number:13 epoch: 821/1200 loss_tea:0.0495289248995919  loss_houyan:1738.48974609375\n",
      "Number:13 epoch: 831/1200 loss_tea:0.06895708220435319  loss_houyan:1585.4227294921875\n",
      "Number:13 epoch: 841/1200 loss_tea:0.05468452021257764  loss_houyan:1642.4833984375\n",
      "Number:13 epoch: 851/1200 loss_tea:0.05291461543905752  loss_houyan:1856.423095703125\n",
      "Number:13 epoch: 861/1200 loss_tea:0.05357663378991746  loss_houyan:1529.408935546875\n",
      "Number:13 epoch: 871/1200 loss_tea:0.060566891838187316  loss_houyan:1601.0615234375\n",
      "Number:13 epoch: 881/1200 loss_tea:0.07037106278890633  loss_houyan:1799.7130126953125\n",
      "Number:13 epoch: 891/1200 loss_tea:0.05098944880235816  loss_houyan:1601.4012451171875\n",
      "Number:13 epoch: 901/1200 loss_tea:0.0470154170563389  loss_houyan:1955.9239501953125\n",
      "Number:13 epoch: 911/1200 loss_tea:0.06741862408009429  loss_houyan:1551.8255615234375\n",
      "Number:13 epoch: 921/1200 loss_tea:0.05339172225145947  loss_houyan:1775.3238525390625\n",
      "Number:13 epoch: 931/1200 loss_tea:0.03995393078109442  loss_houyan:1591.4493408203125\n",
      "Number:13 epoch: 941/1200 loss_tea:0.04766069448607543  loss_houyan:1614.2965087890625\n",
      "Number:13 epoch: 951/1200 loss_tea:0.05574488265642469  loss_houyan:1594.4371337890625\n",
      "Number:13 epoch: 961/1200 loss_tea:0.06158611860374509  loss_houyan:1640.20068359375\n",
      "Number:13 epoch: 971/1200 loss_tea:0.06155671565481253  loss_houyan:1487.4193115234375\n",
      "Number:13 epoch: 981/1200 loss_tea:0.0563884870712667  loss_houyan:1511.3450927734375\n",
      "Number:13 epoch: 991/1200 loss_tea:0.06292159901545168  loss_houyan:1663.5130615234375\n",
      "Number:13 epoch: 1001/1200 loss_tea:0.05213489917540613  loss_houyan:1864.6324462890625\n",
      "Number:13 epoch: 1011/1200 loss_tea:0.047985535508886824  loss_houyan:1647.322998046875\n",
      "Number:13 epoch: 1021/1200 loss_tea:0.051652248750361736  loss_houyan:1614.966796875\n",
      "Number:13 epoch: 1031/1200 loss_tea:0.05151536128698198  loss_houyan:1765.9417724609375\n",
      "Number:13 epoch: 1041/1200 loss_tea:0.05963198849069268  loss_houyan:1528.93359375\n",
      "Number:13 epoch: 1051/1200 loss_tea:0.05171296742369536  loss_houyan:2418.92431640625\n",
      "Number:13 epoch: 1061/1200 loss_tea:0.05390330287575999  loss_houyan:1578.2064208984375\n",
      "Number:13 epoch: 1071/1200 loss_tea:0.07422731988847328  loss_houyan:1604.0291748046875\n",
      "Number:13 epoch: 1081/1200 loss_tea:0.0602906240372552  loss_houyan:1554.8004150390625\n",
      "Number:13 epoch: 1091/1200 loss_tea:0.04467928797347671  loss_houyan:1718.3873291015625\n",
      "Number:13 epoch: 1101/1200 loss_tea:0.04852404977680339  loss_houyan:1836.3577880859375\n",
      "Number:13 epoch: 1111/1200 loss_tea:0.07403227323634146  loss_houyan:1560.5958251953125\n",
      "Number:13 epoch: 1121/1200 loss_tea:0.05477884981056107  loss_houyan:1656.598876953125\n",
      "Number:13 epoch: 1131/1200 loss_tea:0.061140553566616074  loss_houyan:1558.728515625\n",
      "Number:13 epoch: 1141/1200 loss_tea:0.06812592051207947  loss_houyan:1547.372314453125\n",
      "Number:13 epoch: 1151/1200 loss_tea:0.04798299396938403  loss_houyan:1770.9443359375\n",
      "Number:13 epoch: 1161/1200 loss_tea:0.061451396899037884  loss_houyan:1546.099609375\n",
      "Number:13 epoch: 1171/1200 loss_tea:0.0705301324205527  loss_houyan:1511.4949951171875\n",
      "Number:13 epoch: 1181/1200 loss_tea:0.04964383140263684  loss_houyan:1584.93359375\n",
      "Number:13 epoch: 1191/1200 loss_tea:0.05736604632448698  loss_houyan:1540.01220703125\n",
      "finished training number 13 techer!\n",
      "start training number 14 techer!\n",
      "Number:14 epoch: 1/1200 loss_tea:0.07086097067103155  loss_houyan:1545.76171875\n",
      "Number:14 epoch: 11/1200 loss_tea:0.04505069560341345  loss_houyan:1721.588623046875\n",
      "Number:14 epoch: 21/1200 loss_tea:0.055617102606122806  loss_houyan:2390.33837890625\n",
      "Number:14 epoch: 31/1200 loss_tea:0.06273431993729676  loss_houyan:2014.516845703125\n",
      "Number:14 epoch: 41/1200 loss_tea:0.048572956085814765  loss_houyan:1586.6900634765625\n",
      "Number:14 epoch: 51/1200 loss_tea:0.05922267361886834  loss_houyan:1540.7657470703125\n",
      "Number:14 epoch: 61/1200 loss_tea:0.04715326581062009  loss_houyan:1568.548095703125\n",
      "Number:14 epoch: 71/1200 loss_tea:0.04958296517702112  loss_houyan:1557.359130859375\n",
      "Number:14 epoch: 81/1200 loss_tea:0.038402609799886515  loss_houyan:1642.86767578125\n",
      "Number:14 epoch: 91/1200 loss_tea:0.04911242293996417  loss_houyan:1974.638427734375\n",
      "Number:14 epoch: 101/1200 loss_tea:0.05396157767905116  loss_houyan:1562.3131103515625\n",
      "Number:14 epoch: 111/1200 loss_tea:0.04855319707287886  loss_houyan:1536.609130859375\n",
      "Number:14 epoch: 121/1200 loss_tea:0.07137336140071812  loss_houyan:2195.59912109375\n",
      "Number:14 epoch: 131/1200 loss_tea:0.05332567290087352  loss_houyan:1759.4461669921875\n",
      "Number:14 epoch: 141/1200 loss_tea:0.04076002887002135  loss_houyan:1573.5767822265625\n",
      "Number:14 epoch: 151/1200 loss_tea:0.04820219723400991  loss_houyan:1599.427734375\n",
      "Number:14 epoch: 161/1200 loss_tea:0.053926668267758215  loss_houyan:1934.145263671875\n",
      "Number:14 epoch: 171/1200 loss_tea:0.06399592598464438  loss_houyan:1632.0384521484375\n",
      "Number:14 epoch: 181/1200 loss_tea:0.055388468820416835  loss_houyan:1538.26953125\n",
      "Number:14 epoch: 191/1200 loss_tea:0.055801709060132175  loss_houyan:1695.712890625\n",
      "Number:14 epoch: 201/1200 loss_tea:0.05039620035208561  loss_houyan:1687.92333984375\n",
      "Number:14 epoch: 211/1200 loss_tea:0.08181677860182766  loss_houyan:1521.6331787109375\n",
      "Number:14 epoch: 221/1200 loss_tea:0.05161191678704256  loss_houyan:1720.8966064453125\n",
      "Number:14 epoch: 231/1200 loss_tea:0.056463522619628365  loss_houyan:1530.4923095703125\n",
      "Number:14 epoch: 241/1200 loss_tea:0.056079399543920945  loss_houyan:1694.39013671875\n",
      "Number:14 epoch: 251/1200 loss_tea:0.057646975974600057  loss_houyan:2003.75732421875\n",
      "Number:14 epoch: 261/1200 loss_tea:0.04942732510406502  loss_houyan:1731.12939453125\n",
      "Number:14 epoch: 271/1200 loss_tea:0.06092969165141159  loss_houyan:1567.3546142578125\n",
      "Number:14 epoch: 281/1200 loss_tea:0.0640826532415365  loss_houyan:1543.8663330078125\n",
      "Number:14 epoch: 291/1200 loss_tea:0.049363613521036215  loss_houyan:1592.1214599609375\n",
      "Number:14 epoch: 301/1200 loss_tea:0.05698468541571407  loss_houyan:1759.0653076171875\n",
      "Number:14 epoch: 311/1200 loss_tea:0.06247146005753759  loss_houyan:1600.029052734375\n",
      "Number:14 epoch: 321/1200 loss_tea:0.05146084542723713  loss_houyan:1592.2818603515625\n",
      "Number:14 epoch: 331/1200 loss_tea:0.04609070950694622  loss_houyan:1513.7696533203125\n",
      "Number:14 epoch: 341/1200 loss_tea:0.060467088882899464  loss_houyan:1540.541748046875\n",
      "Number:14 epoch: 351/1200 loss_tea:0.03410536263131265  loss_houyan:1548.022216796875\n",
      "Number:14 epoch: 361/1200 loss_tea:0.062407084954051756  loss_houyan:1554.6114501953125\n",
      "Number:14 epoch: 371/1200 loss_tea:0.050622717659042894  loss_houyan:1594.5225830078125\n",
      "Number:14 epoch: 381/1200 loss_tea:0.05316498751416485  loss_houyan:1624.400634765625\n",
      "Number:14 epoch: 391/1200 loss_tea:0.04289823290317532  loss_houyan:1726.880126953125\n",
      "Number:14 epoch: 401/1200 loss_tea:0.069613568137683  loss_houyan:1589.3992919921875\n",
      "Number:14 epoch: 411/1200 loss_tea:0.05540125490182659  loss_houyan:1561.360107421875\n",
      "Number:14 epoch: 421/1200 loss_tea:0.05588355040034713  loss_houyan:1713.60986328125\n",
      "Number:14 epoch: 431/1200 loss_tea:0.04638599114051492  loss_houyan:1545.0845947265625\n",
      "Number:14 epoch: 441/1200 loss_tea:0.05907764984002378  loss_houyan:1585.859375\n",
      "Number:14 epoch: 451/1200 loss_tea:0.056854680182058755  loss_houyan:1670.068115234375\n",
      "Number:14 epoch: 461/1200 loss_tea:0.05946187898727933  loss_houyan:2062.7158203125\n",
      "Number:14 epoch: 471/1200 loss_tea:0.047168309778633066  loss_houyan:1565.7601318359375\n",
      "Number:14 epoch: 481/1200 loss_tea:0.07191791339874183  loss_houyan:1655.624755859375\n",
      "Number:14 epoch: 491/1200 loss_tea:0.05747725968135014  loss_houyan:1574.938232421875\n",
      "Number:14 epoch: 501/1200 loss_tea:0.04595153275873465  loss_houyan:1655.9044189453125\n",
      "Number:14 epoch: 511/1200 loss_tea:0.04459364262422902  loss_houyan:1508.452880859375\n",
      "Number:14 epoch: 521/1200 loss_tea:0.04772108401865498  loss_houyan:1556.7015380859375\n",
      "Number:14 epoch: 531/1200 loss_tea:0.08302994868956719  loss_houyan:1729.7547607421875\n",
      "Number:14 epoch: 541/1200 loss_tea:0.049448070377232166  loss_houyan:1535.30322265625\n",
      "Number:14 epoch: 551/1200 loss_tea:0.053521313605513536  loss_houyan:1727.1844482421875\n",
      "Number:14 epoch: 561/1200 loss_tea:0.04927371600776736  loss_houyan:1734.2584228515625\n",
      "Number:14 epoch: 571/1200 loss_tea:0.056363465743211445  loss_houyan:1647.2420654296875\n",
      "Number:14 epoch: 581/1200 loss_tea:0.05025999650890368  loss_houyan:2006.96728515625\n",
      "Number:14 epoch: 591/1200 loss_tea:0.04436755585937889  loss_houyan:1836.5643310546875\n",
      "Number:14 epoch: 601/1200 loss_tea:0.05084835509838473  loss_houyan:1591.56298828125\n",
      "Number:14 epoch: 611/1200 loss_tea:0.050529223972588556  loss_houyan:1727.0706787109375\n",
      "Number:14 epoch: 621/1200 loss_tea:0.05378171170938889  loss_houyan:1523.86572265625\n",
      "Number:14 epoch: 631/1200 loss_tea:0.07144213039544953  loss_houyan:1505.1153564453125\n",
      "Number:14 epoch: 641/1200 loss_tea:0.05458802351241764  loss_houyan:1528.6658935546875\n",
      "Number:14 epoch: 651/1200 loss_tea:0.0509403821775878  loss_houyan:1534.61572265625\n",
      "Number:14 epoch: 661/1200 loss_tea:0.0441773555220586  loss_houyan:2136.788330078125\n",
      "Number:14 epoch: 671/1200 loss_tea:0.07802936161894873  loss_houyan:1544.4356689453125\n",
      "Number:14 epoch: 681/1200 loss_tea:0.05011239633453093  loss_houyan:1597.2945556640625\n",
      "Number:14 epoch: 691/1200 loss_tea:0.04087901474967982  loss_houyan:1534.4482421875\n",
      "Number:14 epoch: 701/1200 loss_tea:0.06943042930279736  loss_houyan:1831.439453125\n",
      "Number:14 epoch: 711/1200 loss_tea:0.06500860349857508  loss_houyan:1533.5166015625\n",
      "Number:14 epoch: 721/1200 loss_tea:0.05247589590468217  loss_houyan:1539.0897216796875\n",
      "Number:14 epoch: 731/1200 loss_tea:0.06160520018656195  loss_houyan:1529.8790283203125\n",
      "Number:14 epoch: 741/1200 loss_tea:0.06337779802465478  loss_houyan:1661.5987548828125\n",
      "Number:14 epoch: 751/1200 loss_tea:0.05591258251198589  loss_houyan:1714.0181884765625\n",
      "Number:14 epoch: 761/1200 loss_tea:0.05810097681352043  loss_houyan:1800.3580322265625\n",
      "Number:14 epoch: 771/1200 loss_tea:0.050883727949901175  loss_houyan:1603.418212890625\n",
      "Number:14 epoch: 781/1200 loss_tea:0.05421489071691775  loss_houyan:1554.367431640625\n",
      "Number:14 epoch: 791/1200 loss_tea:0.06480403161191352  loss_houyan:1672.0599365234375\n",
      "Number:14 epoch: 801/1200 loss_tea:0.05356012798237588  loss_houyan:1655.6341552734375\n",
      "Number:14 epoch: 811/1200 loss_tea:0.04635506080143215  loss_houyan:1569.1859130859375\n",
      "Number:14 epoch: 821/1200 loss_tea:0.05140772200509655  loss_houyan:1789.26611328125\n",
      "Number:14 epoch: 831/1200 loss_tea:0.05242299867564907  loss_houyan:1642.720458984375\n",
      "Number:14 epoch: 841/1200 loss_tea:0.061635172349590026  loss_houyan:1987.935791015625\n",
      "Number:14 epoch: 851/1200 loss_tea:0.06904677332582468  loss_houyan:1529.45068359375\n",
      "Number:14 epoch: 861/1200 loss_tea:0.05194752172981773  loss_houyan:1631.0166015625\n",
      "Number:14 epoch: 871/1200 loss_tea:0.05634775545907595  loss_houyan:1709.686767578125\n",
      "Number:14 epoch: 881/1200 loss_tea:0.05179194841252446  loss_houyan:1551.389892578125\n",
      "Number:14 epoch: 891/1200 loss_tea:0.07471688851149719  loss_houyan:2432.46435546875\n",
      "Number:14 epoch: 901/1200 loss_tea:0.052385406181177  loss_houyan:1576.6280517578125\n",
      "Number:14 epoch: 911/1200 loss_tea:0.0415416781130121  loss_houyan:1589.6363525390625\n",
      "Number:14 epoch: 921/1200 loss_tea:0.06121405181398584  loss_houyan:1644.114990234375\n",
      "Number:14 epoch: 931/1200 loss_tea:0.0647740180575675  loss_houyan:1530.6094970703125\n",
      "Number:14 epoch: 941/1200 loss_tea:0.041284297152083003  loss_houyan:1809.78125\n",
      "Number:14 epoch: 951/1200 loss_tea:0.06518314699801762  loss_houyan:1800.143310546875\n",
      "Number:14 epoch: 961/1200 loss_tea:0.05191511960603193  loss_houyan:1603.079833984375\n",
      "Number:14 epoch: 971/1200 loss_tea:0.0667575593477129  loss_houyan:1686.6429443359375\n",
      "Number:14 epoch: 981/1200 loss_tea:0.07014404306124465  loss_houyan:1565.8885498046875\n",
      "Number:14 epoch: 991/1200 loss_tea:0.05865866758030437  loss_houyan:1694.94482421875\n",
      "Number:14 epoch: 1001/1200 loss_tea:0.06367407228510184  loss_houyan:1834.4765625\n",
      "Number:14 epoch: 1011/1200 loss_tea:0.05430733847473528  loss_houyan:1577.0291748046875\n",
      "Number:14 epoch: 1021/1200 loss_tea:0.05901336362813681  loss_houyan:1651.33837890625\n",
      "Number:14 epoch: 1031/1200 loss_tea:0.05758230069436017  loss_houyan:1638.0155029296875\n",
      "Number:14 epoch: 1041/1200 loss_tea:0.056093184624892725  loss_houyan:1592.5455322265625\n",
      "Number:14 epoch: 1051/1200 loss_tea:0.04608505278341329  loss_houyan:1576.741943359375\n",
      "Number:14 epoch: 1061/1200 loss_tea:0.05292357610686119  loss_houyan:1643.2713623046875\n",
      "Number:14 epoch: 1071/1200 loss_tea:0.05906877060191989  loss_houyan:1584.1905517578125\n",
      "Number:14 epoch: 1081/1200 loss_tea:0.05671606092576631  loss_houyan:1576.25537109375\n",
      "Number:14 epoch: 1091/1200 loss_tea:0.042900981290610374  loss_houyan:1537.12158203125\n",
      "Number:14 epoch: 1101/1200 loss_tea:0.06992636034248183  loss_houyan:1603.4095458984375\n",
      "Number:14 epoch: 1111/1200 loss_tea:0.07214611115495106  loss_houyan:1875.1231689453125\n",
      "Number:14 epoch: 1121/1200 loss_tea:0.052590553882496546  loss_houyan:1643.024169921875\n",
      "Number:14 epoch: 1131/1200 loss_tea:0.07375481004732592  loss_houyan:1581.1646728515625\n",
      "Number:14 epoch: 1141/1200 loss_tea:0.05680172701028766  loss_houyan:1530.20654296875\n",
      "Number:14 epoch: 1151/1200 loss_tea:0.052888153501553695  loss_houyan:1593.3372802734375\n",
      "Number:14 epoch: 1161/1200 loss_tea:0.03993311835159011  loss_houyan:2087.6708984375\n",
      "Number:14 epoch: 1171/1200 loss_tea:0.05418152603569218  loss_houyan:1592.761474609375\n",
      "Number:14 epoch: 1181/1200 loss_tea:0.058249494059742134  loss_houyan:1557.0604248046875\n",
      "Number:14 epoch: 1191/1200 loss_tea:0.04176044945524172  loss_houyan:1667.232421875\n",
      "finished training number 14 techer!\n",
      "start training number 15 techer!\n",
      "Number:15 epoch: 1/1200 loss_tea:0.04700774091458318  loss_houyan:1639.1026611328125\n",
      "Number:15 epoch: 11/1200 loss_tea:0.06526384956180953  loss_houyan:1539.640380859375\n",
      "Number:15 epoch: 21/1200 loss_tea:0.055083441787812666  loss_houyan:1567.305419921875\n",
      "Number:15 epoch: 31/1200 loss_tea:0.06629506665937858  loss_houyan:1777.1343994140625\n",
      "Number:15 epoch: 41/1200 loss_tea:0.04807255825366246  loss_houyan:1549.0941162109375\n",
      "Number:15 epoch: 51/1200 loss_tea:0.05333903943035123  loss_houyan:1575.986572265625\n",
      "Number:15 epoch: 61/1200 loss_tea:0.04889060695728648  loss_houyan:1564.5816650390625\n",
      "Number:15 epoch: 71/1200 loss_tea:0.05433716190571797  loss_houyan:1666.746826171875\n",
      "Number:15 epoch: 81/1200 loss_tea:0.042272068701981774  loss_houyan:1552.6884765625\n",
      "Number:15 epoch: 91/1200 loss_tea:0.058194813646597975  loss_houyan:1645.7198486328125\n",
      "Number:15 epoch: 101/1200 loss_tea:0.05688655642003684  loss_houyan:2035.790771484375\n",
      "Number:15 epoch: 111/1200 loss_tea:0.056102235062986894  loss_houyan:1665.5361328125\n",
      "Number:15 epoch: 121/1200 loss_tea:0.04008483973041053  loss_houyan:1621.119873046875\n",
      "Number:15 epoch: 131/1200 loss_tea:0.044901543975793955  loss_houyan:1623.265380859375\n",
      "Number:15 epoch: 141/1200 loss_tea:0.06235096001035896  loss_houyan:1868.4056396484375\n",
      "Number:15 epoch: 151/1200 loss_tea:0.07790988296004764  loss_houyan:1746.99658203125\n",
      "Number:15 epoch: 161/1200 loss_tea:0.047236606124799844  loss_houyan:1578.8492431640625\n",
      "Number:15 epoch: 171/1200 loss_tea:0.052183015643644845  loss_houyan:1543.10595703125\n",
      "Number:15 epoch: 181/1200 loss_tea:0.0573930992668086  loss_houyan:1997.029052734375\n",
      "Number:15 epoch: 191/1200 loss_tea:0.04992917388396899  loss_houyan:1673.0516357421875\n",
      "Number:15 epoch: 201/1200 loss_tea:0.05769029697329742  loss_houyan:1548.9517822265625\n",
      "Number:15 epoch: 211/1200 loss_tea:0.06680767462547911  loss_houyan:1520.0247802734375\n",
      "Number:15 epoch: 221/1200 loss_tea:0.05608948105099183  loss_houyan:1580.3570556640625\n",
      "Number:15 epoch: 231/1200 loss_tea:0.061957311875935575  loss_houyan:2159.615478515625\n",
      "Number:15 epoch: 241/1200 loss_tea:0.047579039832177566  loss_houyan:3219.66064453125\n",
      "Number:15 epoch: 251/1200 loss_tea:0.07023718353252989  loss_houyan:1657.7823486328125\n",
      "Number:15 epoch: 261/1200 loss_tea:0.053558786070860544  loss_houyan:1574.457763671875\n",
      "Number:15 epoch: 271/1200 loss_tea:0.06536857315058262  loss_houyan:1676.7119140625\n",
      "Number:15 epoch: 281/1200 loss_tea:0.06612997412219786  loss_houyan:1585.1295166015625\n",
      "Number:15 epoch: 291/1200 loss_tea:0.07008287796993258  loss_houyan:1567.4454345703125\n",
      "Number:15 epoch: 301/1200 loss_tea:0.05572211862603421  loss_houyan:1650.8994140625\n",
      "Number:15 epoch: 311/1200 loss_tea:0.0677559668836213  loss_houyan:1692.0255126953125\n",
      "Number:15 epoch: 321/1200 loss_tea:0.04732454310916251  loss_houyan:1846.6741943359375\n",
      "Number:15 epoch: 331/1200 loss_tea:0.05245837372521507  loss_houyan:1621.471435546875\n",
      "Number:15 epoch: 341/1200 loss_tea:0.05014196686719819  loss_houyan:1680.6473388671875\n",
      "Number:15 epoch: 351/1200 loss_tea:0.04876965580854081  loss_houyan:1560.72509765625\n",
      "Number:15 epoch: 361/1200 loss_tea:0.058245925545535496  loss_houyan:1551.4212646484375\n",
      "Number:15 epoch: 371/1200 loss_tea:0.0608996262697283  loss_houyan:1576.5458984375\n",
      "Number:15 epoch: 381/1200 loss_tea:0.04897290997166418  loss_houyan:1954.3974609375\n",
      "Number:15 epoch: 391/1200 loss_tea:0.046415984056360306  loss_houyan:1481.210693359375\n",
      "Number:15 epoch: 401/1200 loss_tea:0.041000533525292295  loss_houyan:1610.892578125\n",
      "Number:15 epoch: 411/1200 loss_tea:0.06384384183370442  loss_houyan:1577.348876953125\n",
      "Number:15 epoch: 421/1200 loss_tea:0.042035142326128004  loss_houyan:1624.035400390625\n",
      "Number:15 epoch: 431/1200 loss_tea:0.06168277218940892  loss_houyan:1597.7406005859375\n",
      "Number:15 epoch: 441/1200 loss_tea:0.05510513362389285  loss_houyan:1681.3372802734375\n",
      "Number:15 epoch: 451/1200 loss_tea:0.05402145099369456  loss_houyan:1574.403076171875\n",
      "Number:15 epoch: 461/1200 loss_tea:0.05261444716367411  loss_houyan:1698.0933837890625\n",
      "Number:15 epoch: 471/1200 loss_tea:0.05265760561045737  loss_houyan:1526.470703125\n",
      "Number:15 epoch: 481/1200 loss_tea:0.0484631286256041  loss_houyan:1554.6011962890625\n",
      "Number:15 epoch: 491/1200 loss_tea:0.04789794856787682  loss_houyan:1619.2069091796875\n",
      "Number:15 epoch: 501/1200 loss_tea:0.05088820062835786  loss_houyan:1639.0103759765625\n",
      "Number:15 epoch: 511/1200 loss_tea:0.07938692472303158  loss_houyan:1675.9197998046875\n",
      "Number:15 epoch: 521/1200 loss_tea:0.04049444914087487  loss_houyan:2077.01904296875\n",
      "Number:15 epoch: 531/1200 loss_tea:0.049456158238645145  loss_houyan:1568.2738037109375\n",
      "Number:15 epoch: 541/1200 loss_tea:0.03974661475369291  loss_houyan:1562.712646484375\n",
      "Number:15 epoch: 551/1200 loss_tea:0.04399113260256343  loss_houyan:1729.6654052734375\n",
      "Number:15 epoch: 561/1200 loss_tea:0.04391126980009423  loss_houyan:2041.20263671875\n",
      "Number:15 epoch: 571/1200 loss_tea:0.060233988862349624  loss_houyan:1531.419921875\n",
      "Number:15 epoch: 581/1200 loss_tea:0.053331853955431774  loss_houyan:1587.40087890625\n",
      "Number:15 epoch: 591/1200 loss_tea:0.06517105044618608  loss_houyan:2264.162109375\n",
      "Number:15 epoch: 601/1200 loss_tea:0.04910711679401633  loss_houyan:1587.6624755859375\n",
      "Number:15 epoch: 611/1200 loss_tea:0.05772231962604998  loss_houyan:1514.5560302734375\n",
      "Number:15 epoch: 621/1200 loss_tea:0.05628379214014002  loss_houyan:1565.8446044921875\n",
      "Number:15 epoch: 631/1200 loss_tea:0.05348253981333852  loss_houyan:1951.4136962890625\n",
      "Number:15 epoch: 641/1200 loss_tea:0.06515414051557628  loss_houyan:1752.6650390625\n",
      "Number:15 epoch: 651/1200 loss_tea:0.05953835999920998  loss_houyan:1564.208984375\n",
      "Number:15 epoch: 661/1200 loss_tea:0.06462089067707183  loss_houyan:1566.077392578125\n",
      "Number:15 epoch: 671/1200 loss_tea:0.04749730381799995  loss_houyan:1597.3321533203125\n",
      "Number:15 epoch: 681/1200 loss_tea:0.06284298673151402  loss_houyan:1584.036376953125\n",
      "Number:15 epoch: 691/1200 loss_tea:0.0546953632864714  loss_houyan:1551.954345703125\n",
      "Number:15 epoch: 701/1200 loss_tea:0.06410732670806549  loss_houyan:1900.6741943359375\n",
      "Number:15 epoch: 711/1200 loss_tea:0.055123777132340235  loss_houyan:2079.349365234375\n",
      "Number:15 epoch: 721/1200 loss_tea:0.06067273038061604  loss_houyan:1564.3292236328125\n",
      "Number:15 epoch: 731/1200 loss_tea:0.04233244620376762  loss_houyan:1544.1925048828125\n",
      "Number:15 epoch: 741/1200 loss_tea:0.06533912077798144  loss_houyan:2253.33935546875\n",
      "Number:15 epoch: 751/1200 loss_tea:0.046037026324662354  loss_houyan:1823.7525634765625\n",
      "Number:15 epoch: 761/1200 loss_tea:0.03456483443896008  loss_houyan:1567.7962646484375\n",
      "Number:15 epoch: 771/1200 loss_tea:0.048686774315888616  loss_houyan:1795.9027099609375\n",
      "Number:15 epoch: 781/1200 loss_tea:0.061846245383945266  loss_houyan:1667.1722412109375\n",
      "Number:15 epoch: 791/1200 loss_tea:0.05058896894430327  loss_houyan:1576.064208984375\n",
      "Number:15 epoch: 801/1200 loss_tea:0.052979401468774896  loss_houyan:1569.23193359375\n",
      "Number:15 epoch: 811/1200 loss_tea:0.06508296816493786  loss_houyan:1810.0633544921875\n",
      "Number:15 epoch: 821/1200 loss_tea:0.06858833383331904  loss_houyan:1976.1046142578125\n",
      "Number:15 epoch: 831/1200 loss_tea:0.07098485499843789  loss_houyan:1628.001708984375\n",
      "Number:15 epoch: 841/1200 loss_tea:0.057728072971328925  loss_houyan:1544.0255126953125\n",
      "Number:15 epoch: 851/1200 loss_tea:0.05022475077980608  loss_houyan:1598.4935302734375\n",
      "Number:15 epoch: 861/1200 loss_tea:0.05664423766383908  loss_houyan:1657.7652587890625\n",
      "Number:15 epoch: 871/1200 loss_tea:0.05142241013129761  loss_houyan:1650.4625244140625\n",
      "Number:15 epoch: 881/1200 loss_tea:0.0642140345753193  loss_houyan:1522.1563720703125\n",
      "Number:15 epoch: 891/1200 loss_tea:0.044581892116363896  loss_houyan:1533.3792724609375\n",
      "Number:15 epoch: 901/1200 loss_tea:0.05695275090716316  loss_houyan:1828.7421875\n",
      "Number:15 epoch: 911/1200 loss_tea:0.06044032719270744  loss_houyan:1571.377197265625\n",
      "Number:15 epoch: 921/1200 loss_tea:0.058861574266352634  loss_houyan:1651.2335205078125\n",
      "Number:15 epoch: 931/1200 loss_tea:0.052669122109236036  loss_houyan:1661.0211181640625\n",
      "Number:15 epoch: 941/1200 loss_tea:0.06546487923313882  loss_houyan:1687.6337890625\n",
      "Number:15 epoch: 951/1200 loss_tea:0.055952755011073343  loss_houyan:1656.0833740234375\n",
      "Number:15 epoch: 961/1200 loss_tea:0.06718583015963836  loss_houyan:1592.179931640625\n",
      "Number:15 epoch: 971/1200 loss_tea:0.04961237375086284  loss_houyan:1547.645751953125\n",
      "Number:15 epoch: 981/1200 loss_tea:0.05095278273748016  loss_houyan:1539.767578125\n",
      "Number:15 epoch: 991/1200 loss_tea:0.05521690170379672  loss_houyan:1700.1217041015625\n",
      "Number:15 epoch: 1001/1200 loss_tea:0.045656974096336365  loss_houyan:2064.53271484375\n",
      "Number:15 epoch: 1011/1200 loss_tea:0.04066675557519698  loss_houyan:2055.236572265625\n",
      "Number:15 epoch: 1021/1200 loss_tea:0.049553344846043436  loss_houyan:1549.328369140625\n",
      "Number:15 epoch: 1031/1200 loss_tea:0.04855791373898596  loss_houyan:1572.9703369140625\n",
      "Number:15 epoch: 1041/1200 loss_tea:0.054361963609159174  loss_houyan:1574.947509765625\n",
      "Number:15 epoch: 1051/1200 loss_tea:0.047110031802016325  loss_houyan:1687.779052734375\n",
      "Number:15 epoch: 1061/1200 loss_tea:0.06110665506570294  loss_houyan:1808.88916015625\n",
      "Number:15 epoch: 1071/1200 loss_tea:0.04055751660295059  loss_houyan:1600.992431640625\n",
      "Number:15 epoch: 1081/1200 loss_tea:0.06025736756639392  loss_houyan:1555.024658203125\n",
      "Number:15 epoch: 1091/1200 loss_tea:0.06495826507652364  loss_houyan:1534.486328125\n",
      "Number:15 epoch: 1101/1200 loss_tea:0.06670048753814899  loss_houyan:1730.168212890625\n",
      "Number:15 epoch: 1111/1200 loss_tea:0.065540848884272  loss_houyan:1549.9449462890625\n",
      "Number:15 epoch: 1121/1200 loss_tea:0.050894487808415684  loss_houyan:1647.2633056640625\n",
      "Number:15 epoch: 1131/1200 loss_tea:0.06370138819730303  loss_houyan:1789.2496337890625\n",
      "Number:15 epoch: 1141/1200 loss_tea:0.04017112937661373  loss_houyan:2039.177734375\n",
      "Number:15 epoch: 1151/1200 loss_tea:0.05657295174768441  loss_houyan:1562.638916015625\n",
      "Number:15 epoch: 1161/1200 loss_tea:0.0491370595761469  loss_houyan:1570.6693115234375\n",
      "Number:15 epoch: 1171/1200 loss_tea:0.05683565450349703  loss_houyan:1663.523193359375\n",
      "Number:15 epoch: 1181/1200 loss_tea:0.06342045567719976  loss_houyan:1961.921630859375\n",
      "Number:15 epoch: 1191/1200 loss_tea:0.05052091084656105  loss_houyan:1565.806884765625\n",
      "finished training number 15 techer!\n",
      "start training number 16 techer!\n",
      "Number:16 epoch: 1/1200 loss_tea:0.06706380458118186  loss_houyan:2135.55078125\n",
      "Number:16 epoch: 11/1200 loss_tea:0.06836657936735532  loss_houyan:1630.5782470703125\n",
      "Number:16 epoch: 21/1200 loss_tea:0.07641932872026488  loss_houyan:1768.0667724609375\n",
      "Number:16 epoch: 31/1200 loss_tea:0.08339552547354821  loss_houyan:1771.3743896484375\n",
      "Number:16 epoch: 41/1200 loss_tea:0.044828968246727545  loss_houyan:1576.00244140625\n",
      "Number:16 epoch: 51/1200 loss_tea:0.04786242982374528  loss_houyan:1528.6781005859375\n",
      "Number:16 epoch: 61/1200 loss_tea:0.07951327042828839  loss_houyan:1882.9423828125\n",
      "Number:16 epoch: 71/1200 loss_tea:0.06724005138642251  loss_houyan:1568.0701904296875\n",
      "Number:16 epoch: 81/1200 loss_tea:0.0514390580839043  loss_houyan:1582.3642578125\n",
      "Number:16 epoch: 91/1200 loss_tea:0.05043847698937888  loss_houyan:1594.836181640625\n",
      "Number:16 epoch: 101/1200 loss_tea:0.05992517304237564  loss_houyan:1584.26220703125\n",
      "Number:16 epoch: 111/1200 loss_tea:0.05457166913902081  loss_houyan:1520.447021484375\n",
      "Number:16 epoch: 121/1200 loss_tea:0.06217590975976012  loss_houyan:1636.144775390625\n",
      "Number:16 epoch: 131/1200 loss_tea:0.06705602475834178  loss_houyan:1548.067138671875\n",
      "Number:16 epoch: 141/1200 loss_tea:0.04052348248622836  loss_houyan:1538.5018310546875\n",
      "Number:16 epoch: 151/1200 loss_tea:0.05433873899834876  loss_houyan:1496.544921875\n",
      "Number:16 epoch: 161/1200 loss_tea:0.05218341354343339  loss_houyan:1609.9613037109375\n",
      "Number:16 epoch: 171/1200 loss_tea:0.06917850314025481  loss_houyan:1942.585693359375\n",
      "Number:16 epoch: 181/1200 loss_tea:0.06604478312348798  loss_houyan:1543.92431640625\n",
      "Number:16 epoch: 191/1200 loss_tea:0.061067264046046885  loss_houyan:1532.2379150390625\n",
      "Number:16 epoch: 201/1200 loss_tea:0.05545939470088407  loss_houyan:1609.890380859375\n",
      "Number:16 epoch: 211/1200 loss_tea:0.05877223286452517  loss_houyan:1685.58154296875\n",
      "Number:16 epoch: 221/1200 loss_tea:0.05297952333753085  loss_houyan:1737.0474853515625\n",
      "Number:16 epoch: 231/1200 loss_tea:0.046406019782371695  loss_houyan:1741.27880859375\n",
      "Number:16 epoch: 241/1200 loss_tea:0.07065274489899022  loss_houyan:1528.4683837890625\n",
      "Number:16 epoch: 251/1200 loss_tea:0.04647106712682903  loss_houyan:1606.5142822265625\n",
      "Number:16 epoch: 261/1200 loss_tea:0.051127517049204416  loss_houyan:1821.841064453125\n",
      "Number:16 epoch: 271/1200 loss_tea:0.05409120317606615  loss_houyan:1677.541259765625\n",
      "Number:16 epoch: 281/1200 loss_tea:0.056687028444390414  loss_houyan:1551.548828125\n",
      "Number:16 epoch: 291/1200 loss_tea:0.06922292869243034  loss_houyan:1530.2158203125\n",
      "Number:16 epoch: 301/1200 loss_tea:0.041274504675323336  loss_houyan:1604.5037841796875\n",
      "Number:16 epoch: 311/1200 loss_tea:0.04885807796084304  loss_houyan:1968.1444091796875\n",
      "Number:16 epoch: 321/1200 loss_tea:0.05235136694661369  loss_houyan:1601.5718994140625\n",
      "Number:16 epoch: 331/1200 loss_tea:0.05920876646163506  loss_houyan:1563.244873046875\n",
      "Number:16 epoch: 341/1200 loss_tea:0.06079237541255136  loss_houyan:1721.7601318359375\n",
      "Number:16 epoch: 351/1200 loss_tea:0.04880190126640137  loss_houyan:1524.161376953125\n",
      "Number:16 epoch: 361/1200 loss_tea:0.056334241694988255  loss_houyan:1996.677978515625\n",
      "Number:16 epoch: 371/1200 loss_tea:0.055197287612727276  loss_houyan:1582.035400390625\n",
      "Number:16 epoch: 381/1200 loss_tea:0.042523604805804055  loss_houyan:1790.2452392578125\n",
      "Number:16 epoch: 391/1200 loss_tea:0.04592145612006466  loss_houyan:1556.025390625\n",
      "Number:16 epoch: 401/1200 loss_tea:0.0376750368169168  loss_houyan:1538.316162109375\n",
      "Number:16 epoch: 411/1200 loss_tea:0.054172407890805255  loss_houyan:1573.25439453125\n",
      "Number:16 epoch: 421/1200 loss_tea:0.05190773144385485  loss_houyan:1711.743896484375\n",
      "Number:16 epoch: 431/1200 loss_tea:0.06159323722830373  loss_houyan:1817.9400634765625\n",
      "Number:16 epoch: 441/1200 loss_tea:0.07218503120230993  loss_houyan:1571.853271484375\n",
      "Number:16 epoch: 451/1200 loss_tea:0.060396923043523475  loss_houyan:1663.7833251953125\n",
      "Number:16 epoch: 461/1200 loss_tea:0.06333467475345544  loss_houyan:1627.681640625\n",
      "Number:16 epoch: 471/1200 loss_tea:0.045314008476058314  loss_houyan:1584.3863525390625\n",
      "Number:16 epoch: 481/1200 loss_tea:0.04767852251964806  loss_houyan:1529.138916015625\n",
      "Number:16 epoch: 491/1200 loss_tea:0.05845577403048839  loss_houyan:1627.417724609375\n",
      "Number:16 epoch: 501/1200 loss_tea:0.051942508186294596  loss_houyan:1524.9732666015625\n",
      "Number:16 epoch: 511/1200 loss_tea:0.060751311179172755  loss_houyan:1748.67041015625\n",
      "Number:16 epoch: 521/1200 loss_tea:0.04174496233236918  loss_houyan:1548.8868408203125\n",
      "Number:16 epoch: 531/1200 loss_tea:0.04494723217123816  loss_houyan:1589.3876953125\n",
      "Number:16 epoch: 541/1200 loss_tea:0.06235427782995898  loss_houyan:1552.88671875\n",
      "Number:16 epoch: 551/1200 loss_tea:0.05199210885033111  loss_houyan:1578.3927001953125\n",
      "Number:16 epoch: 561/1200 loss_tea:0.04695696955977217  loss_houyan:1542.84375\n",
      "Number:16 epoch: 571/1200 loss_tea:0.05605869025221377  loss_houyan:1619.6522216796875\n",
      "Number:16 epoch: 581/1200 loss_tea:0.07251619915267596  loss_houyan:1750.1158447265625\n",
      "Number:16 epoch: 591/1200 loss_tea:0.05099191718974564  loss_houyan:1693.5733642578125\n",
      "Number:16 epoch: 601/1200 loss_tea:0.05394748734309892  loss_houyan:1828.0384521484375\n",
      "Number:16 epoch: 611/1200 loss_tea:0.06301956864649984  loss_houyan:1563.0340576171875\n",
      "Number:16 epoch: 621/1200 loss_tea:0.051423707001661274  loss_houyan:1529.16064453125\n",
      "Number:16 epoch: 631/1200 loss_tea:0.06706199600593882  loss_houyan:1548.4798583984375\n",
      "Number:16 epoch: 641/1200 loss_tea:0.06382534058992079  loss_houyan:1727.474609375\n",
      "Number:16 epoch: 651/1200 loss_tea:0.046853215355748275  loss_houyan:1652.5396728515625\n",
      "Number:16 epoch: 661/1200 loss_tea:0.049595418370881376  loss_houyan:1598.373046875\n",
      "Number:16 epoch: 671/1200 loss_tea:0.06209808303934011  loss_houyan:1745.0972900390625\n",
      "Number:16 epoch: 681/1200 loss_tea:0.043318625210550646  loss_houyan:1565.7674560546875\n",
      "Number:16 epoch: 691/1200 loss_tea:0.031999850492107254  loss_houyan:1564.842041015625\n",
      "Number:16 epoch: 701/1200 loss_tea:0.04725682552838244  loss_houyan:1903.357421875\n",
      "Number:16 epoch: 711/1200 loss_tea:0.057577480189087225  loss_houyan:1739.849853515625\n",
      "Number:16 epoch: 721/1200 loss_tea:0.05445642132106063  loss_houyan:1558.9996337890625\n",
      "Number:16 epoch: 731/1200 loss_tea:0.06697727935771626  loss_houyan:1620.5186767578125\n",
      "Number:16 epoch: 741/1200 loss_tea:0.052337232691926076  loss_houyan:1765.988525390625\n",
      "Number:16 epoch: 751/1200 loss_tea:0.041914837180976564  loss_houyan:2003.341552734375\n",
      "Number:16 epoch: 761/1200 loss_tea:0.06329037157144471  loss_houyan:1672.956298828125\n",
      "Number:16 epoch: 771/1200 loss_tea:0.055818014335832736  loss_houyan:1616.36328125\n",
      "Number:16 epoch: 781/1200 loss_tea:0.04026026103976817  loss_houyan:1626.2681884765625\n",
      "Number:16 epoch: 791/1200 loss_tea:0.0552457442236636  loss_houyan:1647.717041015625\n",
      "Number:16 epoch: 801/1200 loss_tea:0.044677139415503976  loss_houyan:1591.02392578125\n",
      "Number:16 epoch: 811/1200 loss_tea:0.0530688818268733  loss_houyan:1542.0125732421875\n",
      "Number:16 epoch: 821/1200 loss_tea:0.06581586481260461  loss_houyan:1519.4486083984375\n",
      "Number:16 epoch: 831/1200 loss_tea:0.05727810478594372  loss_houyan:1675.69580078125\n",
      "Number:16 epoch: 841/1200 loss_tea:0.06113132109674264  loss_houyan:1919.010009765625\n",
      "Number:16 epoch: 851/1200 loss_tea:0.056967197356193634  loss_houyan:1704.71923828125\n",
      "Number:16 epoch: 861/1200 loss_tea:0.050442742819337374  loss_houyan:1617.7418212890625\n",
      "Number:16 epoch: 871/1200 loss_tea:0.0564417960324074  loss_houyan:1549.860595703125\n",
      "Number:16 epoch: 881/1200 loss_tea:0.05314571851197044  loss_houyan:1530.0146484375\n",
      "Number:16 epoch: 891/1200 loss_tea:0.043861192459244705  loss_houyan:1597.27587890625\n",
      "Number:16 epoch: 901/1200 loss_tea:0.05684948208688458  loss_houyan:1563.703125\n",
      "Number:16 epoch: 911/1200 loss_tea:0.05186915254578408  loss_houyan:1621.163818359375\n",
      "Number:16 epoch: 921/1200 loss_tea:0.05904376272361892  loss_houyan:1511.4796142578125\n",
      "Number:16 epoch: 931/1200 loss_tea:0.05961109083904195  loss_houyan:1549.0218505859375\n",
      "Number:16 epoch: 941/1200 loss_tea:0.048449448839732456  loss_houyan:1862.9622802734375\n",
      "Number:16 epoch: 951/1200 loss_tea:0.0606255587861953  loss_houyan:1586.663330078125\n",
      "Number:16 epoch: 961/1200 loss_tea:0.06697805173105018  loss_houyan:1575.084716796875\n",
      "Number:16 epoch: 971/1200 loss_tea:0.06069379418044045  loss_houyan:1748.6583251953125\n",
      "Number:16 epoch: 981/1200 loss_tea:0.05714061806770334  loss_houyan:2031.27978515625\n",
      "Number:16 epoch: 991/1200 loss_tea:0.058716516721829946  loss_houyan:1684.9091796875\n",
      "Number:16 epoch: 1001/1200 loss_tea:0.05279023321758573  loss_houyan:2384.00439453125\n",
      "Number:16 epoch: 1011/1200 loss_tea:0.05637851643138553  loss_houyan:1606.153564453125\n",
      "Number:16 epoch: 1021/1200 loss_tea:0.05096986624118901  loss_houyan:1540.9510498046875\n",
      "Number:16 epoch: 1031/1200 loss_tea:0.05302669134927419  loss_houyan:1574.281005859375\n",
      "Number:16 epoch: 1041/1200 loss_tea:0.06246730868027583  loss_houyan:1668.259521484375\n",
      "Number:16 epoch: 1051/1200 loss_tea:0.05817004798007731  loss_houyan:1568.1915283203125\n",
      "Number:16 epoch: 1061/1200 loss_tea:0.051181661066574104  loss_houyan:1598.6171875\n",
      "Number:16 epoch: 1071/1200 loss_tea:0.04528670556389032  loss_houyan:1767.025146484375\n",
      "Number:16 epoch: 1081/1200 loss_tea:0.058968158660644046  loss_houyan:1548.8634033203125\n",
      "Number:16 epoch: 1091/1200 loss_tea:0.05317178637684237  loss_houyan:1626.653564453125\n",
      "Number:16 epoch: 1101/1200 loss_tea:0.06472605443073835  loss_houyan:2059.201416015625\n",
      "Number:16 epoch: 1111/1200 loss_tea:0.04503277880123614  loss_houyan:1793.2755126953125\n",
      "Number:16 epoch: 1121/1200 loss_tea:0.050225217848358206  loss_houyan:1627.650146484375\n",
      "Number:16 epoch: 1131/1200 loss_tea:0.051158663841462074  loss_houyan:1963.881591796875\n",
      "Number:16 epoch: 1141/1200 loss_tea:0.056589793325593316  loss_houyan:1739.099853515625\n",
      "Number:16 epoch: 1151/1200 loss_tea:0.048484180404821435  loss_houyan:1592.609130859375\n",
      "Number:16 epoch: 1161/1200 loss_tea:0.05098461102489031  loss_houyan:1510.9598388671875\n",
      "Number:16 epoch: 1171/1200 loss_tea:0.05897868036430013  loss_houyan:1744.534423828125\n",
      "Number:16 epoch: 1181/1200 loss_tea:0.05663193313184962  loss_houyan:1561.2867431640625\n",
      "Number:16 epoch: 1191/1200 loss_tea:0.05279017362803227  loss_houyan:1601.541748046875\n",
      "finished training number 16 techer!\n",
      "start training number 17 techer!\n",
      "Number:17 epoch: 1/1200 loss_tea:0.058659397891134245  loss_houyan:1536.359375\n",
      "Number:17 epoch: 11/1200 loss_tea:0.05288220249631034  loss_houyan:1631.962646484375\n",
      "Number:17 epoch: 21/1200 loss_tea:0.04693118402693275  loss_houyan:1517.756103515625\n",
      "Number:17 epoch: 31/1200 loss_tea:0.0533434594952133  loss_houyan:1804.327392578125\n",
      "Number:17 epoch: 41/1200 loss_tea:0.05926857443806708  loss_houyan:1604.26025390625\n",
      "Number:17 epoch: 51/1200 loss_tea:0.06538454866074879  loss_houyan:1537.567138671875\n",
      "Number:17 epoch: 61/1200 loss_tea:0.05078597271055472  loss_houyan:1541.1363525390625\n",
      "Number:17 epoch: 71/1200 loss_tea:0.04723056097693571  loss_houyan:1592.5728759765625\n",
      "Number:17 epoch: 81/1200 loss_tea:0.04909672269909662  loss_houyan:1558.0445556640625\n",
      "Number:17 epoch: 91/1200 loss_tea:0.06266671422614678  loss_houyan:1546.764404296875\n",
      "Number:17 epoch: 101/1200 loss_tea:0.05398004457991399  loss_houyan:1718.087158203125\n",
      "Number:17 epoch: 111/1200 loss_tea:0.049943507367362854  loss_houyan:1514.453125\n",
      "Number:17 epoch: 121/1200 loss_tea:0.04556383567803545  loss_houyan:1573.103515625\n",
      "Number:17 epoch: 131/1200 loss_tea:0.05175504850626221  loss_houyan:2044.0322265625\n",
      "Number:17 epoch: 141/1200 loss_tea:0.07669608427239184  loss_houyan:1567.8953857421875\n",
      "Number:17 epoch: 151/1200 loss_tea:0.05542555872440724  loss_houyan:1583.48681640625\n",
      "Number:17 epoch: 161/1200 loss_tea:0.05498984263723549  loss_houyan:1926.538330078125\n",
      "Number:17 epoch: 171/1200 loss_tea:0.047764127192340064  loss_houyan:1559.9241943359375\n",
      "Number:17 epoch: 181/1200 loss_tea:0.04752522345699471  loss_houyan:1691.6309814453125\n",
      "Number:17 epoch: 191/1200 loss_tea:0.04435510848699213  loss_houyan:1557.6466064453125\n",
      "Number:17 epoch: 201/1200 loss_tea:0.036872373906307335  loss_houyan:1725.63818359375\n",
      "Number:17 epoch: 211/1200 loss_tea:0.055844212759415195  loss_houyan:1684.80224609375\n",
      "Number:17 epoch: 221/1200 loss_tea:0.06207869762012609  loss_houyan:1599.4449462890625\n",
      "Number:17 epoch: 231/1200 loss_tea:0.05826721702156604  loss_houyan:1764.7447509765625\n",
      "Number:17 epoch: 241/1200 loss_tea:0.05233372785022729  loss_houyan:1573.3389892578125\n",
      "Number:17 epoch: 251/1200 loss_tea:0.05406182182355523  loss_houyan:1544.6583251953125\n",
      "Number:17 epoch: 261/1200 loss_tea:0.05596625392191476  loss_houyan:1704.990966796875\n",
      "Number:17 epoch: 271/1200 loss_tea:0.06387123413322086  loss_houyan:1531.8782958984375\n",
      "Number:17 epoch: 281/1200 loss_tea:0.05811038627373181  loss_houyan:1519.4266357421875\n",
      "Number:17 epoch: 291/1200 loss_tea:0.04375842476483598  loss_houyan:1528.15087890625\n",
      "Number:17 epoch: 301/1200 loss_tea:0.048711746573789245  loss_houyan:1594.90087890625\n",
      "Number:17 epoch: 311/1200 loss_tea:0.08564981392925994  loss_houyan:1540.6282958984375\n",
      "Number:17 epoch: 321/1200 loss_tea:0.046250340980582325  loss_houyan:1529.36083984375\n",
      "Number:17 epoch: 331/1200 loss_tea:0.05872331354944407  loss_houyan:1912.2095947265625\n",
      "Number:17 epoch: 341/1200 loss_tea:0.04785382436127744  loss_houyan:1536.59375\n",
      "Number:17 epoch: 351/1200 loss_tea:0.046982999151568436  loss_houyan:1520.0999755859375\n",
      "Number:17 epoch: 361/1200 loss_tea:0.05621389140115954  loss_houyan:1689.041015625\n",
      "Number:17 epoch: 371/1200 loss_tea:0.04625462808117877  loss_houyan:1638.30322265625\n",
      "Number:17 epoch: 381/1200 loss_tea:0.07020090921508992  loss_houyan:1555.2674560546875\n",
      "Number:17 epoch: 391/1200 loss_tea:0.06836485103638953  loss_houyan:1625.46875\n",
      "Number:17 epoch: 401/1200 loss_tea:0.04803282237319883  loss_houyan:1585.5147705078125\n",
      "Number:17 epoch: 411/1200 loss_tea:0.050942055984398896  loss_houyan:1547.531982421875\n",
      "Number:17 epoch: 421/1200 loss_tea:0.0453748135765796  loss_houyan:1538.9027099609375\n",
      "Number:17 epoch: 431/1200 loss_tea:0.05352905838444138  loss_houyan:1915.185546875\n",
      "Number:17 epoch: 441/1200 loss_tea:0.0526143742790523  loss_houyan:1614.120361328125\n",
      "Number:17 epoch: 451/1200 loss_tea:0.07104272277773344  loss_houyan:1850.2740478515625\n",
      "Number:17 epoch: 461/1200 loss_tea:0.058600803893395274  loss_houyan:1566.7677001953125\n",
      "Number:17 epoch: 471/1200 loss_tea:0.047635934733462545  loss_houyan:1577.277099609375\n",
      "Number:17 epoch: 481/1200 loss_tea:0.04646224210048968  loss_houyan:1558.2691650390625\n",
      "Number:17 epoch: 491/1200 loss_tea:0.06037718874977889  loss_houyan:1539.9976806640625\n",
      "Number:17 epoch: 501/1200 loss_tea:0.042331052475478914  loss_houyan:1534.392578125\n",
      "Number:17 epoch: 511/1200 loss_tea:0.03949687347843781  loss_houyan:1562.927978515625\n",
      "Number:17 epoch: 521/1200 loss_tea:0.056141132180606734  loss_houyan:1699.170654296875\n",
      "Number:17 epoch: 531/1200 loss_tea:0.055128973906647345  loss_houyan:2192.5986328125\n",
      "Number:17 epoch: 541/1200 loss_tea:0.07165507786013661  loss_houyan:1575.318603515625\n",
      "Number:17 epoch: 551/1200 loss_tea:0.0441477235835985  loss_houyan:1755.9010009765625\n",
      "Number:17 epoch: 561/1200 loss_tea:0.04691549800887194  loss_houyan:1590.4930419921875\n",
      "Number:17 epoch: 571/1200 loss_tea:0.05272051726417163  loss_houyan:1860.50244140625\n",
      "Number:17 epoch: 581/1200 loss_tea:0.05719443249007046  loss_houyan:1571.119384765625\n",
      "Number:17 epoch: 591/1200 loss_tea:0.06384295981519976  loss_houyan:1514.3323974609375\n",
      "Number:17 epoch: 601/1200 loss_tea:0.04821729470487304  loss_houyan:1695.239990234375\n",
      "Number:17 epoch: 611/1200 loss_tea:0.05725960485472693  loss_houyan:2034.981201171875\n",
      "Number:17 epoch: 621/1200 loss_tea:0.050179290133801  loss_houyan:1718.076171875\n",
      "Number:17 epoch: 631/1200 loss_tea:0.04390783939364953  loss_houyan:2161.92333984375\n",
      "Number:17 epoch: 641/1200 loss_tea:0.04757630863374657  loss_houyan:1629.779296875\n",
      "Number:17 epoch: 651/1200 loss_tea:0.052159358574696606  loss_houyan:1565.1976318359375\n",
      "Number:17 epoch: 661/1200 loss_tea:0.04553486403218945  loss_houyan:1729.0750732421875\n",
      "Number:17 epoch: 671/1200 loss_tea:0.04648654649201279  loss_houyan:1523.4254150390625\n",
      "Number:17 epoch: 681/1200 loss_tea:0.04010891737459774  loss_houyan:1568.7052001953125\n",
      "Number:17 epoch: 691/1200 loss_tea:0.04870583933221804  loss_houyan:1586.8116455078125\n",
      "Number:17 epoch: 701/1200 loss_tea:0.0522066544630471  loss_houyan:1830.1314697265625\n",
      "Number:17 epoch: 711/1200 loss_tea:0.05215632955526531  loss_houyan:1699.0174560546875\n",
      "Number:17 epoch: 721/1200 loss_tea:0.055323560322441144  loss_houyan:1599.236328125\n",
      "Number:17 epoch: 731/1200 loss_tea:0.06162209162012826  loss_houyan:1749.1126708984375\n",
      "Number:17 epoch: 741/1200 loss_tea:0.06512438561424012  loss_houyan:1565.707275390625\n",
      "Number:17 epoch: 751/1200 loss_tea:0.04405050761454413  loss_houyan:1897.513427734375\n",
      "Number:17 epoch: 761/1200 loss_tea:0.051237858023742734  loss_houyan:1824.635498046875\n",
      "Number:17 epoch: 771/1200 loss_tea:0.05443514473374095  loss_houyan:1588.42333984375\n",
      "Number:17 epoch: 781/1200 loss_tea:0.04526305628771818  loss_houyan:1547.568359375\n",
      "Number:17 epoch: 791/1200 loss_tea:0.04630597768487635  loss_houyan:1510.8194580078125\n",
      "Number:17 epoch: 801/1200 loss_tea:0.04675670120248626  loss_houyan:1545.928466796875\n",
      "Number:17 epoch: 811/1200 loss_tea:0.055529938696462756  loss_houyan:1526.083251953125\n",
      "Number:17 epoch: 821/1200 loss_tea:0.04817593084368685  loss_houyan:1825.214599609375\n",
      "Number:17 epoch: 831/1200 loss_tea:0.03847598484291923  loss_houyan:1652.8714599609375\n",
      "Number:17 epoch: 841/1200 loss_tea:0.05326514144884398  loss_houyan:1604.8048095703125\n",
      "Number:17 epoch: 851/1200 loss_tea:0.06184755070657534  loss_houyan:1777.5823974609375\n",
      "Number:17 epoch: 861/1200 loss_tea:0.06165190274301439  loss_houyan:1544.523681640625\n",
      "Number:17 epoch: 871/1200 loss_tea:0.04508933797554132  loss_houyan:1700.44140625\n",
      "Number:17 epoch: 881/1200 loss_tea:0.05709482694272211  loss_houyan:1553.074462890625\n",
      "Number:17 epoch: 891/1200 loss_tea:0.05711181058420303  loss_houyan:1885.875\n",
      "Number:17 epoch: 901/1200 loss_tea:0.04337294490889835  loss_houyan:1680.9698486328125\n",
      "Number:17 epoch: 911/1200 loss_tea:0.041572113066205424  loss_houyan:1561.4796142578125\n",
      "Number:17 epoch: 921/1200 loss_tea:0.05780581423927137  loss_houyan:1544.025146484375\n",
      "Number:17 epoch: 931/1200 loss_tea:0.05358322156249499  loss_houyan:2034.7109375\n",
      "Number:17 epoch: 941/1200 loss_tea:0.0462310009379244  loss_houyan:1584.2581787109375\n",
      "Number:17 epoch: 951/1200 loss_tea:0.059588374173487876  loss_houyan:1606.048095703125\n",
      "Number:17 epoch: 961/1200 loss_tea:0.06196651745724164  loss_houyan:1893.349609375\n",
      "Number:17 epoch: 971/1200 loss_tea:0.061434219622822674  loss_houyan:1557.2694091796875\n",
      "Number:17 epoch: 981/1200 loss_tea:0.048784716338891115  loss_houyan:1572.75732421875\n",
      "Number:17 epoch: 991/1200 loss_tea:0.052531821537421065  loss_houyan:1698.4111328125\n",
      "Number:17 epoch: 1001/1200 loss_tea:0.05086571152620236  loss_houyan:1599.180419921875\n",
      "Number:17 epoch: 1011/1200 loss_tea:0.059544624078619664  loss_houyan:1524.8228759765625\n",
      "Number:17 epoch: 1021/1200 loss_tea:0.048840993842163234  loss_houyan:1581.0335693359375\n",
      "Number:17 epoch: 1031/1200 loss_tea:0.05019265346844802  loss_houyan:1515.95654296875\n",
      "Number:17 epoch: 1041/1200 loss_tea:0.04978037355416744  loss_houyan:1979.230224609375\n",
      "Number:17 epoch: 1051/1200 loss_tea:0.05004369045694709  loss_houyan:1899.4588623046875\n",
      "Number:17 epoch: 1061/1200 loss_tea:0.05128883175643948  loss_houyan:1736.3026123046875\n",
      "Number:17 epoch: 1071/1200 loss_tea:0.03828540787776883  loss_houyan:1635.0928955078125\n",
      "Number:17 epoch: 1081/1200 loss_tea:0.06167688049754803  loss_houyan:1812.4066162109375\n",
      "Number:17 epoch: 1091/1200 loss_tea:0.03778784980453024  loss_houyan:1531.0635986328125\n",
      "Number:17 epoch: 1101/1200 loss_tea:0.06474055507078282  loss_houyan:1690.70556640625\n",
      "Number:17 epoch: 1111/1200 loss_tea:0.061464109900016065  loss_houyan:1994.9481201171875\n",
      "Number:17 epoch: 1121/1200 loss_tea:0.060205613864151505  loss_houyan:1543.0999755859375\n",
      "Number:17 epoch: 1131/1200 loss_tea:0.05641608548075027  loss_houyan:1707.764892578125\n",
      "Number:17 epoch: 1141/1200 loss_tea:0.11583683694621703  loss_houyan:1579.545654296875\n",
      "Number:17 epoch: 1151/1200 loss_tea:0.05285921455461872  loss_houyan:1740.513671875\n",
      "Number:17 epoch: 1161/1200 loss_tea:0.04650746907677418  loss_houyan:1557.24365234375\n",
      "Number:17 epoch: 1171/1200 loss_tea:0.045316211296728755  loss_houyan:1692.1314697265625\n",
      "Number:17 epoch: 1181/1200 loss_tea:0.04447504526265652  loss_houyan:1851.07763671875\n",
      "Number:17 epoch: 1191/1200 loss_tea:0.058268782838346  loss_houyan:1525.367431640625\n",
      "finished training number 17 techer!\n",
      "start training number 18 techer!\n",
      "Number:18 epoch: 1/1200 loss_tea:0.06377463919239439  loss_houyan:1583.007568359375\n",
      "Number:18 epoch: 11/1200 loss_tea:0.055634631683451216  loss_houyan:1552.6033935546875\n",
      "Number:18 epoch: 21/1200 loss_tea:0.06264296360276164  loss_houyan:1615.93115234375\n",
      "Number:18 epoch: 31/1200 loss_tea:0.05648664367725329  loss_houyan:1623.78173828125\n",
      "Number:18 epoch: 41/1200 loss_tea:0.061754291115416575  loss_houyan:1592.8685302734375\n",
      "Number:18 epoch: 51/1200 loss_tea:0.04170281692572709  loss_houyan:1625.8262939453125\n",
      "Number:18 epoch: 61/1200 loss_tea:0.06366763882156395  loss_houyan:1574.6339111328125\n",
      "Number:18 epoch: 71/1200 loss_tea:0.040152656466390574  loss_houyan:1701.0743408203125\n",
      "Number:18 epoch: 81/1200 loss_tea:0.048136743325464686  loss_houyan:1795.66064453125\n",
      "Number:18 epoch: 91/1200 loss_tea:0.045858962345526534  loss_houyan:1607.781982421875\n",
      "Number:18 epoch: 101/1200 loss_tea:0.07000937786255865  loss_houyan:2268.1669921875\n",
      "Number:18 epoch: 111/1200 loss_tea:0.07030449732300517  loss_houyan:1777.503173828125\n",
      "Number:18 epoch: 121/1200 loss_tea:0.06076547169349733  loss_houyan:2061.7822265625\n",
      "Number:18 epoch: 131/1200 loss_tea:0.05763114940360753  loss_houyan:1675.5404052734375\n",
      "Number:18 epoch: 141/1200 loss_tea:0.05774808028840767  loss_houyan:1754.192626953125\n",
      "Number:18 epoch: 151/1200 loss_tea:0.06283861601184575  loss_houyan:1653.07421875\n",
      "Number:18 epoch: 161/1200 loss_tea:0.051878536705977286  loss_houyan:1636.6011962890625\n",
      "Number:18 epoch: 171/1200 loss_tea:0.03727185237584554  loss_houyan:1630.6339111328125\n",
      "Number:18 epoch: 181/1200 loss_tea:0.05149778201293252  loss_houyan:1574.128173828125\n",
      "Number:18 epoch: 191/1200 loss_tea:0.04864158234616535  loss_houyan:2010.8603515625\n",
      "Number:18 epoch: 201/1200 loss_tea:0.05321204819951749  loss_houyan:1568.986572265625\n",
      "Number:18 epoch: 211/1200 loss_tea:0.06539575034495558  loss_houyan:1509.447265625\n",
      "Number:18 epoch: 221/1200 loss_tea:0.06423287448134286  loss_houyan:1592.5806884765625\n",
      "Number:18 epoch: 231/1200 loss_tea:0.05311656147747151  loss_houyan:1509.227294921875\n",
      "Number:18 epoch: 241/1200 loss_tea:0.04142156198352171  loss_houyan:1591.47314453125\n",
      "Number:18 epoch: 251/1200 loss_tea:0.054038505603577994  loss_houyan:1670.041015625\n",
      "Number:18 epoch: 261/1200 loss_tea:0.052673458125301165  loss_houyan:1723.3934326171875\n",
      "Number:18 epoch: 271/1200 loss_tea:0.03995149878376417  loss_houyan:1564.5877685546875\n",
      "Number:18 epoch: 281/1200 loss_tea:0.04924946038042253  loss_houyan:1563.67138671875\n",
      "Number:18 epoch: 291/1200 loss_tea:0.043719634114911206  loss_houyan:1825.03515625\n",
      "Number:18 epoch: 301/1200 loss_tea:0.03679760062343827  loss_houyan:1608.6204833984375\n",
      "Number:18 epoch: 311/1200 loss_tea:0.07760136838755628  loss_houyan:2109.67822265625\n",
      "Number:18 epoch: 321/1200 loss_tea:0.05867842184702274  loss_houyan:1522.840576171875\n",
      "Number:18 epoch: 331/1200 loss_tea:0.04750684642602503  loss_houyan:1632.070556640625\n",
      "Number:18 epoch: 341/1200 loss_tea:0.053852849953662824  loss_houyan:1646.2352294921875\n",
      "Number:18 epoch: 351/1200 loss_tea:0.04206465891791979  loss_houyan:1695.56494140625\n",
      "Number:18 epoch: 361/1200 loss_tea:0.06549320665899501  loss_houyan:1617.1044921875\n",
      "Number:18 epoch: 371/1200 loss_tea:0.07150322678016945  loss_houyan:1521.7449951171875\n",
      "Number:18 epoch: 381/1200 loss_tea:0.044599793222067884  loss_houyan:2558.784912109375\n",
      "Number:18 epoch: 391/1200 loss_tea:0.058250972105830405  loss_houyan:1652.155517578125\n",
      "Number:18 epoch: 401/1200 loss_tea:0.0710096311556839  loss_houyan:1594.885009765625\n",
      "Number:18 epoch: 411/1200 loss_tea:0.047316534974772076  loss_houyan:1584.1282958984375\n",
      "Number:18 epoch: 421/1200 loss_tea:0.04258559728129953  loss_houyan:1542.2545166015625\n",
      "Number:18 epoch: 431/1200 loss_tea:0.0614479906526514  loss_houyan:1518.66259765625\n",
      "Number:18 epoch: 441/1200 loss_tea:0.049800705256727945  loss_houyan:1676.3135986328125\n",
      "Number:18 epoch: 451/1200 loss_tea:0.05507656814617644  loss_houyan:1584.186767578125\n",
      "Number:18 epoch: 461/1200 loss_tea:0.04410621216447097  loss_houyan:1565.7344970703125\n",
      "Number:18 epoch: 471/1200 loss_tea:0.05555000621655679  loss_houyan:2065.0673828125\n",
      "Number:18 epoch: 481/1200 loss_tea:0.06727312207795455  loss_houyan:1529.7308349609375\n",
      "Number:18 epoch: 491/1200 loss_tea:0.05498217817502319  loss_houyan:1671.1947021484375\n",
      "Number:18 epoch: 501/1200 loss_tea:0.04682781966167325  loss_houyan:1545.67919921875\n",
      "Number:18 epoch: 511/1200 loss_tea:0.060844196578239126  loss_houyan:1568.74853515625\n",
      "Number:18 epoch: 521/1200 loss_tea:0.048432921617619536  loss_houyan:1557.336669921875\n",
      "Number:18 epoch: 531/1200 loss_tea:0.0872985085463787  loss_houyan:1614.822509765625\n",
      "Number:18 epoch: 541/1200 loss_tea:0.05012014237999711  loss_houyan:1596.1759033203125\n",
      "Number:18 epoch: 551/1200 loss_tea:0.05604499003496975  loss_houyan:1890.402099609375\n",
      "Number:18 epoch: 561/1200 loss_tea:0.05867706584512693  loss_houyan:1501.56982421875\n",
      "Number:18 epoch: 571/1200 loss_tea:0.05292133705270587  loss_houyan:1573.9615478515625\n",
      "Number:18 epoch: 581/1200 loss_tea:0.046083326595413775  loss_houyan:1536.1910400390625\n",
      "Number:18 epoch: 591/1200 loss_tea:0.058118726035735854  loss_houyan:1712.123046875\n",
      "Number:18 epoch: 601/1200 loss_tea:0.05634958797593046  loss_houyan:1850.337158203125\n",
      "Number:18 epoch: 611/1200 loss_tea:0.061996970488694556  loss_houyan:1561.3243408203125\n",
      "Number:18 epoch: 621/1200 loss_tea:0.06551731329359203  loss_houyan:1615.5869140625\n",
      "Number:18 epoch: 631/1200 loss_tea:0.05309507125942951  loss_houyan:1589.2225341796875\n",
      "Number:18 epoch: 641/1200 loss_tea:0.0661125741823557  loss_houyan:1638.7930908203125\n",
      "Number:18 epoch: 651/1200 loss_tea:0.044893354344988534  loss_houyan:1734.8699951171875\n",
      "Number:18 epoch: 661/1200 loss_tea:0.05638094407939974  loss_houyan:1542.1614990234375\n",
      "Number:18 epoch: 671/1200 loss_tea:0.05134411297130372  loss_houyan:1637.0150146484375\n",
      "Number:18 epoch: 681/1200 loss_tea:0.06257470709841237  loss_houyan:1571.2496337890625\n",
      "Number:18 epoch: 691/1200 loss_tea:0.07837718704831197  loss_houyan:1721.8900146484375\n",
      "Number:18 epoch: 701/1200 loss_tea:0.0560138002520924  loss_houyan:1538.3372802734375\n",
      "Number:18 epoch: 711/1200 loss_tea:0.05760385035104251  loss_houyan:1694.982177734375\n",
      "Number:18 epoch: 721/1200 loss_tea:0.0805163750655501  loss_houyan:1607.5709228515625\n",
      "Number:18 epoch: 731/1200 loss_tea:0.06484356038053557  loss_houyan:1569.4571533203125\n",
      "Number:18 epoch: 741/1200 loss_tea:0.04217067412943016  loss_houyan:1752.8665771484375\n",
      "Number:18 epoch: 751/1200 loss_tea:0.06063586638413003  loss_houyan:1566.894775390625\n",
      "Number:18 epoch: 761/1200 loss_tea:0.07058398171894122  loss_houyan:1676.8331298828125\n",
      "Number:18 epoch: 771/1200 loss_tea:0.06446724079122229  loss_houyan:1581.94873046875\n",
      "Number:18 epoch: 781/1200 loss_tea:0.034520034377020045  loss_houyan:1603.0084228515625\n",
      "Number:18 epoch: 791/1200 loss_tea:0.05156714417204843  loss_houyan:1866.08154296875\n",
      "Number:18 epoch: 801/1200 loss_tea:0.05916608604648733  loss_houyan:1705.2889404296875\n",
      "Number:18 epoch: 811/1200 loss_tea:0.05680982985888231  loss_houyan:1697.38232421875\n",
      "Number:18 epoch: 821/1200 loss_tea:0.052861424790405176  loss_houyan:1691.7164306640625\n",
      "Number:18 epoch: 831/1200 loss_tea:0.08443456212787197  loss_houyan:1731.80810546875\n",
      "Number:18 epoch: 841/1200 loss_tea:0.055050061461424236  loss_houyan:1974.672119140625\n",
      "Number:18 epoch: 851/1200 loss_tea:0.06391997804387224  loss_houyan:2019.1929931640625\n",
      "Number:18 epoch: 861/1200 loss_tea:0.057519782521321096  loss_houyan:1561.2139892578125\n",
      "Number:18 epoch: 871/1200 loss_tea:0.05939332361119719  loss_houyan:1547.3221435546875\n",
      "Number:18 epoch: 881/1200 loss_tea:0.04805037226540757  loss_houyan:1575.97607421875\n",
      "Number:18 epoch: 891/1200 loss_tea:0.05269244671809618  loss_houyan:2016.2308349609375\n",
      "Number:18 epoch: 901/1200 loss_tea:0.058292625650781624  loss_houyan:1601.258544921875\n",
      "Number:18 epoch: 911/1200 loss_tea:0.04922879483805075  loss_houyan:1569.691162109375\n",
      "Number:18 epoch: 921/1200 loss_tea:0.07347239679110806  loss_houyan:1683.9588623046875\n",
      "Number:18 epoch: 931/1200 loss_tea:0.05904675801547766  loss_houyan:1512.5841064453125\n",
      "Number:18 epoch: 941/1200 loss_tea:0.049031005173631834  loss_houyan:1649.6817626953125\n",
      "Number:18 epoch: 951/1200 loss_tea:0.049309964983876665  loss_houyan:1681.3431396484375\n",
      "Number:18 epoch: 961/1200 loss_tea:0.04049306250927551  loss_houyan:1606.86669921875\n",
      "Number:18 epoch: 971/1200 loss_tea:0.052514836487920755  loss_houyan:1751.3489990234375\n",
      "Number:18 epoch: 981/1200 loss_tea:0.06768551554530104  loss_houyan:1843.4219970703125\n",
      "Number:18 epoch: 991/1200 loss_tea:0.06687905042925923  loss_houyan:1599.2130126953125\n",
      "Number:18 epoch: 1001/1200 loss_tea:0.04490526987200858  loss_houyan:1585.571044921875\n",
      "Number:18 epoch: 1011/1200 loss_tea:0.05591913364436768  loss_houyan:1589.907958984375\n",
      "Number:18 epoch: 1021/1200 loss_tea:0.0562217940657679  loss_houyan:1507.7337646484375\n",
      "Number:18 epoch: 1031/1200 loss_tea:0.07632698299027851  loss_houyan:1543.68603515625\n",
      "Number:18 epoch: 1041/1200 loss_tea:0.04631915592005771  loss_houyan:1584.5640869140625\n",
      "Number:18 epoch: 1051/1200 loss_tea:0.06430151678753063  loss_houyan:1928.434326171875\n",
      "Number:18 epoch: 1061/1200 loss_tea:0.04694211650628454  loss_houyan:1682.347900390625\n",
      "Number:18 epoch: 1071/1200 loss_tea:0.04732890323737094  loss_houyan:1511.9481201171875\n",
      "Number:18 epoch: 1081/1200 loss_tea:0.055803494177440266  loss_houyan:1529.011962890625\n",
      "Number:18 epoch: 1091/1200 loss_tea:0.05039119426932971  loss_houyan:1536.1240234375\n",
      "Number:18 epoch: 1101/1200 loss_tea:0.05107140640905687  loss_houyan:1549.584228515625\n",
      "Number:18 epoch: 1111/1200 loss_tea:0.04738690755568657  loss_houyan:1552.807861328125\n",
      "Number:18 epoch: 1121/1200 loss_tea:0.056058973855689295  loss_houyan:1587.76318359375\n",
      "Number:18 epoch: 1131/1200 loss_tea:0.05105547152922925  loss_houyan:1597.0731201171875\n",
      "Number:18 epoch: 1141/1200 loss_tea:0.06169232260077401  loss_houyan:2054.11376953125\n",
      "Number:18 epoch: 1151/1200 loss_tea:0.06872907398709624  loss_houyan:1555.4869384765625\n",
      "Number:18 epoch: 1161/1200 loss_tea:0.04828627164769334  loss_houyan:1603.4464111328125\n",
      "Number:18 epoch: 1171/1200 loss_tea:0.06008206700073494  loss_houyan:1705.6748046875\n",
      "Number:18 epoch: 1181/1200 loss_tea:0.04225709934616852  loss_houyan:1609.629150390625\n",
      "Number:18 epoch: 1191/1200 loss_tea:0.05476594481263321  loss_houyan:1715.593505859375\n",
      "finished training number 18 techer!\n",
      "start training number 19 techer!\n",
      "Number:19 epoch: 1/1200 loss_tea:0.06252176876441896  loss_houyan:1583.29736328125\n",
      "Number:19 epoch: 11/1200 loss_tea:0.05459439976479279  loss_houyan:1547.043212890625\n",
      "Number:19 epoch: 21/1200 loss_tea:0.05218172887087786  loss_houyan:2073.44921875\n",
      "Number:19 epoch: 31/1200 loss_tea:0.05594474958708249  loss_houyan:1601.8514404296875\n",
      "Number:19 epoch: 41/1200 loss_tea:0.05598776496747607  loss_houyan:1760.3406982421875\n",
      "Number:19 epoch: 51/1200 loss_tea:0.04881001546705798  loss_houyan:1619.7423095703125\n",
      "Number:19 epoch: 61/1200 loss_tea:0.05765454928151673  loss_houyan:1555.0118408203125\n",
      "Number:19 epoch: 71/1200 loss_tea:0.05211819026144574  loss_houyan:1732.3931884765625\n",
      "Number:19 epoch: 81/1200 loss_tea:0.04969222402150679  loss_houyan:1531.6875\n",
      "Number:19 epoch: 91/1200 loss_tea:0.04576012839355497  loss_houyan:1559.2880859375\n",
      "Number:19 epoch: 101/1200 loss_tea:0.055214320310026085  loss_houyan:1765.5556640625\n",
      "Number:19 epoch: 111/1200 loss_tea:0.0525901984383407  loss_houyan:1773.6046142578125\n",
      "Number:19 epoch: 121/1200 loss_tea:0.06471655791648395  loss_houyan:1531.353515625\n",
      "Number:19 epoch: 131/1200 loss_tea:0.06317670575418717  loss_houyan:1753.4342041015625\n",
      "Number:19 epoch: 141/1200 loss_tea:0.06198108064964133  loss_houyan:1666.9407958984375\n",
      "Number:19 epoch: 151/1200 loss_tea:0.07430798469833065  loss_houyan:1548.6224365234375\n",
      "Number:19 epoch: 161/1200 loss_tea:0.05181299411635699  loss_houyan:1730.8612060546875\n",
      "Number:19 epoch: 171/1200 loss_tea:0.06275820402409267  loss_houyan:1647.2796630859375\n",
      "Number:19 epoch: 181/1200 loss_tea:0.053850804041510464  loss_houyan:1561.493408203125\n",
      "Number:19 epoch: 191/1200 loss_tea:0.03976532867782351  loss_houyan:1635.946533203125\n",
      "Number:19 epoch: 201/1200 loss_tea:0.05652679437316814  loss_houyan:1546.8621826171875\n",
      "Number:19 epoch: 211/1200 loss_tea:0.04453285356105605  loss_houyan:1575.8427734375\n",
      "Number:19 epoch: 221/1200 loss_tea:0.046054912726901334  loss_houyan:1574.764404296875\n",
      "Number:19 epoch: 231/1200 loss_tea:0.050339895898371734  loss_houyan:1563.2386474609375\n",
      "Number:19 epoch: 241/1200 loss_tea:0.05107354960093644  loss_houyan:1521.349853515625\n",
      "Number:19 epoch: 251/1200 loss_tea:0.05956460293914375  loss_houyan:1542.8515625\n",
      "Number:19 epoch: 261/1200 loss_tea:0.04707165095741592  loss_houyan:1580.333984375\n",
      "Number:19 epoch: 271/1200 loss_tea:0.03940905002412681  loss_houyan:1570.893310546875\n",
      "Number:19 epoch: 281/1200 loss_tea:0.06854040312780912  loss_houyan:1531.25927734375\n",
      "Number:19 epoch: 291/1200 loss_tea:0.05623815683597057  loss_houyan:1570.3116455078125\n",
      "Number:19 epoch: 301/1200 loss_tea:0.05382403863546021  loss_houyan:1575.46240234375\n",
      "Number:19 epoch: 311/1200 loss_tea:0.07655943134535124  loss_houyan:1538.8458251953125\n",
      "Number:19 epoch: 321/1200 loss_tea:0.05950036379818989  loss_houyan:1566.6107177734375\n",
      "Number:19 epoch: 331/1200 loss_tea:0.051789342776825245  loss_houyan:1632.8197021484375\n",
      "Number:19 epoch: 341/1200 loss_tea:0.0826656730392454  loss_houyan:1545.2261962890625\n",
      "Number:19 epoch: 351/1200 loss_tea:0.040930830253861995  loss_houyan:1664.4620361328125\n",
      "Number:19 epoch: 361/1200 loss_tea:0.06781767278416197  loss_houyan:1686.6138916015625\n",
      "Number:19 epoch: 371/1200 loss_tea:0.054746002883944454  loss_houyan:1635.638916015625\n",
      "Number:19 epoch: 381/1200 loss_tea:0.039267811872154106  loss_houyan:1645.877197265625\n",
      "Number:19 epoch: 391/1200 loss_tea:0.03734578470517828  loss_houyan:1475.6781005859375\n",
      "Number:19 epoch: 401/1200 loss_tea:0.049581344700641954  loss_houyan:1753.6258544921875\n",
      "Number:19 epoch: 411/1200 loss_tea:0.051751165660782965  loss_houyan:1834.3543701171875\n",
      "Number:19 epoch: 421/1200 loss_tea:0.04250560854372042  loss_houyan:1732.1109619140625\n",
      "Number:19 epoch: 431/1200 loss_tea:0.07584194421273194  loss_houyan:1834.296630859375\n",
      "Number:19 epoch: 441/1200 loss_tea:0.043859085271462074  loss_houyan:1844.4010009765625\n",
      "Number:19 epoch: 451/1200 loss_tea:0.0674408741362494  loss_houyan:1630.388671875\n",
      "Number:19 epoch: 461/1200 loss_tea:0.0672065726484585  loss_houyan:2008.9345703125\n",
      "Number:19 epoch: 471/1200 loss_tea:0.07614023970047279  loss_houyan:1585.372314453125\n",
      "Number:19 epoch: 481/1200 loss_tea:0.05405810127494691  loss_houyan:1671.2921142578125\n",
      "Number:19 epoch: 491/1200 loss_tea:0.05076164894556019  loss_houyan:2408.091552734375\n",
      "Number:19 epoch: 501/1200 loss_tea:0.06021499906036326  loss_houyan:1671.358154296875\n",
      "Number:19 epoch: 511/1200 loss_tea:0.0515796022815262  loss_houyan:1633.9617919921875\n",
      "Number:19 epoch: 521/1200 loss_tea:0.06775510170362499  loss_houyan:1663.994873046875\n",
      "Number:19 epoch: 531/1200 loss_tea:0.049698126159573655  loss_houyan:2113.26953125\n",
      "Number:19 epoch: 541/1200 loss_tea:0.05650323314423597  loss_houyan:1749.984375\n",
      "Number:19 epoch: 551/1200 loss_tea:0.06320279449298419  loss_houyan:1647.7825927734375\n",
      "Number:19 epoch: 561/1200 loss_tea:0.0484780428490828  loss_houyan:1549.406982421875\n",
      "Number:19 epoch: 571/1200 loss_tea:0.05042172349153356  loss_houyan:1749.4483642578125\n",
      "Number:19 epoch: 581/1200 loss_tea:0.0453387488744412  loss_houyan:1896.093994140625\n",
      "Number:19 epoch: 591/1200 loss_tea:0.05444034627463887  loss_houyan:1883.835693359375\n",
      "Number:19 epoch: 601/1200 loss_tea:0.049040321048657756  loss_houyan:1612.3109130859375\n",
      "Number:19 epoch: 611/1200 loss_tea:0.04650130838794109  loss_houyan:1516.8353271484375\n",
      "Number:19 epoch: 621/1200 loss_tea:0.04193197385024476  loss_houyan:1644.6954345703125\n",
      "Number:19 epoch: 631/1200 loss_tea:0.06372077459141579  loss_houyan:1881.801025390625\n",
      "Number:19 epoch: 641/1200 loss_tea:0.07257526947069572  loss_houyan:1576.0250244140625\n",
      "Number:19 epoch: 651/1200 loss_tea:0.05871017219742856  loss_houyan:1554.7242431640625\n",
      "Number:19 epoch: 661/1200 loss_tea:0.04463245073083999  loss_houyan:2008.57666015625\n",
      "Number:19 epoch: 671/1200 loss_tea:0.05925468102713237  loss_houyan:1492.6136474609375\n",
      "Number:19 epoch: 681/1200 loss_tea:0.045779196570005065  loss_houyan:1546.3865966796875\n",
      "Number:19 epoch: 691/1200 loss_tea:0.05760219376568179  loss_houyan:1605.4693603515625\n",
      "Number:19 epoch: 701/1200 loss_tea:0.050649863680802376  loss_houyan:1572.742919921875\n",
      "Number:19 epoch: 711/1200 loss_tea:0.05259134671814002  loss_houyan:1556.357421875\n",
      "Number:19 epoch: 721/1200 loss_tea:0.05609622238860733  loss_houyan:1821.6875\n",
      "Number:19 epoch: 731/1200 loss_tea:0.06293102899720601  loss_houyan:1547.726806640625\n",
      "Number:19 epoch: 741/1200 loss_tea:0.05903792921857407  loss_houyan:1564.962158203125\n",
      "Number:19 epoch: 751/1200 loss_tea:0.06555315380561731  loss_houyan:1778.1478271484375\n",
      "Number:19 epoch: 761/1200 loss_tea:0.04829682786895216  loss_houyan:1502.631591796875\n",
      "Number:19 epoch: 771/1200 loss_tea:0.04957589158569122  loss_houyan:1603.2415771484375\n",
      "Number:19 epoch: 781/1200 loss_tea:0.05956157548430419  loss_houyan:1652.7945556640625\n",
      "Number:19 epoch: 791/1200 loss_tea:0.05799892173462402  loss_houyan:1589.8114013671875\n",
      "Number:19 epoch: 801/1200 loss_tea:0.05107933941286103  loss_houyan:1570.334228515625\n",
      "Number:19 epoch: 811/1200 loss_tea:0.04518942719905307  loss_houyan:1737.108642578125\n",
      "Number:19 epoch: 821/1200 loss_tea:0.053831853396573336  loss_houyan:1568.9833984375\n",
      "Number:19 epoch: 831/1200 loss_tea:0.056724963896089727  loss_houyan:1581.154296875\n",
      "Number:19 epoch: 841/1200 loss_tea:0.05437923128082247  loss_houyan:1770.38818359375\n",
      "Number:19 epoch: 851/1200 loss_tea:0.060867839572652765  loss_houyan:1574.1058349609375\n",
      "Number:19 epoch: 861/1200 loss_tea:0.04893635326748571  loss_houyan:1597.84375\n",
      "Number:19 epoch: 871/1200 loss_tea:0.06346502515823343  loss_houyan:1701.093994140625\n",
      "Number:19 epoch: 881/1200 loss_tea:0.0676995791903821  loss_houyan:2109.08349609375\n",
      "Number:19 epoch: 891/1200 loss_tea:0.053186002767880436  loss_houyan:1559.6568603515625\n",
      "Number:19 epoch: 901/1200 loss_tea:0.04194035938136764  loss_houyan:1555.6568603515625\n",
      "Number:19 epoch: 911/1200 loss_tea:0.05341855920843685  loss_houyan:1648.9222412109375\n",
      "Number:19 epoch: 921/1200 loss_tea:0.0631115542502509  loss_houyan:1614.058837890625\n",
      "Number:19 epoch: 931/1200 loss_tea:0.06656860449031246  loss_houyan:1591.8394775390625\n",
      "Number:19 epoch: 941/1200 loss_tea:0.06457144943161583  loss_houyan:1905.474365234375\n",
      "Number:19 epoch: 951/1200 loss_tea:0.04702862483125383  loss_houyan:1874.2664794921875\n",
      "Number:19 epoch: 961/1200 loss_tea:0.05613795685619025  loss_houyan:1520.4083251953125\n",
      "Number:19 epoch: 971/1200 loss_tea:0.04084045188386406  loss_houyan:1583.8167724609375\n",
      "Number:19 epoch: 981/1200 loss_tea:0.0631132643776992  loss_houyan:1582.3590087890625\n",
      "Number:19 epoch: 991/1200 loss_tea:0.06532240094055235  loss_houyan:1534.182861328125\n",
      "Number:19 epoch: 1001/1200 loss_tea:0.050425072879894  loss_houyan:1898.640625\n",
      "Number:19 epoch: 1011/1200 loss_tea:0.05711406619730014  loss_houyan:1581.932861328125\n",
      "Number:19 epoch: 1021/1200 loss_tea:0.04464946183926281  loss_houyan:1658.835205078125\n",
      "Number:19 epoch: 1031/1200 loss_tea:0.044771424159845266  loss_houyan:1987.2412109375\n",
      "Number:19 epoch: 1041/1200 loss_tea:0.05747382040234599  loss_houyan:1518.1124267578125\n",
      "Number:19 epoch: 1051/1200 loss_tea:0.052123583583110304  loss_houyan:1648.5203857421875\n",
      "Number:19 epoch: 1061/1200 loss_tea:0.05384478564450548  loss_houyan:1540.8909912109375\n",
      "Number:19 epoch: 1071/1200 loss_tea:0.05077463470992468  loss_houyan:1561.7403564453125\n",
      "Number:19 epoch: 1081/1200 loss_tea:0.057257734486598925  loss_houyan:1570.8568115234375\n",
      "Number:19 epoch: 1091/1200 loss_tea:0.0610748638212379  loss_houyan:2002.8720703125\n",
      "Number:19 epoch: 1101/1200 loss_tea:0.04305039170479784  loss_houyan:1529.4818115234375\n",
      "Number:19 epoch: 1111/1200 loss_tea:0.04902584722134684  loss_houyan:1549.1422119140625\n",
      "Number:19 epoch: 1121/1200 loss_tea:0.07155572591171783  loss_houyan:1611.9593505859375\n",
      "Number:19 epoch: 1131/1200 loss_tea:0.05592695073557934  loss_houyan:1572.4810791015625\n",
      "Number:19 epoch: 1141/1200 loss_tea:0.05543682508582597  loss_houyan:1578.7691650390625\n",
      "Number:19 epoch: 1151/1200 loss_tea:0.07147566713234275  loss_houyan:1667.3260498046875\n",
      "Number:19 epoch: 1161/1200 loss_tea:0.04727002171203811  loss_houyan:1504.927734375\n",
      "Number:19 epoch: 1171/1200 loss_tea:0.06088451890764218  loss_houyan:1702.7806396484375\n",
      "Number:19 epoch: 1181/1200 loss_tea:0.06446169589845932  loss_houyan:1591.10595703125\n",
      "Number:19 epoch: 1191/1200 loss_tea:0.047120211644828956  loss_houyan:1779.336181640625\n",
      "finished training number 19 techer!\n",
      "start training number 20 techer!\n",
      "Number:20 epoch: 1/1200 loss_tea:0.0507986625946616  loss_houyan:1522.2335205078125\n",
      "Number:20 epoch: 11/1200 loss_tea:0.049800383083632487  loss_houyan:1544.3778076171875\n",
      "Number:20 epoch: 21/1200 loss_tea:0.04569242381016711  loss_houyan:1547.565673828125\n",
      "Number:20 epoch: 31/1200 loss_tea:0.04474396774715467  loss_houyan:1555.6185302734375\n",
      "Number:20 epoch: 41/1200 loss_tea:0.08162483647812806  loss_houyan:2494.9033203125\n",
      "Number:20 epoch: 51/1200 loss_tea:0.05411251506385036  loss_houyan:1538.17919921875\n",
      "Number:20 epoch: 61/1200 loss_tea:0.04961593010713148  loss_houyan:1654.7349853515625\n",
      "Number:20 epoch: 71/1200 loss_tea:0.058407172485235276  loss_houyan:2350.588623046875\n",
      "Number:20 epoch: 81/1200 loss_tea:0.059773003181839895  loss_houyan:2086.98486328125\n",
      "Number:20 epoch: 91/1200 loss_tea:0.04111626573436676  loss_houyan:1713.1849365234375\n",
      "Number:20 epoch: 101/1200 loss_tea:0.04977464278247257  loss_houyan:1624.6505126953125\n",
      "Number:20 epoch: 111/1200 loss_tea:0.05696625345010602  loss_houyan:1610.05078125\n",
      "Number:20 epoch: 121/1200 loss_tea:0.060769888874409726  loss_houyan:1622.0482177734375\n",
      "Number:20 epoch: 131/1200 loss_tea:0.06711122429676328  loss_houyan:1548.671142578125\n",
      "Number:20 epoch: 141/1200 loss_tea:0.04333274548313795  loss_houyan:1551.7532958984375\n",
      "Number:20 epoch: 151/1200 loss_tea:0.0600610640686981  loss_houyan:1634.7047119140625\n",
      "Number:20 epoch: 161/1200 loss_tea:0.05157404237969916  loss_houyan:1621.05322265625\n",
      "Number:20 epoch: 171/1200 loss_tea:0.04784272773033251  loss_houyan:1549.090087890625\n",
      "Number:20 epoch: 181/1200 loss_tea:0.06862900472523979  loss_houyan:1586.7119140625\n",
      "Number:20 epoch: 191/1200 loss_tea:0.061040755979307804  loss_houyan:1855.10205078125\n",
      "Number:20 epoch: 201/1200 loss_tea:0.04937205235995405  loss_houyan:1561.9959716796875\n",
      "Number:20 epoch: 211/1200 loss_tea:0.0664437597816335  loss_houyan:1783.151611328125\n",
      "Number:20 epoch: 221/1200 loss_tea:0.05740877776411422  loss_houyan:1571.42431640625\n",
      "Number:20 epoch: 231/1200 loss_tea:0.048891515720236485  loss_houyan:1890.3177490234375\n",
      "Number:20 epoch: 241/1200 loss_tea:0.04506001402057084  loss_houyan:1637.671142578125\n",
      "Number:20 epoch: 251/1200 loss_tea:0.05887800984307957  loss_houyan:1684.18359375\n",
      "Number:20 epoch: 261/1200 loss_tea:0.050540644103532525  loss_houyan:1991.203125\n",
      "Number:20 epoch: 271/1200 loss_tea:0.048108801112885846  loss_houyan:1682.331787109375\n",
      "Number:20 epoch: 281/1200 loss_tea:0.04882963498232214  loss_houyan:1550.783203125\n",
      "Number:20 epoch: 291/1200 loss_tea:0.0503954442369991  loss_houyan:1640.926025390625\n",
      "Number:20 epoch: 301/1200 loss_tea:0.06526065900309114  loss_houyan:1700.5068359375\n",
      "Number:20 epoch: 311/1200 loss_tea:0.05166514305081328  loss_houyan:1657.0164794921875\n",
      "Number:20 epoch: 321/1200 loss_tea:0.04860491108798971  loss_houyan:1602.754638671875\n",
      "Number:20 epoch: 331/1200 loss_tea:0.05356832120076338  loss_houyan:1709.009521484375\n",
      "Number:20 epoch: 341/1200 loss_tea:0.06609306057678535  loss_houyan:1592.2327880859375\n",
      "Number:20 epoch: 351/1200 loss_tea:0.04466578654917732  loss_houyan:1590.7718505859375\n",
      "Number:20 epoch: 361/1200 loss_tea:0.07997429321757907  loss_houyan:1586.928955078125\n",
      "Number:20 epoch: 371/1200 loss_tea:0.05377783820241334  loss_houyan:1594.6390380859375\n",
      "Number:20 epoch: 381/1200 loss_tea:0.05433533149787427  loss_houyan:1621.0093994140625\n",
      "Number:20 epoch: 391/1200 loss_tea:0.04665119505864854  loss_houyan:1658.7159423828125\n",
      "Number:20 epoch: 401/1200 loss_tea:0.04624621306012366  loss_houyan:1701.085693359375\n",
      "Number:20 epoch: 411/1200 loss_tea:0.03937961989951467  loss_houyan:1781.737548828125\n",
      "Number:20 epoch: 421/1200 loss_tea:0.0370128398830196  loss_houyan:1628.219482421875\n",
      "Number:20 epoch: 431/1200 loss_tea:0.06289381816521429  loss_houyan:2323.07666015625\n",
      "Number:20 epoch: 441/1200 loss_tea:0.04132576279850752  loss_houyan:1956.070068359375\n",
      "Number:20 epoch: 451/1200 loss_tea:0.0546161625643708  loss_houyan:1744.498291015625\n",
      "Number:20 epoch: 461/1200 loss_tea:0.05270977128287281  loss_houyan:1547.6346435546875\n",
      "Number:20 epoch: 471/1200 loss_tea:0.0467938305928938  loss_houyan:1601.356201171875\n",
      "Number:20 epoch: 481/1200 loss_tea:0.050472213162483896  loss_houyan:1582.14990234375\n",
      "Number:20 epoch: 491/1200 loss_tea:0.04365605659555008  loss_houyan:1587.3863525390625\n",
      "Number:20 epoch: 501/1200 loss_tea:0.04077013845334585  loss_houyan:1614.3470458984375\n",
      "Number:20 epoch: 511/1200 loss_tea:0.046966839903927485  loss_houyan:1750.8538818359375\n",
      "Number:20 epoch: 521/1200 loss_tea:0.050762916621781326  loss_houyan:1556.029052734375\n",
      "Number:20 epoch: 531/1200 loss_tea:0.047711877352660886  loss_houyan:1930.057373046875\n",
      "Number:20 epoch: 541/1200 loss_tea:0.05004401305033558  loss_houyan:1544.2916259765625\n",
      "Number:20 epoch: 551/1200 loss_tea:0.04995265526514812  loss_houyan:1574.3604736328125\n",
      "Number:20 epoch: 561/1200 loss_tea:0.05038555472385963  loss_houyan:1693.616455078125\n",
      "Number:20 epoch: 571/1200 loss_tea:0.07017680930555868  loss_houyan:1736.0635986328125\n",
      "Number:20 epoch: 581/1200 loss_tea:0.05782902700132703  loss_houyan:1481.3795166015625\n",
      "Number:20 epoch: 591/1200 loss_tea:0.05366280469692548  loss_houyan:1609.048828125\n",
      "Number:20 epoch: 601/1200 loss_tea:0.07147960488416416  loss_houyan:1544.675048828125\n",
      "Number:20 epoch: 611/1200 loss_tea:0.06194724658825196  loss_houyan:1622.72021484375\n",
      "Number:20 epoch: 621/1200 loss_tea:0.04475933520271635  loss_houyan:3011.823974609375\n",
      "Number:20 epoch: 631/1200 loss_tea:0.042379900894595805  loss_houyan:1557.84912109375\n",
      "Number:20 epoch: 641/1200 loss_tea:0.06571335917693752  loss_houyan:1599.9365234375\n",
      "Number:20 epoch: 651/1200 loss_tea:0.05458045945685234  loss_houyan:2044.33984375\n",
      "Number:20 epoch: 661/1200 loss_tea:0.05244995109285689  loss_houyan:1735.9066162109375\n",
      "Number:20 epoch: 671/1200 loss_tea:0.0463170166341869  loss_houyan:1528.516845703125\n",
      "Number:20 epoch: 681/1200 loss_tea:0.05097870580913906  loss_houyan:1725.69091796875\n",
      "Number:20 epoch: 691/1200 loss_tea:0.03954349257239077  loss_houyan:1708.6429443359375\n",
      "Number:20 epoch: 701/1200 loss_tea:0.04361072280748861  loss_houyan:1970.2293701171875\n",
      "Number:20 epoch: 711/1200 loss_tea:0.04664580338835076  loss_houyan:1604.8448486328125\n",
      "Number:20 epoch: 721/1200 loss_tea:0.054444275075048724  loss_houyan:1638.15576171875\n",
      "Number:20 epoch: 731/1200 loss_tea:0.05153495156471609  loss_houyan:1567.834228515625\n",
      "Number:20 epoch: 741/1200 loss_tea:0.05305239416437036  loss_houyan:1643.453369140625\n",
      "Number:20 epoch: 751/1200 loss_tea:0.06334694540356368  loss_houyan:1529.205322265625\n",
      "Number:20 epoch: 761/1200 loss_tea:0.06819008699553576  loss_houyan:1531.5113525390625\n",
      "Number:20 epoch: 771/1200 loss_tea:0.08386384806085845  loss_houyan:1551.1981201171875\n",
      "Number:20 epoch: 781/1200 loss_tea:0.07021012936668017  loss_houyan:1564.1114501953125\n",
      "Number:20 epoch: 791/1200 loss_tea:0.04220123758853834  loss_houyan:1728.2808837890625\n",
      "Number:20 epoch: 801/1200 loss_tea:0.048042129719181294  loss_houyan:1621.966796875\n",
      "Number:20 epoch: 811/1200 loss_tea:0.054290039128672384  loss_houyan:1570.445068359375\n",
      "Number:20 epoch: 821/1200 loss_tea:0.043835596613629714  loss_houyan:1724.065673828125\n",
      "Number:20 epoch: 831/1200 loss_tea:0.07728383150302569  loss_houyan:2028.58154296875\n",
      "Number:20 epoch: 841/1200 loss_tea:0.05494006674081021  loss_houyan:1542.1435546875\n",
      "Number:20 epoch: 851/1200 loss_tea:0.051267571347760534  loss_houyan:1852.05859375\n",
      "Number:20 epoch: 861/1200 loss_tea:0.05109796127470816  loss_houyan:1639.6533203125\n",
      "Number:20 epoch: 871/1200 loss_tea:0.0678475804020271  loss_houyan:1616.293701171875\n",
      "Number:20 epoch: 881/1200 loss_tea:0.06094811397819884  loss_houyan:1741.7479248046875\n",
      "Number:20 epoch: 891/1200 loss_tea:0.04637291917872798  loss_houyan:1610.9794921875\n",
      "Number:20 epoch: 901/1200 loss_tea:0.05871400200864287  loss_houyan:1599.8143310546875\n",
      "Number:20 epoch: 911/1200 loss_tea:0.057842419479560675  loss_houyan:1753.00634765625\n",
      "Number:20 epoch: 921/1200 loss_tea:0.035082696864774936  loss_houyan:1620.4259033203125\n",
      "Number:20 epoch: 931/1200 loss_tea:0.04762330983292122  loss_houyan:1822.97314453125\n",
      "Number:20 epoch: 941/1200 loss_tea:0.06689440875608646  loss_houyan:1952.6636962890625\n",
      "Number:20 epoch: 951/1200 loss_tea:0.04207192044550467  loss_houyan:1555.8082275390625\n",
      "Number:20 epoch: 961/1200 loss_tea:0.037632990901633655  loss_houyan:1568.514404296875\n",
      "Number:20 epoch: 971/1200 loss_tea:0.07942461891709877  loss_houyan:1582.091552734375\n",
      "Number:20 epoch: 981/1200 loss_tea:0.04295539610949586  loss_houyan:1577.7989501953125\n",
      "Number:20 epoch: 991/1200 loss_tea:0.04221324507559821  loss_houyan:1643.3040771484375\n",
      "Number:20 epoch: 1001/1200 loss_tea:0.055170153139216804  loss_houyan:1649.0340576171875\n",
      "Number:20 epoch: 1011/1200 loss_tea:0.057655318902572666  loss_houyan:1588.02880859375\n",
      "Number:20 epoch: 1021/1200 loss_tea:0.06614134224734979  loss_houyan:2183.134033203125\n",
      "Number:20 epoch: 1031/1200 loss_tea:0.044974226130299756  loss_houyan:1538.01123046875\n",
      "Number:20 epoch: 1041/1200 loss_tea:0.04353026067809777  loss_houyan:1522.6636962890625\n",
      "Number:20 epoch: 1051/1200 loss_tea:0.048642364224384664  loss_houyan:1526.89306640625\n",
      "Number:20 epoch: 1061/1200 loss_tea:0.058014094963872  loss_houyan:1526.9659423828125\n",
      "Number:20 epoch: 1071/1200 loss_tea:0.049160120587328895  loss_houyan:1889.078369140625\n",
      "Number:20 epoch: 1081/1200 loss_tea:0.048799482760703804  loss_houyan:1609.8431396484375\n",
      "Number:20 epoch: 1091/1200 loss_tea:0.09245302793070906  loss_houyan:1598.4189453125\n",
      "Number:20 epoch: 1101/1200 loss_tea:0.05191227589093084  loss_houyan:1521.385009765625\n",
      "Number:20 epoch: 1111/1200 loss_tea:0.05130764114858531  loss_houyan:1567.7430419921875\n",
      "Number:20 epoch: 1121/1200 loss_tea:0.04794430664665235  loss_houyan:1544.5433349609375\n",
      "Number:20 epoch: 1131/1200 loss_tea:0.047223214589018604  loss_houyan:1719.212890625\n",
      "Number:20 epoch: 1141/1200 loss_tea:0.039768505044672345  loss_houyan:1882.491455078125\n",
      "Number:20 epoch: 1151/1200 loss_tea:0.059943244905517196  loss_houyan:1691.2000732421875\n",
      "Number:20 epoch: 1161/1200 loss_tea:0.05539190435451198  loss_houyan:1783.90966796875\n",
      "Number:20 epoch: 1171/1200 loss_tea:0.05303348456968753  loss_houyan:1546.7823486328125\n",
      "Number:20 epoch: 1181/1200 loss_tea:0.04306596657459326  loss_houyan:1557.9053955078125\n",
      "Number:20 epoch: 1191/1200 loss_tea:0.060790640345955875  loss_houyan:1718.08349609375\n",
      "finished training number 20 techer!\n",
      "start training number 21 techer!\n",
      "Number:21 epoch: 1/1200 loss_tea:0.045781204359617605  loss_houyan:1531.087890625\n",
      "Number:21 epoch: 11/1200 loss_tea:0.06225429565364867  loss_houyan:1500.87109375\n",
      "Number:21 epoch: 21/1200 loss_tea:0.07412302795724393  loss_houyan:1581.800537109375\n",
      "Number:21 epoch: 31/1200 loss_tea:0.06249036725345642  loss_houyan:1524.1138916015625\n",
      "Number:21 epoch: 41/1200 loss_tea:0.058560604710604994  loss_houyan:1601.7540283203125\n",
      "Number:21 epoch: 51/1200 loss_tea:0.047866984116393035  loss_houyan:1579.128173828125\n",
      "Number:21 epoch: 61/1200 loss_tea:0.04347577821693489  loss_houyan:1575.568603515625\n",
      "Number:21 epoch: 71/1200 loss_tea:0.07357344135501702  loss_houyan:1810.50927734375\n",
      "Number:21 epoch: 81/1200 loss_tea:0.051915212602467585  loss_houyan:1631.8125\n",
      "Number:21 epoch: 91/1200 loss_tea:0.0650378220599746  loss_houyan:1612.932861328125\n",
      "Number:21 epoch: 101/1200 loss_tea:0.04951468849558808  loss_houyan:1681.249267578125\n",
      "Number:21 epoch: 111/1200 loss_tea:0.0748976861670574  loss_houyan:1674.11669921875\n",
      "Number:21 epoch: 121/1200 loss_tea:0.059395885694879906  loss_houyan:1651.297607421875\n",
      "Number:21 epoch: 131/1200 loss_tea:0.0551572771815001  loss_houyan:1843.3975830078125\n",
      "Number:21 epoch: 141/1200 loss_tea:0.05818046854515778  loss_houyan:1842.2515869140625\n",
      "Number:21 epoch: 151/1200 loss_tea:0.0687889803343573  loss_houyan:1734.0400390625\n",
      "Number:21 epoch: 161/1200 loss_tea:0.048315470809588944  loss_houyan:1576.8973388671875\n",
      "Number:21 epoch: 171/1200 loss_tea:0.060978553706283196  loss_houyan:1604.96337890625\n",
      "Number:21 epoch: 181/1200 loss_tea:0.053900235281049515  loss_houyan:1848.519775390625\n",
      "Number:21 epoch: 191/1200 loss_tea:0.0430745913211165  loss_houyan:1602.602294921875\n",
      "Number:21 epoch: 201/1200 loss_tea:0.05789910058864631  loss_houyan:1742.074462890625\n",
      "Number:21 epoch: 211/1200 loss_tea:0.06060406629049708  loss_houyan:1546.4827880859375\n",
      "Number:21 epoch: 221/1200 loss_tea:0.05598462761355444  loss_houyan:1517.2283935546875\n",
      "Number:21 epoch: 231/1200 loss_tea:0.042892410515899815  loss_houyan:1559.823486328125\n",
      "Number:21 epoch: 241/1200 loss_tea:0.06025636527918963  loss_houyan:1992.0555419921875\n",
      "Number:21 epoch: 251/1200 loss_tea:0.042832619734131085  loss_houyan:1525.9444580078125\n",
      "Number:21 epoch: 261/1200 loss_tea:0.047633913765739326  loss_houyan:1542.89892578125\n",
      "Number:21 epoch: 271/1200 loss_tea:0.06762314660382979  loss_houyan:1740.1015625\n",
      "Number:21 epoch: 281/1200 loss_tea:0.04594761352093446  loss_houyan:1548.975341796875\n",
      "Number:21 epoch: 291/1200 loss_tea:0.040823418262976974  loss_houyan:1609.3839111328125\n",
      "Number:21 epoch: 301/1200 loss_tea:0.04710998128611404  loss_houyan:1581.9835205078125\n",
      "Number:21 epoch: 311/1200 loss_tea:0.04464922168971026  loss_houyan:1563.0496826171875\n",
      "Number:21 epoch: 321/1200 loss_tea:0.0451776457006134  loss_houyan:1781.3927001953125\n",
      "Number:21 epoch: 331/1200 loss_tea:0.058773232072724585  loss_houyan:1898.3870849609375\n",
      "Number:21 epoch: 341/1200 loss_tea:0.04967869370413655  loss_houyan:1510.5665283203125\n",
      "Number:21 epoch: 351/1200 loss_tea:0.0542307176799071  loss_houyan:1682.117919921875\n",
      "Number:21 epoch: 361/1200 loss_tea:0.0601771061925728  loss_houyan:1594.395263671875\n",
      "Number:21 epoch: 371/1200 loss_tea:0.06430164381864704  loss_houyan:1777.177734375\n",
      "Number:21 epoch: 381/1200 loss_tea:0.07166947877000213  loss_houyan:2290.7998046875\n",
      "Number:21 epoch: 391/1200 loss_tea:0.07116982677952587  loss_houyan:1656.008544921875\n",
      "Number:21 epoch: 401/1200 loss_tea:0.04995003101544774  loss_houyan:1580.1856689453125\n",
      "Number:21 epoch: 411/1200 loss_tea:0.053328194776019855  loss_houyan:1531.691162109375\n",
      "Number:21 epoch: 421/1200 loss_tea:0.04683754474818042  loss_houyan:1554.4705810546875\n",
      "Number:21 epoch: 431/1200 loss_tea:0.055308182187135436  loss_houyan:1676.3834228515625\n",
      "Number:21 epoch: 441/1200 loss_tea:0.0576442758884173  loss_houyan:1627.797119140625\n",
      "Number:21 epoch: 451/1200 loss_tea:0.06419672905955089  loss_houyan:1647.7095947265625\n",
      "Number:21 epoch: 461/1200 loss_tea:0.05528936264834618  loss_houyan:1588.3917236328125\n",
      "Number:21 epoch: 471/1200 loss_tea:0.05958010908536929  loss_houyan:2049.343505859375\n",
      "Number:21 epoch: 481/1200 loss_tea:0.05828625016579967  loss_houyan:1586.459228515625\n",
      "Number:21 epoch: 491/1200 loss_tea:0.03964649608439102  loss_houyan:2176.002197265625\n",
      "Number:21 epoch: 501/1200 loss_tea:0.03959564175566174  loss_houyan:1523.364501953125\n",
      "Number:21 epoch: 511/1200 loss_tea:0.04618788770093823  loss_houyan:1565.518798828125\n",
      "Number:21 epoch: 521/1200 loss_tea:0.04867488399475469  loss_houyan:1554.30908203125\n",
      "Number:21 epoch: 531/1200 loss_tea:0.08633907582985864  loss_houyan:1797.7279052734375\n",
      "Number:21 epoch: 541/1200 loss_tea:0.07606533286553152  loss_houyan:1509.7626953125\n",
      "Number:21 epoch: 551/1200 loss_tea:0.060270905052514506  loss_houyan:1556.330322265625\n",
      "Number:21 epoch: 561/1200 loss_tea:0.05967700290132419  loss_houyan:1533.767822265625\n",
      "Number:21 epoch: 571/1200 loss_tea:0.047209974155750546  loss_houyan:1929.4837646484375\n",
      "Number:21 epoch: 581/1200 loss_tea:0.06179543543687735  loss_houyan:1714.91064453125\n",
      "Number:21 epoch: 591/1200 loss_tea:0.051857329053128565  loss_houyan:1588.2470703125\n",
      "Number:21 epoch: 601/1200 loss_tea:0.06079597958073651  loss_houyan:1578.5791015625\n",
      "Number:21 epoch: 611/1200 loss_tea:0.05359433923676498  loss_houyan:1786.930419921875\n",
      "Number:21 epoch: 621/1200 loss_tea:0.05437431601305312  loss_houyan:1561.6004638671875\n",
      "Number:21 epoch: 631/1200 loss_tea:0.0477268446088954  loss_houyan:1540.33447265625\n",
      "Number:21 epoch: 641/1200 loss_tea:0.04911163086492281  loss_houyan:1710.814697265625\n",
      "Number:21 epoch: 651/1200 loss_tea:0.062345206999352566  loss_houyan:1613.9862060546875\n",
      "Number:21 epoch: 661/1200 loss_tea:0.05173421566839978  loss_houyan:1568.27880859375\n",
      "Number:21 epoch: 671/1200 loss_tea:0.04411903019192394  loss_houyan:1698.6009521484375\n",
      "Number:21 epoch: 681/1200 loss_tea:0.04874525234519723  loss_houyan:1570.8675537109375\n",
      "Number:21 epoch: 691/1200 loss_tea:0.04092746493945165  loss_houyan:1948.9610595703125\n",
      "Number:21 epoch: 701/1200 loss_tea:0.04132143812959967  loss_houyan:1526.678466796875\n",
      "Number:21 epoch: 711/1200 loss_tea:0.05791047761993585  loss_houyan:2268.27490234375\n",
      "Number:21 epoch: 721/1200 loss_tea:0.04171040244113766  loss_houyan:1597.2227783203125\n",
      "Number:21 epoch: 731/1200 loss_tea:0.040288835179142946  loss_houyan:1589.153564453125\n",
      "Number:21 epoch: 741/1200 loss_tea:0.08090390124035093  loss_houyan:1936.6650390625\n",
      "Number:21 epoch: 751/1200 loss_tea:0.07385941925338822  loss_houyan:1617.050048828125\n",
      "Number:21 epoch: 761/1200 loss_tea:0.060241269103538324  loss_houyan:1641.0643310546875\n",
      "Number:21 epoch: 771/1200 loss_tea:0.04080035764228267  loss_houyan:1529.419921875\n",
      "Number:21 epoch: 781/1200 loss_tea:0.06731474283967202  loss_houyan:2200.4462890625\n",
      "Number:21 epoch: 791/1200 loss_tea:0.05140328280822405  loss_houyan:2488.5576171875\n",
      "Number:21 epoch: 801/1200 loss_tea:0.05508499068610034  loss_houyan:1582.5631103515625\n",
      "Number:21 epoch: 811/1200 loss_tea:0.05809108688461532  loss_houyan:1910.727783203125\n",
      "Number:21 epoch: 821/1200 loss_tea:0.048085692757995284  loss_houyan:1641.251220703125\n",
      "Number:21 epoch: 831/1200 loss_tea:0.046341087560476696  loss_houyan:1630.7745361328125\n",
      "Number:21 epoch: 841/1200 loss_tea:0.04637258008110371  loss_houyan:1605.9566650390625\n",
      "Number:21 epoch: 851/1200 loss_tea:0.04885967732938178  loss_houyan:1671.0526123046875\n",
      "Number:21 epoch: 861/1200 loss_tea:0.05809695832318131  loss_houyan:1688.965576171875\n",
      "Number:21 epoch: 871/1200 loss_tea:0.0698571992181249  loss_houyan:2001.878173828125\n",
      "Number:21 epoch: 881/1200 loss_tea:0.05352333414124796  loss_houyan:1820.48779296875\n",
      "Number:21 epoch: 891/1200 loss_tea:0.04906847066717011  loss_houyan:1560.2052001953125\n",
      "Number:21 epoch: 901/1200 loss_tea:0.06577531494923103  loss_houyan:1628.635498046875\n",
      "Number:21 epoch: 911/1200 loss_tea:0.055885176378263984  loss_houyan:2187.03466796875\n",
      "Number:21 epoch: 921/1200 loss_tea:0.060527023475059546  loss_houyan:1519.6439208984375\n",
      "Number:21 epoch: 931/1200 loss_tea:0.048081504304398885  loss_houyan:1643.3568115234375\n",
      "Number:21 epoch: 941/1200 loss_tea:0.07076237040160818  loss_houyan:1700.2064208984375\n",
      "Number:21 epoch: 951/1200 loss_tea:0.05053065414526321  loss_houyan:1600.4598388671875\n",
      "Number:21 epoch: 961/1200 loss_tea:0.05504981245478083  loss_houyan:1801.792724609375\n",
      "Number:21 epoch: 971/1200 loss_tea:0.06937180476816639  loss_houyan:1704.3076171875\n",
      "Number:21 epoch: 981/1200 loss_tea:0.04059119966859781  loss_houyan:1803.858642578125\n",
      "Number:21 epoch: 991/1200 loss_tea:0.05499681157484599  loss_houyan:1558.77978515625\n",
      "Number:21 epoch: 1001/1200 loss_tea:0.06343144923448563  loss_houyan:1636.0224609375\n",
      "Number:21 epoch: 1011/1200 loss_tea:0.0496850529901045  loss_houyan:1576.1795654296875\n",
      "Number:21 epoch: 1021/1200 loss_tea:0.04845373808782256  loss_houyan:1543.0164794921875\n",
      "Number:21 epoch: 1031/1200 loss_tea:0.049085547574466606  loss_houyan:2200.271484375\n",
      "Number:21 epoch: 1041/1200 loss_tea:0.08689306326118441  loss_houyan:1575.72509765625\n",
      "Number:21 epoch: 1051/1200 loss_tea:0.03987197266770491  loss_houyan:1622.79248046875\n",
      "Number:21 epoch: 1061/1200 loss_tea:0.05351539879866776  loss_houyan:1546.8424072265625\n",
      "Number:21 epoch: 1071/1200 loss_tea:0.04636690732833403  loss_houyan:1647.316162109375\n",
      "Number:21 epoch: 1081/1200 loss_tea:0.08666951632731196  loss_houyan:1890.3990478515625\n",
      "Number:21 epoch: 1091/1200 loss_tea:0.0536470847407683  loss_houyan:1705.064453125\n",
      "Number:21 epoch: 1101/1200 loss_tea:0.05150379767048714  loss_houyan:1504.6690673828125\n",
      "Number:21 epoch: 1111/1200 loss_tea:0.06735411590745823  loss_houyan:1571.9783935546875\n",
      "Number:21 epoch: 1121/1200 loss_tea:0.03625083540871676  loss_houyan:1548.8345947265625\n",
      "Number:21 epoch: 1131/1200 loss_tea:0.05833348373646482  loss_houyan:1522.2181396484375\n",
      "Number:21 epoch: 1141/1200 loss_tea:0.08574961910598887  loss_houyan:1587.285888671875\n",
      "Number:21 epoch: 1151/1200 loss_tea:0.05262825796212748  loss_houyan:1687.32666015625\n",
      "Number:21 epoch: 1161/1200 loss_tea:0.05482460415311096  loss_houyan:1656.5704345703125\n",
      "Number:21 epoch: 1171/1200 loss_tea:0.045054714415574786  loss_houyan:1554.0367431640625\n",
      "Number:21 epoch: 1181/1200 loss_tea:0.047218788891701724  loss_houyan:1697.4921875\n",
      "Number:21 epoch: 1191/1200 loss_tea:0.04929989861150502  loss_houyan:1712.5267333984375\n",
      "finished training number 21 techer!\n",
      "start training number 22 techer!\n",
      "Number:22 epoch: 1/1200 loss_tea:0.07186783158331837  loss_houyan:1793.86328125\n",
      "Number:22 epoch: 11/1200 loss_tea:0.052586294353726616  loss_houyan:1566.098388671875\n",
      "Number:22 epoch: 21/1200 loss_tea:0.04715840153264835  loss_houyan:1538.98681640625\n",
      "Number:22 epoch: 31/1200 loss_tea:0.046802571017716926  loss_houyan:1511.2890625\n",
      "Number:22 epoch: 41/1200 loss_tea:0.05195625588193714  loss_houyan:1758.9322509765625\n",
      "Number:22 epoch: 51/1200 loss_tea:0.06585175121919525  loss_houyan:1695.82177734375\n",
      "Number:22 epoch: 61/1200 loss_tea:0.062488997815011216  loss_houyan:1575.84423828125\n",
      "Number:22 epoch: 71/1200 loss_tea:0.06987262694234908  loss_houyan:1530.8013916015625\n",
      "Number:22 epoch: 81/1200 loss_tea:0.05141294255010604  loss_houyan:2117.6357421875\n",
      "Number:22 epoch: 91/1200 loss_tea:0.0545489249371048  loss_houyan:1684.0919189453125\n",
      "Number:22 epoch: 101/1200 loss_tea:0.045571800414152125  loss_houyan:2267.5546875\n",
      "Number:22 epoch: 111/1200 loss_tea:0.05078244455353388  loss_houyan:1657.2568359375\n",
      "Number:22 epoch: 121/1200 loss_tea:0.07978184110488225  loss_houyan:1570.560791015625\n",
      "Number:22 epoch: 131/1200 loss_tea:0.05571471406790151  loss_houyan:2135.78466796875\n",
      "Number:22 epoch: 141/1200 loss_tea:0.06041339856846168  loss_houyan:1545.0189208984375\n",
      "Number:22 epoch: 151/1200 loss_tea:0.04969997539307416  loss_houyan:1517.461181640625\n",
      "Number:22 epoch: 161/1200 loss_tea:0.05305377439989453  loss_houyan:1555.1510009765625\n",
      "Number:22 epoch: 171/1200 loss_tea:0.06670216687592583  loss_houyan:1790.741943359375\n",
      "Number:22 epoch: 181/1200 loss_tea:0.04515173802020604  loss_houyan:1595.163330078125\n",
      "Number:22 epoch: 191/1200 loss_tea:0.05289575373438874  loss_houyan:1595.6549072265625\n",
      "Number:22 epoch: 201/1200 loss_tea:0.07352738479763142  loss_houyan:1682.922607421875\n",
      "Number:22 epoch: 211/1200 loss_tea:0.046828244096137915  loss_houyan:1775.97705078125\n",
      "Number:22 epoch: 221/1200 loss_tea:0.051591929693453366  loss_houyan:1677.02734375\n",
      "Number:22 epoch: 231/1200 loss_tea:0.040471660469390616  loss_houyan:1816.47607421875\n",
      "Number:22 epoch: 241/1200 loss_tea:0.0515526274333472  loss_houyan:1555.3836669921875\n",
      "Number:22 epoch: 251/1200 loss_tea:0.05330856854097187  loss_houyan:1643.1126708984375\n",
      "Number:22 epoch: 261/1200 loss_tea:0.05002809170288718  loss_houyan:1997.465576171875\n",
      "Number:22 epoch: 271/1200 loss_tea:0.04352673662365002  loss_houyan:1703.848876953125\n",
      "Number:22 epoch: 281/1200 loss_tea:0.044017571052704256  loss_houyan:1673.931396484375\n",
      "Number:22 epoch: 291/1200 loss_tea:0.05570764437767569  loss_houyan:1627.273681640625\n",
      "Number:22 epoch: 301/1200 loss_tea:0.06743684899630106  loss_houyan:1579.24609375\n",
      "Number:22 epoch: 311/1200 loss_tea:0.07025207782566777  loss_houyan:1630.762451171875\n",
      "Number:22 epoch: 321/1200 loss_tea:0.07182816022481481  loss_houyan:1604.3753662109375\n",
      "Number:22 epoch: 331/1200 loss_tea:0.07650205191750746  loss_houyan:1635.8438720703125\n",
      "Number:22 epoch: 341/1200 loss_tea:0.04921924823724188  loss_houyan:1694.8271484375\n",
      "Number:22 epoch: 351/1200 loss_tea:0.0445669132892434  loss_houyan:1527.5201416015625\n",
      "Number:22 epoch: 361/1200 loss_tea:0.059146715370353015  loss_houyan:1600.201904296875\n",
      "Number:22 epoch: 371/1200 loss_tea:0.05091147213051477  loss_houyan:1610.7567138671875\n",
      "Number:22 epoch: 381/1200 loss_tea:0.050139818919291736  loss_houyan:1607.54638671875\n",
      "Number:22 epoch: 391/1200 loss_tea:0.05636223762810943  loss_houyan:1611.2271728515625\n",
      "Number:22 epoch: 401/1200 loss_tea:0.06383872807708037  loss_houyan:1590.5517578125\n",
      "Number:22 epoch: 411/1200 loss_tea:0.06083744351806997  loss_houyan:1562.3675537109375\n",
      "Number:22 epoch: 421/1200 loss_tea:0.05061744188662359  loss_houyan:1546.032470703125\n",
      "Number:22 epoch: 431/1200 loss_tea:0.03917677319992137  loss_houyan:1570.7744140625\n",
      "Number:22 epoch: 441/1200 loss_tea:0.056380370655582816  loss_houyan:1931.175048828125\n",
      "Number:22 epoch: 451/1200 loss_tea:0.052920128683076106  loss_houyan:1591.083251953125\n",
      "Number:22 epoch: 461/1200 loss_tea:0.05652844827302994  loss_houyan:1522.610595703125\n",
      "Number:22 epoch: 471/1200 loss_tea:0.06118105929469396  loss_houyan:1716.0723876953125\n",
      "Number:22 epoch: 481/1200 loss_tea:0.03839985122015681  loss_houyan:1567.9818115234375\n",
      "Number:22 epoch: 491/1200 loss_tea:0.06082917519814697  loss_houyan:1537.1925048828125\n",
      "Number:22 epoch: 501/1200 loss_tea:0.05550272411147858  loss_houyan:1676.0697021484375\n",
      "Number:22 epoch: 511/1200 loss_tea:0.050158009282431186  loss_houyan:1600.391845703125\n",
      "Number:22 epoch: 521/1200 loss_tea:0.04794692887826707  loss_houyan:1578.1053466796875\n",
      "Number:22 epoch: 531/1200 loss_tea:0.05161231101624681  loss_houyan:1857.47412109375\n",
      "Number:22 epoch: 541/1200 loss_tea:0.09118285415785042  loss_houyan:1568.55712890625\n",
      "Number:22 epoch: 551/1200 loss_tea:0.07072239127919358  loss_houyan:1577.6190185546875\n",
      "Number:22 epoch: 561/1200 loss_tea:0.047153774213520454  loss_houyan:1539.2698974609375\n",
      "Number:22 epoch: 571/1200 loss_tea:0.060641181336990105  loss_houyan:1590.326171875\n",
      "Number:22 epoch: 581/1200 loss_tea:0.05839965342630228  loss_houyan:1598.84228515625\n",
      "Number:22 epoch: 591/1200 loss_tea:0.05866397108904548  loss_houyan:2058.5478515625\n",
      "Number:22 epoch: 601/1200 loss_tea:0.05278303510067445  loss_houyan:1539.3565673828125\n",
      "Number:22 epoch: 611/1200 loss_tea:0.04991818435379965  loss_houyan:1585.9476318359375\n",
      "Number:22 epoch: 621/1200 loss_tea:0.06021280239770256  loss_houyan:1797.58447265625\n",
      "Number:22 epoch: 631/1200 loss_tea:0.04243307580301002  loss_houyan:1552.3565673828125\n",
      "Number:22 epoch: 641/1200 loss_tea:0.04824193042388131  loss_houyan:1735.963134765625\n",
      "Number:22 epoch: 651/1200 loss_tea:0.05379686078423789  loss_houyan:1634.888427734375\n",
      "Number:22 epoch: 661/1200 loss_tea:0.05583620339432829  loss_houyan:1656.0843505859375\n",
      "Number:22 epoch: 671/1200 loss_tea:0.04969837750008842  loss_houyan:1530.6024169921875\n",
      "Number:22 epoch: 681/1200 loss_tea:0.052923740307123816  loss_houyan:1642.97998046875\n",
      "Number:22 epoch: 691/1200 loss_tea:0.043637988946657325  loss_houyan:1560.0391845703125\n",
      "Number:22 epoch: 701/1200 loss_tea:0.042270240720821  loss_houyan:2130.2431640625\n",
      "Number:22 epoch: 711/1200 loss_tea:0.05864465336417389  loss_houyan:1717.8582763671875\n",
      "Number:22 epoch: 721/1200 loss_tea:0.07133964007142732  loss_houyan:1715.6715087890625\n",
      "Number:22 epoch: 731/1200 loss_tea:0.06394728807234827  loss_houyan:1815.0687255859375\n",
      "Number:22 epoch: 741/1200 loss_tea:0.06720454229959863  loss_houyan:1539.589111328125\n",
      "Number:22 epoch: 751/1200 loss_tea:0.06080177222379118  loss_houyan:1624.8660888671875\n",
      "Number:22 epoch: 761/1200 loss_tea:0.059082830419467  loss_houyan:2118.736328125\n",
      "Number:22 epoch: 771/1200 loss_tea:0.056321040191267914  loss_houyan:1520.1707763671875\n",
      "Number:22 epoch: 781/1200 loss_tea:0.06978076481420138  loss_houyan:1574.8961181640625\n",
      "Number:22 epoch: 791/1200 loss_tea:0.04182650787080133  loss_houyan:1591.062744140625\n",
      "Number:22 epoch: 801/1200 loss_tea:0.036200291247944466  loss_houyan:1662.5390625\n",
      "Number:22 epoch: 811/1200 loss_tea:0.049218204411202074  loss_houyan:1540.24365234375\n",
      "Number:22 epoch: 821/1200 loss_tea:0.0692653541077975  loss_houyan:1788.75439453125\n",
      "Number:22 epoch: 831/1200 loss_tea:0.04961410460132311  loss_houyan:1928.9306640625\n",
      "Number:22 epoch: 841/1200 loss_tea:0.045192517487323544  loss_houyan:1742.427978515625\n",
      "Number:22 epoch: 851/1200 loss_tea:0.057062254933399274  loss_houyan:1553.8800048828125\n",
      "Number:22 epoch: 861/1200 loss_tea:0.047358181797675765  loss_houyan:1555.908447265625\n",
      "Number:22 epoch: 871/1200 loss_tea:0.06941371856127912  loss_houyan:1799.9119873046875\n",
      "Number:22 epoch: 881/1200 loss_tea:0.06534488317163362  loss_houyan:1546.431396484375\n",
      "Number:22 epoch: 891/1200 loss_tea:0.04778304503065192  loss_houyan:1613.883544921875\n",
      "Number:22 epoch: 901/1200 loss_tea:0.06058562369796461  loss_houyan:1583.3270263671875\n",
      "Number:22 epoch: 911/1200 loss_tea:0.046544682327337245  loss_houyan:1524.9161376953125\n",
      "Number:22 epoch: 921/1200 loss_tea:0.04261700874649128  loss_houyan:2045.0892333984375\n",
      "Number:22 epoch: 931/1200 loss_tea:0.051179339064155754  loss_houyan:1531.1793212890625\n",
      "Number:22 epoch: 941/1200 loss_tea:0.047215900254293036  loss_houyan:1672.50146484375\n",
      "Number:22 epoch: 951/1200 loss_tea:0.058921988539628614  loss_houyan:1680.4998779296875\n",
      "Number:22 epoch: 961/1200 loss_tea:0.06321896238983317  loss_houyan:1583.22509765625\n",
      "Number:22 epoch: 971/1200 loss_tea:0.05504175860050697  loss_houyan:1791.353515625\n",
      "Number:22 epoch: 981/1200 loss_tea:0.05710289307528719  loss_houyan:1745.5118408203125\n",
      "Number:22 epoch: 991/1200 loss_tea:0.05829756872202912  loss_houyan:1506.2208251953125\n",
      "Number:22 epoch: 1001/1200 loss_tea:0.04326623068216578  loss_houyan:1621.91943359375\n",
      "Number:22 epoch: 1011/1200 loss_tea:0.046883115769398465  loss_houyan:1525.6734619140625\n",
      "Number:22 epoch: 1021/1200 loss_tea:0.05009888992142117  loss_houyan:1728.619140625\n",
      "Number:22 epoch: 1031/1200 loss_tea:0.05853107266757371  loss_houyan:1894.143798828125\n",
      "Number:22 epoch: 1041/1200 loss_tea:0.04538543432111897  loss_houyan:1546.26611328125\n",
      "Number:22 epoch: 1051/1200 loss_tea:0.045719241440078856  loss_houyan:1661.8646240234375\n",
      "Number:22 epoch: 1061/1200 loss_tea:0.06309223267634026  loss_houyan:1851.238525390625\n",
      "Number:22 epoch: 1071/1200 loss_tea:0.08076501719643042  loss_houyan:2444.95654296875\n",
      "Number:22 epoch: 1081/1200 loss_tea:0.061471773541261  loss_houyan:1524.5618896484375\n",
      "Number:22 epoch: 1091/1200 loss_tea:0.061820055574106254  loss_houyan:1621.7896728515625\n",
      "Number:22 epoch: 1101/1200 loss_tea:0.05337704993982124  loss_houyan:1607.09765625\n",
      "Number:22 epoch: 1111/1200 loss_tea:0.0625609303425627  loss_houyan:1543.5032958984375\n",
      "Number:22 epoch: 1121/1200 loss_tea:0.05894381317396999  loss_houyan:2031.26611328125\n",
      "Number:22 epoch: 1131/1200 loss_tea:0.04879170852693102  loss_houyan:1638.0765380859375\n",
      "Number:22 epoch: 1141/1200 loss_tea:0.05198467919836528  loss_houyan:1673.52001953125\n",
      "Number:22 epoch: 1151/1200 loss_tea:0.07212017494366434  loss_houyan:1532.2425537109375\n",
      "Number:22 epoch: 1161/1200 loss_tea:0.05070974491725259  loss_houyan:1528.503662109375\n",
      "Number:22 epoch: 1171/1200 loss_tea:0.05252981884791732  loss_houyan:1577.2103271484375\n",
      "Number:22 epoch: 1181/1200 loss_tea:0.0693873145306475  loss_houyan:1760.34375\n",
      "Number:22 epoch: 1191/1200 loss_tea:0.058394047652924136  loss_houyan:1939.938232421875\n",
      "finished training number 22 techer!\n",
      "start training number 23 techer!\n",
      "Number:23 epoch: 1/1200 loss_tea:0.04427079334009966  loss_houyan:1893.2913818359375\n",
      "Number:23 epoch: 11/1200 loss_tea:0.05190681601016778  loss_houyan:1530.1612548828125\n",
      "Number:23 epoch: 21/1200 loss_tea:0.06319250991722278  loss_houyan:1552.408203125\n",
      "Number:23 epoch: 31/1200 loss_tea:0.0538835729630232  loss_houyan:1836.442138671875\n",
      "Number:23 epoch: 41/1200 loss_tea:0.0441322852433911  loss_houyan:2051.392578125\n",
      "Number:23 epoch: 51/1200 loss_tea:0.04585455687547203  loss_houyan:1535.1126708984375\n",
      "Number:23 epoch: 61/1200 loss_tea:0.037735894093007386  loss_houyan:1817.785888671875\n",
      "Number:23 epoch: 71/1200 loss_tea:0.062484625033492475  loss_houyan:1554.3553466796875\n",
      "Number:23 epoch: 81/1200 loss_tea:0.0586360341969086  loss_houyan:1601.5953369140625\n",
      "Number:23 epoch: 91/1200 loss_tea:0.05874531277252977  loss_houyan:1608.486328125\n",
      "Number:23 epoch: 101/1200 loss_tea:0.053591782164906884  loss_houyan:1542.2855224609375\n",
      "Number:23 epoch: 111/1200 loss_tea:0.05430025066210779  loss_houyan:1594.68310546875\n",
      "Number:23 epoch: 121/1200 loss_tea:0.05120428059048647  loss_houyan:1571.4853515625\n",
      "Number:23 epoch: 131/1200 loss_tea:0.04978985872013272  loss_houyan:1794.60791015625\n",
      "Number:23 epoch: 141/1200 loss_tea:0.05958893243317749  loss_houyan:1703.12890625\n",
      "Number:23 epoch: 151/1200 loss_tea:0.037367649483163394  loss_houyan:1759.552978515625\n",
      "Number:23 epoch: 161/1200 loss_tea:0.050697269389760116  loss_houyan:1670.0052490234375\n",
      "Number:23 epoch: 171/1200 loss_tea:0.05261564752283562  loss_houyan:1646.6156005859375\n",
      "Number:23 epoch: 181/1200 loss_tea:0.04762840251905585  loss_houyan:1576.643798828125\n",
      "Number:23 epoch: 191/1200 loss_tea:0.05495776657406705  loss_houyan:1573.1156005859375\n",
      "Number:23 epoch: 201/1200 loss_tea:0.04628768714862795  loss_houyan:1573.461181640625\n",
      "Number:23 epoch: 211/1200 loss_tea:0.06832714988032297  loss_houyan:1609.8856201171875\n",
      "Number:23 epoch: 221/1200 loss_tea:0.05240744998060198  loss_houyan:1588.369384765625\n",
      "Number:23 epoch: 231/1200 loss_tea:0.051684584381073526  loss_houyan:1596.6778564453125\n",
      "Number:23 epoch: 241/1200 loss_tea:0.06967223987378691  loss_houyan:1690.8468017578125\n",
      "Number:23 epoch: 251/1200 loss_tea:0.05929644712987749  loss_houyan:1575.49365234375\n",
      "Number:23 epoch: 261/1200 loss_tea:0.04126107742897117  loss_houyan:1665.787109375\n",
      "Number:23 epoch: 271/1200 loss_tea:0.06074580220976821  loss_houyan:1569.0438232421875\n",
      "Number:23 epoch: 281/1200 loss_tea:0.07096342925349783  loss_houyan:1789.6964111328125\n",
      "Number:23 epoch: 291/1200 loss_tea:0.05350988741050323  loss_houyan:1587.354736328125\n",
      "Number:23 epoch: 301/1200 loss_tea:0.04458267498645967  loss_houyan:1577.7135009765625\n",
      "Number:23 epoch: 311/1200 loss_tea:0.0635722248852917  loss_houyan:1533.503662109375\n",
      "Number:23 epoch: 321/1200 loss_tea:0.05196364979880866  loss_houyan:1640.58154296875\n",
      "Number:23 epoch: 331/1200 loss_tea:0.05609225757844129  loss_houyan:1635.7001953125\n",
      "Number:23 epoch: 341/1200 loss_tea:0.05237741844571262  loss_houyan:1623.89013671875\n",
      "Number:23 epoch: 351/1200 loss_tea:0.040164772493182614  loss_houyan:1641.541748046875\n",
      "Number:23 epoch: 361/1200 loss_tea:0.05507111004392447  loss_houyan:1810.154296875\n",
      "Number:23 epoch: 371/1200 loss_tea:0.058456261295742995  loss_houyan:1676.8128662109375\n",
      "Number:23 epoch: 381/1200 loss_tea:0.06923850257525396  loss_houyan:1570.346435546875\n",
      "Number:23 epoch: 391/1200 loss_tea:0.04929599211039465  loss_houyan:1610.1002197265625\n",
      "Number:23 epoch: 401/1200 loss_tea:0.06287097554304519  loss_houyan:1539.698974609375\n",
      "Number:23 epoch: 411/1200 loss_tea:0.055673150601103547  loss_houyan:1748.173583984375\n",
      "Number:23 epoch: 421/1200 loss_tea:0.05382275912199144  loss_houyan:1668.9556884765625\n",
      "Number:23 epoch: 431/1200 loss_tea:0.06349516428398874  loss_houyan:1502.6015625\n",
      "Number:23 epoch: 441/1200 loss_tea:0.04552019094561236  loss_houyan:1609.1397705078125\n",
      "Number:23 epoch: 451/1200 loss_tea:0.04923988337854347  loss_houyan:1781.27001953125\n",
      "Number:23 epoch: 461/1200 loss_tea:0.0521362008297156  loss_houyan:1535.559814453125\n",
      "Number:23 epoch: 471/1200 loss_tea:0.03880943009637014  loss_houyan:1614.31005859375\n",
      "Number:23 epoch: 481/1200 loss_tea:0.07545924181273671  loss_houyan:1516.5230712890625\n",
      "Number:23 epoch: 491/1200 loss_tea:0.05121673309467378  loss_houyan:1554.4375\n",
      "Number:23 epoch: 501/1200 loss_tea:0.04680251991363076  loss_houyan:1795.276611328125\n",
      "Number:23 epoch: 511/1200 loss_tea:0.05357636567390059  loss_houyan:1797.9427490234375\n",
      "Number:23 epoch: 521/1200 loss_tea:0.06464731274333009  loss_houyan:1729.5692138671875\n",
      "Number:23 epoch: 531/1200 loss_tea:0.06479370448431228  loss_houyan:1569.1072998046875\n",
      "Number:23 epoch: 541/1200 loss_tea:0.04846525496992054  loss_houyan:1750.7474365234375\n",
      "Number:23 epoch: 551/1200 loss_tea:0.0602642649308176  loss_houyan:1925.87109375\n",
      "Number:23 epoch: 561/1200 loss_tea:0.04509845080828652  loss_houyan:1904.5823974609375\n",
      "Number:23 epoch: 571/1200 loss_tea:0.051647208370336255  loss_houyan:1551.43994140625\n",
      "Number:23 epoch: 581/1200 loss_tea:0.059382804257258344  loss_houyan:1562.951416015625\n",
      "Number:23 epoch: 591/1200 loss_tea:0.047722406069626806  loss_houyan:1568.8511962890625\n",
      "Number:23 epoch: 601/1200 loss_tea:0.05706780517131621  loss_houyan:1710.3896484375\n",
      "Number:23 epoch: 611/1200 loss_tea:0.07807841463135591  loss_houyan:1575.6849365234375\n",
      "Number:23 epoch: 621/1200 loss_tea:0.0551103112733579  loss_houyan:1685.39453125\n",
      "Number:23 epoch: 631/1200 loss_tea:0.051950346456653126  loss_houyan:1764.49853515625\n",
      "Number:23 epoch: 641/1200 loss_tea:0.05194427013542093  loss_houyan:1584.6219482421875\n",
      "Number:23 epoch: 651/1200 loss_tea:0.04444068771158246  loss_houyan:1589.1427001953125\n",
      "Number:23 epoch: 661/1200 loss_tea:0.05084994185247328  loss_houyan:1583.80859375\n",
      "Number:23 epoch: 671/1200 loss_tea:0.05999730486956255  loss_houyan:1511.1885986328125\n",
      "Number:23 epoch: 681/1200 loss_tea:0.052671870528740124  loss_houyan:1790.899169921875\n",
      "Number:23 epoch: 691/1200 loss_tea:0.05011793593628001  loss_houyan:1517.69970703125\n",
      "Number:23 epoch: 701/1200 loss_tea:0.04104532783819713  loss_houyan:1663.9483642578125\n",
      "Number:23 epoch: 711/1200 loss_tea:0.05247041014458321  loss_houyan:1560.9583740234375\n",
      "Number:23 epoch: 721/1200 loss_tea:0.054427810693157816  loss_houyan:1567.370849609375\n",
      "Number:23 epoch: 731/1200 loss_tea:0.07029912164270673  loss_houyan:1667.39453125\n",
      "Number:23 epoch: 741/1200 loss_tea:0.0514034860451827  loss_houyan:1634.5242919921875\n",
      "Number:23 epoch: 751/1200 loss_tea:0.06310497142535992  loss_houyan:1530.649169921875\n",
      "Number:23 epoch: 761/1200 loss_tea:0.05910194527183512  loss_houyan:1489.875\n",
      "Number:23 epoch: 771/1200 loss_tea:0.03649875153582758  loss_houyan:1563.6492919921875\n",
      "Number:23 epoch: 781/1200 loss_tea:0.046255988568212426  loss_houyan:2180.064697265625\n",
      "Number:23 epoch: 791/1200 loss_tea:0.057148337623390055  loss_houyan:1662.7603759765625\n",
      "Number:23 epoch: 801/1200 loss_tea:0.048277490224303665  loss_houyan:1599.603515625\n",
      "Number:23 epoch: 811/1200 loss_tea:0.05694697319356589  loss_houyan:1571.6258544921875\n",
      "Number:23 epoch: 821/1200 loss_tea:0.05089371495517806  loss_houyan:1569.903076171875\n",
      "Number:23 epoch: 831/1200 loss_tea:0.059008848798137364  loss_houyan:1557.085693359375\n",
      "Number:23 epoch: 841/1200 loss_tea:0.06001602867586134  loss_houyan:1811.4036865234375\n",
      "Number:23 epoch: 851/1200 loss_tea:0.05779951945790847  loss_houyan:1588.065185546875\n",
      "Number:23 epoch: 861/1200 loss_tea:0.056821598407467394  loss_houyan:1540.7939453125\n",
      "Number:23 epoch: 871/1200 loss_tea:0.04384636868403367  loss_houyan:1743.0577392578125\n",
      "Number:23 epoch: 881/1200 loss_tea:0.047570519267015665  loss_houyan:1580.463134765625\n",
      "Number:23 epoch: 891/1200 loss_tea:0.07626218220088099  loss_houyan:1603.9635009765625\n",
      "Number:23 epoch: 901/1200 loss_tea:0.05313871582679138  loss_houyan:2453.337646484375\n",
      "Number:23 epoch: 911/1200 loss_tea:0.053277087108686144  loss_houyan:1805.2281494140625\n",
      "Number:23 epoch: 921/1200 loss_tea:0.04793500255114978  loss_houyan:1580.796142578125\n",
      "Number:23 epoch: 931/1200 loss_tea:0.04623985899876783  loss_houyan:2241.134521484375\n",
      "Number:23 epoch: 941/1200 loss_tea:0.05703580257397372  loss_houyan:1548.5987548828125\n",
      "Number:23 epoch: 951/1200 loss_tea:0.06104398121446498  loss_houyan:1967.832275390625\n",
      "Number:23 epoch: 961/1200 loss_tea:0.052058048387855466  loss_houyan:1764.1734619140625\n",
      "Number:23 epoch: 971/1200 loss_tea:0.04102708698254692  loss_houyan:1529.892333984375\n",
      "Number:23 epoch: 981/1200 loss_tea:0.05811245797712455  loss_houyan:1834.792236328125\n",
      "Number:23 epoch: 991/1200 loss_tea:0.05286729676924014  loss_houyan:1517.548828125\n",
      "Number:23 epoch: 1001/1200 loss_tea:0.045361290251768646  loss_houyan:1730.964599609375\n",
      "Number:23 epoch: 1011/1200 loss_tea:0.053893410860103755  loss_houyan:2262.672119140625\n",
      "Number:23 epoch: 1021/1200 loss_tea:0.045961890417191335  loss_houyan:1693.6793212890625\n",
      "Number:23 epoch: 1031/1200 loss_tea:0.05459148045014292  loss_houyan:2040.7236328125\n",
      "Number:23 epoch: 1041/1200 loss_tea:0.05668742783369143  loss_houyan:1672.892822265625\n",
      "Number:23 epoch: 1051/1200 loss_tea:0.04577572903968809  loss_houyan:1559.6527099609375\n",
      "Number:23 epoch: 1061/1200 loss_tea:0.0537458447524808  loss_houyan:1571.52734375\n",
      "Number:23 epoch: 1071/1200 loss_tea:0.04239837664953122  loss_houyan:1582.2823486328125\n",
      "Number:23 epoch: 1081/1200 loss_tea:0.06739603244379413  loss_houyan:1631.9583740234375\n",
      "Number:23 epoch: 1091/1200 loss_tea:0.05529676365247496  loss_houyan:1522.8240966796875\n",
      "Number:23 epoch: 1101/1200 loss_tea:0.04923123403770952  loss_houyan:1571.639404296875\n",
      "Number:23 epoch: 1111/1200 loss_tea:0.06042772421456009  loss_houyan:1562.9544677734375\n",
      "Number:23 epoch: 1121/1200 loss_tea:0.06184069256522538  loss_houyan:1610.7242431640625\n",
      "Number:23 epoch: 1131/1200 loss_tea:0.06266594880538019  loss_houyan:1673.3238525390625\n",
      "Number:23 epoch: 1141/1200 loss_tea:0.07061040677780658  loss_houyan:1887.474853515625\n",
      "Number:23 epoch: 1151/1200 loss_tea:0.049689861548340514  loss_houyan:1518.26904296875\n",
      "Number:23 epoch: 1161/1200 loss_tea:0.06617946867846708  loss_houyan:2061.484130859375\n",
      "Number:23 epoch: 1171/1200 loss_tea:0.06549464578609948  loss_houyan:1536.6842041015625\n",
      "Number:23 epoch: 1181/1200 loss_tea:0.05455923900657128  loss_houyan:1537.9093017578125\n",
      "Number:23 epoch: 1191/1200 loss_tea:0.04226926469627194  loss_houyan:1571.0926513671875\n",
      "finished training number 23 techer!\n",
      "start training number 24 techer!\n",
      "Number:24 epoch: 1/1200 loss_tea:0.060300249376228424  loss_houyan:1791.3338623046875\n",
      "Number:24 epoch: 11/1200 loss_tea:0.0533092020070816  loss_houyan:1749.3021240234375\n",
      "Number:24 epoch: 21/1200 loss_tea:0.05860600826768697  loss_houyan:1873.88623046875\n",
      "Number:24 epoch: 31/1200 loss_tea:0.048306355168727916  loss_houyan:1851.645751953125\n",
      "Number:24 epoch: 41/1200 loss_tea:0.052395119768104646  loss_houyan:1626.61767578125\n",
      "Number:24 epoch: 51/1200 loss_tea:0.06341508016328302  loss_houyan:1564.906982421875\n",
      "Number:24 epoch: 61/1200 loss_tea:0.04340141821906939  loss_houyan:1569.696044921875\n",
      "Number:24 epoch: 71/1200 loss_tea:0.06320258726097805  loss_houyan:1559.5599365234375\n",
      "Number:24 epoch: 81/1200 loss_tea:0.04346865754373068  loss_houyan:1547.1229248046875\n",
      "Number:24 epoch: 91/1200 loss_tea:0.06041179913993497  loss_houyan:1533.392333984375\n",
      "Number:24 epoch: 101/1200 loss_tea:0.05422951581991465  loss_houyan:1573.5260009765625\n",
      "Number:24 epoch: 111/1200 loss_tea:0.05761734523363555  loss_houyan:1599.436279296875\n",
      "Number:24 epoch: 121/1200 loss_tea:0.05021905989897548  loss_houyan:1556.923828125\n",
      "Number:24 epoch: 131/1200 loss_tea:0.05762311824591193  loss_houyan:2199.14453125\n",
      "Number:24 epoch: 141/1200 loss_tea:0.044047483864621595  loss_houyan:1746.860595703125\n",
      "Number:24 epoch: 151/1200 loss_tea:0.04525172285786917  loss_houyan:1541.202392578125\n",
      "Number:24 epoch: 161/1200 loss_tea:0.0446512219366852  loss_houyan:1651.744140625\n",
      "Number:24 epoch: 171/1200 loss_tea:0.05263598215650614  loss_houyan:1714.1661376953125\n",
      "Number:24 epoch: 181/1200 loss_tea:0.06327830980584125  loss_houyan:2600.405029296875\n",
      "Number:24 epoch: 191/1200 loss_tea:0.04583889308163143  loss_houyan:1528.6121826171875\n",
      "Number:24 epoch: 201/1200 loss_tea:0.05331261615301521  loss_houyan:1590.91552734375\n",
      "Number:24 epoch: 211/1200 loss_tea:0.04123297439281072  loss_houyan:1746.2613525390625\n",
      "Number:24 epoch: 221/1200 loss_tea:0.05889867711114285  loss_houyan:1564.669921875\n",
      "Number:24 epoch: 231/1200 loss_tea:0.0560648620127877  loss_houyan:1704.630859375\n",
      "Number:24 epoch: 241/1200 loss_tea:0.03574411114513669  loss_houyan:1518.315185546875\n",
      "Number:24 epoch: 251/1200 loss_tea:0.04394906035784601  loss_houyan:1533.2581787109375\n",
      "Number:24 epoch: 261/1200 loss_tea:0.04143796222846965  loss_houyan:1625.168701171875\n",
      "Number:24 epoch: 271/1200 loss_tea:0.06839020165422559  loss_houyan:2009.880859375\n",
      "Number:24 epoch: 281/1200 loss_tea:0.08303041603579946  loss_houyan:1619.5450439453125\n",
      "Number:24 epoch: 291/1200 loss_tea:0.04661314565547268  loss_houyan:1543.109130859375\n",
      "Number:24 epoch: 301/1200 loss_tea:0.05344830014842325  loss_houyan:1785.83203125\n",
      "Number:24 epoch: 311/1200 loss_tea:0.06499010476145832  loss_houyan:1588.7603759765625\n",
      "Number:24 epoch: 321/1200 loss_tea:0.069144398212457  loss_houyan:1624.3209228515625\n",
      "Number:24 epoch: 331/1200 loss_tea:0.07850321119318057  loss_houyan:1672.0025634765625\n",
      "Number:24 epoch: 341/1200 loss_tea:0.07940332108724871  loss_houyan:1715.7237548828125\n",
      "Number:24 epoch: 351/1200 loss_tea:0.08172832354480665  loss_houyan:2171.87841796875\n",
      "Number:24 epoch: 361/1200 loss_tea:0.05656271485594064  loss_houyan:1523.814208984375\n",
      "Number:24 epoch: 371/1200 loss_tea:0.054703471352509796  loss_houyan:1560.5478515625\n",
      "Number:24 epoch: 381/1200 loss_tea:0.061488138297103014  loss_houyan:1526.1246337890625\n",
      "Number:24 epoch: 391/1200 loss_tea:0.0390917699986264  loss_houyan:1778.1463623046875\n",
      "Number:24 epoch: 401/1200 loss_tea:0.05346278677429835  loss_houyan:1711.025390625\n",
      "Number:24 epoch: 411/1200 loss_tea:0.06839317539380477  loss_houyan:1504.0858154296875\n",
      "Number:24 epoch: 421/1200 loss_tea:0.06084336480625684  loss_houyan:2305.1650390625\n",
      "Number:24 epoch: 431/1200 loss_tea:0.05587405140583564  loss_houyan:1530.0706787109375\n",
      "Number:24 epoch: 441/1200 loss_tea:0.04540297265696711  loss_houyan:1563.662353515625\n",
      "Number:24 epoch: 451/1200 loss_tea:0.05453166673108951  loss_houyan:1726.30517578125\n",
      "Number:24 epoch: 461/1200 loss_tea:0.053211631896628864  loss_houyan:1705.8330078125\n",
      "Number:24 epoch: 471/1200 loss_tea:0.05670388273220205  loss_houyan:1583.99462890625\n",
      "Number:24 epoch: 481/1200 loss_tea:0.049224024023185355  loss_houyan:1631.9169921875\n",
      "Number:24 epoch: 491/1200 loss_tea:0.05575714743940821  loss_houyan:1635.8929443359375\n",
      "Number:24 epoch: 501/1200 loss_tea:0.061335005682563164  loss_houyan:1566.60107421875\n",
      "Number:24 epoch: 511/1200 loss_tea:0.05496713399573186  loss_houyan:1573.491943359375\n",
      "Number:24 epoch: 521/1200 loss_tea:0.056607328744290905  loss_houyan:1553.9656982421875\n",
      "Number:24 epoch: 531/1200 loss_tea:0.059709695151805006  loss_houyan:1621.9248046875\n",
      "Number:24 epoch: 541/1200 loss_tea:0.06329802987300627  loss_houyan:1993.13134765625\n",
      "Number:24 epoch: 551/1200 loss_tea:0.053045817478624996  loss_houyan:2223.7998046875\n",
      "Number:24 epoch: 561/1200 loss_tea:0.052227266489741096  loss_houyan:1530.21533203125\n",
      "Number:24 epoch: 571/1200 loss_tea:0.06669094527231939  loss_houyan:1513.6524658203125\n",
      "Number:24 epoch: 581/1200 loss_tea:0.04555781365817246  loss_houyan:1833.9241943359375\n",
      "Number:24 epoch: 591/1200 loss_tea:0.03984327159228899  loss_houyan:1523.0489501953125\n",
      "Number:24 epoch: 601/1200 loss_tea:0.04993396712527817  loss_houyan:1504.07861328125\n",
      "Number:24 epoch: 611/1200 loss_tea:0.07090083253470865  loss_houyan:1517.3858642578125\n",
      "Number:24 epoch: 621/1200 loss_tea:0.04931998849050922  loss_houyan:1664.32373046875\n",
      "Number:24 epoch: 631/1200 loss_tea:0.05742668031330217  loss_houyan:1561.5062255859375\n",
      "Number:24 epoch: 641/1200 loss_tea:0.06879995708897671  loss_houyan:1619.7130126953125\n",
      "Number:24 epoch: 651/1200 loss_tea:0.046118220772027534  loss_houyan:1715.6431884765625\n",
      "Number:24 epoch: 661/1200 loss_tea:0.06037897639186343  loss_houyan:1568.1436767578125\n",
      "Number:24 epoch: 671/1200 loss_tea:0.048056840131476944  loss_houyan:1655.0072021484375\n",
      "Number:24 epoch: 681/1200 loss_tea:0.03548888018679614  loss_houyan:1804.8486328125\n",
      "Number:24 epoch: 691/1200 loss_tea:0.05399669802651271  loss_houyan:1548.534912109375\n",
      "Number:24 epoch: 701/1200 loss_tea:0.06186307129367706  loss_houyan:1649.9354248046875\n",
      "Number:24 epoch: 711/1200 loss_tea:0.061128872405780646  loss_houyan:1716.6099853515625\n",
      "Number:24 epoch: 721/1200 loss_tea:0.068081040764618  loss_houyan:2247.7646484375\n",
      "Number:24 epoch: 731/1200 loss_tea:0.05857672698377766  loss_houyan:1545.776123046875\n",
      "Number:24 epoch: 741/1200 loss_tea:0.05677801630417659  loss_houyan:1660.4952392578125\n",
      "Number:24 epoch: 751/1200 loss_tea:0.04374299552883966  loss_houyan:1537.4180908203125\n",
      "Number:24 epoch: 761/1200 loss_tea:0.05229366641942127  loss_houyan:1684.4140625\n",
      "Number:24 epoch: 771/1200 loss_tea:0.04283878225919953  loss_houyan:1511.6514892578125\n",
      "Number:24 epoch: 781/1200 loss_tea:0.04394188691107928  loss_houyan:1567.1669921875\n",
      "Number:24 epoch: 791/1200 loss_tea:0.06645900254108197  loss_houyan:1641.991943359375\n",
      "Number:24 epoch: 801/1200 loss_tea:0.052137075196170654  loss_houyan:1584.536376953125\n",
      "Number:24 epoch: 811/1200 loss_tea:0.0667407763256026  loss_houyan:1641.95947265625\n",
      "Number:24 epoch: 821/1200 loss_tea:0.06112340088584342  loss_houyan:1931.758056640625\n",
      "Number:24 epoch: 831/1200 loss_tea:0.055478101835810804  loss_houyan:1804.5078125\n",
      "Number:24 epoch: 841/1200 loss_tea:0.051079978142065725  loss_houyan:1566.5030517578125\n",
      "Number:24 epoch: 851/1200 loss_tea:0.06296123413632025  loss_houyan:1557.944091796875\n",
      "Number:24 epoch: 861/1200 loss_tea:0.05778977096892693  loss_houyan:1755.8033447265625\n",
      "Number:24 epoch: 871/1200 loss_tea:0.05238046005959812  loss_houyan:1835.78564453125\n",
      "Number:24 epoch: 881/1200 loss_tea:0.041398711389367196  loss_houyan:1570.2718505859375\n",
      "Number:24 epoch: 891/1200 loss_tea:0.04105481761418085  loss_houyan:1548.385498046875\n",
      "Number:24 epoch: 901/1200 loss_tea:0.051115943046868846  loss_houyan:1752.031494140625\n",
      "Number:24 epoch: 911/1200 loss_tea:0.05943756227747303  loss_houyan:2284.41650390625\n",
      "Number:24 epoch: 921/1200 loss_tea:0.0522895145973431  loss_houyan:1774.2716064453125\n",
      "Number:24 epoch: 931/1200 loss_tea:0.05704556272891651  loss_houyan:1571.0379638671875\n",
      "Number:24 epoch: 941/1200 loss_tea:0.04615435062675419  loss_houyan:1513.1588134765625\n",
      "Number:24 epoch: 951/1200 loss_tea:0.06331488698213864  loss_houyan:1573.9918212890625\n",
      "Number:24 epoch: 961/1200 loss_tea:0.05564955434070948  loss_houyan:2105.76220703125\n",
      "Number:24 epoch: 971/1200 loss_tea:0.0652594169190937  loss_houyan:1527.9498291015625\n",
      "Number:24 epoch: 981/1200 loss_tea:0.05659140708030224  loss_houyan:1712.2783203125\n",
      "Number:24 epoch: 991/1200 loss_tea:0.06087743916396177  loss_houyan:1545.3961181640625\n",
      "Number:24 epoch: 1001/1200 loss_tea:0.05446105922598913  loss_houyan:1576.2366943359375\n",
      "Number:24 epoch: 1011/1200 loss_tea:0.04448431817950907  loss_houyan:1583.279296875\n",
      "Number:24 epoch: 1021/1200 loss_tea:0.06525020979696472  loss_houyan:1709.49755859375\n",
      "Number:24 epoch: 1031/1200 loss_tea:0.05876799071075971  loss_houyan:1487.41015625\n",
      "Number:24 epoch: 1041/1200 loss_tea:0.06255876171352266  loss_houyan:1561.6246337890625\n",
      "Number:24 epoch: 1051/1200 loss_tea:0.049892606135591315  loss_houyan:1891.490478515625\n",
      "Number:24 epoch: 1061/1200 loss_tea:0.04722135634638229  loss_houyan:1753.1259765625\n",
      "Number:24 epoch: 1071/1200 loss_tea:0.054501687513627765  loss_houyan:1806.9503173828125\n",
      "Number:24 epoch: 1081/1200 loss_tea:0.06477652351676222  loss_houyan:1670.595703125\n",
      "Number:24 epoch: 1091/1200 loss_tea:0.05037834145942149  loss_houyan:1552.295166015625\n",
      "Number:24 epoch: 1101/1200 loss_tea:0.03732277895875708  loss_houyan:1954.523193359375\n",
      "Number:24 epoch: 1111/1200 loss_tea:0.058755370187076064  loss_houyan:1716.6993408203125\n",
      "Number:24 epoch: 1121/1200 loss_tea:0.05367368109583106  loss_houyan:1517.70947265625\n",
      "Number:24 epoch: 1131/1200 loss_tea:0.04126945302699268  loss_houyan:1596.0179443359375\n",
      "Number:24 epoch: 1141/1200 loss_tea:0.056355929550357696  loss_houyan:1666.7979736328125\n",
      "Number:24 epoch: 1151/1200 loss_tea:0.07895529840510518  loss_houyan:1580.8929443359375\n",
      "Number:24 epoch: 1161/1200 loss_tea:0.04502118556351466  loss_houyan:1579.3184814453125\n",
      "Number:24 epoch: 1171/1200 loss_tea:0.06536007008002383  loss_houyan:1527.3587646484375\n",
      "Number:24 epoch: 1181/1200 loss_tea:0.04146555428073815  loss_houyan:1544.53466796875\n",
      "Number:24 epoch: 1191/1200 loss_tea:0.05607875053262829  loss_houyan:1549.0751953125\n",
      "finished training number 24 techer!\n",
      "start training number 25 techer!\n",
      "Number:25 epoch: 1/1200 loss_tea:0.05174084552083635  loss_houyan:1831.5185546875\n",
      "Number:25 epoch: 11/1200 loss_tea:0.07224165467729882  loss_houyan:1618.8494873046875\n",
      "Number:25 epoch: 21/1200 loss_tea:0.04804668458869444  loss_houyan:1537.5848388671875\n",
      "Number:25 epoch: 31/1200 loss_tea:0.05631440358094197  loss_houyan:1788.8975830078125\n",
      "Number:25 epoch: 41/1200 loss_tea:0.06316820107564913  loss_houyan:1648.9461669921875\n",
      "Number:25 epoch: 51/1200 loss_tea:0.05412605701850835  loss_houyan:1588.7315673828125\n",
      "Number:25 epoch: 61/1200 loss_tea:0.05955012142884603  loss_houyan:1638.9971923828125\n",
      "Number:25 epoch: 71/1200 loss_tea:0.05385971586062451  loss_houyan:1654.3348388671875\n",
      "Number:25 epoch: 81/1200 loss_tea:0.042562641341488974  loss_houyan:2099.6640625\n",
      "Number:25 epoch: 91/1200 loss_tea:0.038862366694108134  loss_houyan:1523.635009765625\n",
      "Number:25 epoch: 101/1200 loss_tea:0.0488531212632729  loss_houyan:1936.4119873046875\n",
      "Number:25 epoch: 111/1200 loss_tea:0.05081755672867286  loss_houyan:1525.9327392578125\n",
      "Number:25 epoch: 121/1200 loss_tea:0.04519500571789953  loss_houyan:1522.3515625\n",
      "Number:25 epoch: 131/1200 loss_tea:0.07191625537819736  loss_houyan:1554.62158203125\n",
      "Number:25 epoch: 141/1200 loss_tea:0.057079523127221955  loss_houyan:1612.4862060546875\n",
      "Number:25 epoch: 151/1200 loss_tea:0.0507960477221364  loss_houyan:1569.3555908203125\n",
      "Number:25 epoch: 161/1200 loss_tea:0.04592511785594478  loss_houyan:1540.2457275390625\n",
      "Number:25 epoch: 171/1200 loss_tea:0.06406627458357814  loss_houyan:1725.819580078125\n",
      "Number:25 epoch: 181/1200 loss_tea:0.04271200466927474  loss_houyan:1600.374267578125\n",
      "Number:25 epoch: 191/1200 loss_tea:0.046689246893412  loss_houyan:1636.4150390625\n",
      "Number:25 epoch: 201/1200 loss_tea:0.05241204950153369  loss_houyan:1789.586181640625\n",
      "Number:25 epoch: 211/1200 loss_tea:0.04987152784018471  loss_houyan:1531.2166748046875\n",
      "Number:25 epoch: 221/1200 loss_tea:0.05689728285620056  loss_houyan:1543.465087890625\n",
      "Number:25 epoch: 231/1200 loss_tea:0.04854813959817003  loss_houyan:1539.822998046875\n",
      "Number:25 epoch: 241/1200 loss_tea:0.05728231750674665  loss_houyan:1548.89013671875\n",
      "Number:25 epoch: 251/1200 loss_tea:0.05027143538740692  loss_houyan:1572.6865234375\n",
      "Number:25 epoch: 261/1200 loss_tea:0.04844527010380896  loss_houyan:1867.92529296875\n",
      "Number:25 epoch: 271/1200 loss_tea:0.042634041862196714  loss_houyan:1616.4373779296875\n",
      "Number:25 epoch: 281/1200 loss_tea:0.05421481894282784  loss_houyan:1570.8809814453125\n",
      "Number:25 epoch: 291/1200 loss_tea:0.05428035174290287  loss_houyan:1567.8470458984375\n",
      "Number:25 epoch: 301/1200 loss_tea:0.07027732203919225  loss_houyan:1548.764404296875\n",
      "Number:25 epoch: 311/1200 loss_tea:0.06626565781728878  loss_houyan:1858.178466796875\n",
      "Number:25 epoch: 321/1200 loss_tea:0.05532071418442996  loss_houyan:1780.6641845703125\n",
      "Number:25 epoch: 331/1200 loss_tea:0.055793867876395925  loss_houyan:1630.129150390625\n",
      "Number:25 epoch: 341/1200 loss_tea:0.060792939465418264  loss_houyan:2801.90869140625\n",
      "Number:25 epoch: 351/1200 loss_tea:0.04914186251098288  loss_houyan:1487.995361328125\n",
      "Number:25 epoch: 361/1200 loss_tea:0.05065597324253505  loss_houyan:1737.36083984375\n",
      "Number:25 epoch: 371/1200 loss_tea:0.061229052332603236  loss_houyan:1621.1448974609375\n",
      "Number:25 epoch: 381/1200 loss_tea:0.050706666574887184  loss_houyan:1603.1451416015625\n",
      "Number:25 epoch: 391/1200 loss_tea:0.05571554855782903  loss_houyan:1587.283203125\n",
      "Number:25 epoch: 401/1200 loss_tea:0.055934941860175044  loss_houyan:1529.005126953125\n",
      "Number:25 epoch: 411/1200 loss_tea:0.04824696996294344  loss_houyan:1819.82373046875\n",
      "Number:25 epoch: 421/1200 loss_tea:0.06465589494593645  loss_houyan:1625.380615234375\n",
      "Number:25 epoch: 431/1200 loss_tea:0.06631150979421171  loss_houyan:1748.893310546875\n",
      "Number:25 epoch: 441/1200 loss_tea:0.06627924198762762  loss_houyan:1569.979736328125\n",
      "Number:25 epoch: 451/1200 loss_tea:0.06147225757622033  loss_houyan:1587.8958740234375\n",
      "Number:25 epoch: 461/1200 loss_tea:0.042194658232762645  loss_houyan:1743.8641357421875\n",
      "Number:25 epoch: 471/1200 loss_tea:0.05873092442165388  loss_houyan:1777.377685546875\n",
      "Number:25 epoch: 481/1200 loss_tea:0.04965269874048687  loss_houyan:1576.0543212890625\n",
      "Number:25 epoch: 491/1200 loss_tea:0.05696787673950569  loss_houyan:1607.5986328125\n",
      "Number:25 epoch: 501/1200 loss_tea:0.050919268536820016  loss_houyan:1681.9169921875\n",
      "Number:25 epoch: 511/1200 loss_tea:0.04849414383067833  loss_houyan:1515.94921875\n",
      "Number:25 epoch: 521/1200 loss_tea:0.04817630301392076  loss_houyan:1812.5616455078125\n",
      "Number:25 epoch: 531/1200 loss_tea:0.04102251997886459  loss_houyan:1546.138671875\n",
      "Number:25 epoch: 541/1200 loss_tea:0.05736684653714091  loss_houyan:1734.732421875\n",
      "Number:25 epoch: 551/1200 loss_tea:0.05666477350732781  loss_houyan:1565.267822265625\n",
      "Number:25 epoch: 561/1200 loss_tea:0.06587585859817123  loss_houyan:1576.761962890625\n",
      "Number:25 epoch: 571/1200 loss_tea:0.044075666620636215  loss_houyan:1844.315673828125\n",
      "Number:25 epoch: 581/1200 loss_tea:0.056711170926512025  loss_houyan:1514.2799072265625\n",
      "Number:25 epoch: 591/1200 loss_tea:0.04927803223685912  loss_houyan:2007.724853515625\n",
      "Number:25 epoch: 601/1200 loss_tea:0.051292840720765356  loss_houyan:1734.8359375\n",
      "Number:25 epoch: 611/1200 loss_tea:0.05966498519555987  loss_houyan:1584.1517333984375\n",
      "Number:25 epoch: 621/1200 loss_tea:0.06568283770600866  loss_houyan:1601.7613525390625\n",
      "Number:25 epoch: 631/1200 loss_tea:0.05466916183632638  loss_houyan:1515.3287353515625\n",
      "Number:25 epoch: 641/1200 loss_tea:0.0548000454673261  loss_houyan:2047.62060546875\n",
      "Number:25 epoch: 651/1200 loss_tea:0.05904460983403315  loss_houyan:1625.43603515625\n",
      "Number:25 epoch: 661/1200 loss_tea:0.0603455793642976  loss_houyan:1566.0418701171875\n",
      "Number:25 epoch: 671/1200 loss_tea:0.043751150878975936  loss_houyan:1648.5067138671875\n",
      "Number:25 epoch: 681/1200 loss_tea:0.06262295085582893  loss_houyan:1569.7730712890625\n",
      "Number:25 epoch: 691/1200 loss_tea:0.04519369844811296  loss_houyan:2020.11474609375\n",
      "Number:25 epoch: 701/1200 loss_tea:0.06639544513127099  loss_houyan:1660.98828125\n",
      "Number:25 epoch: 711/1200 loss_tea:0.046075812978785075  loss_houyan:1992.083251953125\n",
      "Number:25 epoch: 721/1200 loss_tea:0.06264913928292651  loss_houyan:1736.167236328125\n",
      "Number:25 epoch: 731/1200 loss_tea:0.035997098515137306  loss_houyan:1564.214111328125\n",
      "Number:25 epoch: 741/1200 loss_tea:0.05180609802865827  loss_houyan:1687.704833984375\n",
      "Number:25 epoch: 751/1200 loss_tea:0.03845964788451885  loss_houyan:1655.45849609375\n",
      "Number:25 epoch: 761/1200 loss_tea:0.05904154763522118  loss_houyan:2012.1058349609375\n",
      "Number:25 epoch: 771/1200 loss_tea:0.05030990289499674  loss_houyan:1609.35986328125\n",
      "Number:25 epoch: 781/1200 loss_tea:0.05627735299585511  loss_houyan:1521.1431884765625\n",
      "Number:25 epoch: 791/1200 loss_tea:0.04271925139794206  loss_houyan:1625.84521484375\n",
      "Number:25 epoch: 801/1200 loss_tea:0.04389111258063887  loss_houyan:1803.3319091796875\n",
      "Number:25 epoch: 811/1200 loss_tea:0.05573697340427012  loss_houyan:1658.87744140625\n",
      "Number:25 epoch: 821/1200 loss_tea:0.0466518275374092  loss_houyan:1595.146728515625\n",
      "Number:25 epoch: 831/1200 loss_tea:0.04026161549046578  loss_houyan:1576.929443359375\n",
      "Number:25 epoch: 841/1200 loss_tea:0.07589368026719442  loss_houyan:1721.142578125\n",
      "Number:25 epoch: 851/1200 loss_tea:0.0544428463552901  loss_houyan:1607.98828125\n",
      "Number:25 epoch: 861/1200 loss_tea:0.05902275090921329  loss_houyan:1530.8616943359375\n",
      "Number:25 epoch: 871/1200 loss_tea:0.04395456572646358  loss_houyan:1563.432861328125\n",
      "Number:25 epoch: 881/1200 loss_tea:0.05154313378116006  loss_houyan:1530.486083984375\n",
      "Number:25 epoch: 891/1200 loss_tea:0.05134182868697804  loss_houyan:1911.705810546875\n",
      "Number:25 epoch: 901/1200 loss_tea:0.04714639537852859  loss_houyan:1898.1820068359375\n",
      "Number:25 epoch: 911/1200 loss_tea:0.05250201911090332  loss_houyan:1984.528076171875\n",
      "Number:25 epoch: 921/1200 loss_tea:0.06253283488752269  loss_houyan:1572.9801025390625\n",
      "Number:25 epoch: 931/1200 loss_tea:0.06315823685297098  loss_houyan:1950.654296875\n",
      "Number:25 epoch: 941/1200 loss_tea:0.0487767935934061  loss_houyan:1560.9403076171875\n",
      "Number:25 epoch: 951/1200 loss_tea:0.06176169464205812  loss_houyan:1572.5914306640625\n",
      "Number:25 epoch: 961/1200 loss_tea:0.040488577269942795  loss_houyan:1646.4765625\n",
      "Number:25 epoch: 971/1200 loss_tea:0.052370419280959306  loss_houyan:2386.2880859375\n",
      "Number:25 epoch: 981/1200 loss_tea:0.0710489344103075  loss_houyan:1531.569580078125\n",
      "Number:25 epoch: 991/1200 loss_tea:0.0487733393159018  loss_houyan:2260.334228515625\n",
      "Number:25 epoch: 1001/1200 loss_tea:0.058434178127990406  loss_houyan:1566.3577880859375\n",
      "Number:25 epoch: 1011/1200 loss_tea:0.03784387947224475  loss_houyan:1655.427978515625\n",
      "Number:25 epoch: 1021/1200 loss_tea:0.051688509873090566  loss_houyan:1569.5009765625\n",
      "Number:25 epoch: 1031/1200 loss_tea:0.06473837201671245  loss_houyan:1641.5657958984375\n",
      "Number:25 epoch: 1041/1200 loss_tea:0.05277410768808528  loss_houyan:1540.6180419921875\n",
      "Number:25 epoch: 1051/1200 loss_tea:0.0779626190455633  loss_houyan:1542.1990966796875\n",
      "Number:25 epoch: 1061/1200 loss_tea:0.0457985392415695  loss_houyan:1562.0042724609375\n",
      "Number:25 epoch: 1071/1200 loss_tea:0.05159768361861597  loss_houyan:1595.1103515625\n",
      "Number:25 epoch: 1081/1200 loss_tea:0.06641231443456051  loss_houyan:1508.0975341796875\n",
      "Number:25 epoch: 1091/1200 loss_tea:0.044741035361542426  loss_houyan:1651.420654296875\n",
      "Number:25 epoch: 1101/1200 loss_tea:0.04709836713711646  loss_houyan:1815.649658203125\n",
      "Number:25 epoch: 1111/1200 loss_tea:0.04535973901093211  loss_houyan:1557.6795654296875\n",
      "Number:25 epoch: 1121/1200 loss_tea:0.05289364086774551  loss_houyan:1558.629638671875\n",
      "Number:25 epoch: 1131/1200 loss_tea:0.045020139325128675  loss_houyan:1619.128662109375\n",
      "Number:25 epoch: 1141/1200 loss_tea:0.04052158681147973  loss_houyan:1575.9820556640625\n",
      "Number:25 epoch: 1151/1200 loss_tea:0.04545435675336676  loss_houyan:1737.852294921875\n",
      "Number:25 epoch: 1161/1200 loss_tea:0.0631779455054451  loss_houyan:1712.492431640625\n",
      "Number:25 epoch: 1171/1200 loss_tea:0.0412915992009717  loss_houyan:1574.0570068359375\n",
      "Number:25 epoch: 1181/1200 loss_tea:0.04744756862850162  loss_houyan:1667.613525390625\n",
      "Number:25 epoch: 1191/1200 loss_tea:0.05480396724989872  loss_houyan:1532.606201171875\n",
      "finished training number 25 techer!\n",
      "start training number 26 techer!\n",
      "Number:26 epoch: 1/1200 loss_tea:0.11497251351992321  loss_houyan:2110.34912109375\n",
      "Number:26 epoch: 11/1200 loss_tea:0.06047669111179248  loss_houyan:1542.6302490234375\n",
      "Number:26 epoch: 21/1200 loss_tea:0.05424594520920378  loss_houyan:1783.1981201171875\n",
      "Number:26 epoch: 31/1200 loss_tea:0.06397065764388526  loss_houyan:1578.390869140625\n",
      "Number:26 epoch: 41/1200 loss_tea:0.046247565056030775  loss_houyan:2066.795654296875\n",
      "Number:26 epoch: 51/1200 loss_tea:0.045662540238420805  loss_houyan:2073.36669921875\n",
      "Number:26 epoch: 61/1200 loss_tea:0.04319975997310178  loss_houyan:1532.296875\n",
      "Number:26 epoch: 71/1200 loss_tea:0.05952611241164431  loss_houyan:1585.9476318359375\n",
      "Number:26 epoch: 81/1200 loss_tea:0.04785682666795515  loss_houyan:1562.8779296875\n",
      "Number:26 epoch: 91/1200 loss_tea:0.06951568276444645  loss_houyan:1758.220458984375\n",
      "Number:26 epoch: 101/1200 loss_tea:0.0503895897178686  loss_houyan:1526.67236328125\n",
      "Number:26 epoch: 111/1200 loss_tea:0.050912425273617  loss_houyan:1575.251953125\n",
      "Number:26 epoch: 121/1200 loss_tea:0.06333182481696109  loss_houyan:1539.2669677734375\n",
      "Number:26 epoch: 131/1200 loss_tea:0.05954322455111715  loss_houyan:1608.241943359375\n",
      "Number:26 epoch: 141/1200 loss_tea:0.053112070922978816  loss_houyan:1968.4552001953125\n",
      "Number:26 epoch: 151/1200 loss_tea:0.04004049712876173  loss_houyan:1578.3685302734375\n",
      "Number:26 epoch: 161/1200 loss_tea:0.06552465894844867  loss_houyan:1796.4920654296875\n",
      "Number:26 epoch: 171/1200 loss_tea:0.05458236083460767  loss_houyan:1608.951171875\n",
      "Number:26 epoch: 181/1200 loss_tea:0.042698748535005965  loss_houyan:1598.7030029296875\n",
      "Number:26 epoch: 191/1200 loss_tea:0.06258411315345791  loss_houyan:1747.9185791015625\n",
      "Number:26 epoch: 201/1200 loss_tea:0.046175225879502776  loss_houyan:1653.86181640625\n",
      "Number:26 epoch: 211/1200 loss_tea:0.04912027424846146  loss_houyan:1568.5196533203125\n",
      "Number:26 epoch: 221/1200 loss_tea:0.05996257323618538  loss_houyan:1563.149658203125\n",
      "Number:26 epoch: 231/1200 loss_tea:0.05827682108707169  loss_houyan:1752.6131591796875\n",
      "Number:26 epoch: 241/1200 loss_tea:0.06274860484100364  loss_houyan:1608.662109375\n",
      "Number:26 epoch: 251/1200 loss_tea:0.04190169209914336  loss_houyan:1675.0640869140625\n",
      "Number:26 epoch: 261/1200 loss_tea:0.052725201651849365  loss_houyan:1562.212646484375\n",
      "Number:26 epoch: 271/1200 loss_tea:0.07876755007128979  loss_houyan:1681.400390625\n",
      "Number:26 epoch: 281/1200 loss_tea:0.046355531912192126  loss_houyan:1577.5716552734375\n",
      "Number:26 epoch: 291/1200 loss_tea:0.04322805031998234  loss_houyan:1567.188720703125\n",
      "Number:26 epoch: 301/1200 loss_tea:0.05150335242397176  loss_houyan:1515.5830078125\n",
      "Number:26 epoch: 311/1200 loss_tea:0.05373332724473195  loss_houyan:1652.2503662109375\n",
      "Number:26 epoch: 321/1200 loss_tea:0.07637193090934515  loss_houyan:1589.6966552734375\n",
      "Number:26 epoch: 331/1200 loss_tea:0.05507088312152936  loss_houyan:1524.599853515625\n",
      "Number:26 epoch: 341/1200 loss_tea:0.05602733809549979  loss_houyan:1595.6405029296875\n",
      "Number:26 epoch: 351/1200 loss_tea:0.03860832285683045  loss_houyan:1643.2720947265625\n",
      "Number:26 epoch: 361/1200 loss_tea:0.04148370557511254  loss_houyan:1562.6690673828125\n",
      "Number:26 epoch: 371/1200 loss_tea:0.05603809660371919  loss_houyan:1628.766845703125\n",
      "Number:26 epoch: 381/1200 loss_tea:0.07226377629318627  loss_houyan:1534.466796875\n",
      "Number:26 epoch: 391/1200 loss_tea:0.053529667464877785  loss_houyan:1544.6748046875\n",
      "Number:26 epoch: 401/1200 loss_tea:0.05702418930064951  loss_houyan:1596.1002197265625\n",
      "Number:26 epoch: 411/1200 loss_tea:0.047921688672240406  loss_houyan:1638.41796875\n",
      "Number:26 epoch: 421/1200 loss_tea:0.043275913122135155  loss_houyan:1545.9747314453125\n",
      "Number:26 epoch: 431/1200 loss_tea:0.050869178902965245  loss_houyan:1595.34521484375\n",
      "Number:26 epoch: 441/1200 loss_tea:0.039878102436326696  loss_houyan:1561.8011474609375\n",
      "Number:26 epoch: 451/1200 loss_tea:0.04626423661446726  loss_houyan:1570.603271484375\n",
      "Number:26 epoch: 461/1200 loss_tea:0.055288663360725805  loss_houyan:1540.6053466796875\n",
      "Number:26 epoch: 471/1200 loss_tea:0.07861234693029087  loss_houyan:2093.773193359375\n",
      "Number:26 epoch: 481/1200 loss_tea:0.0502492006402557  loss_houyan:1539.2769775390625\n",
      "Number:26 epoch: 491/1200 loss_tea:0.04909027058458386  loss_houyan:1779.1953125\n",
      "Number:26 epoch: 501/1200 loss_tea:0.05040856942244693  loss_houyan:1515.3441162109375\n",
      "Number:26 epoch: 511/1200 loss_tea:0.0612383937243963  loss_houyan:1568.8905029296875\n",
      "Number:26 epoch: 521/1200 loss_tea:0.07078023313425658  loss_houyan:1649.908935546875\n",
      "Number:26 epoch: 531/1200 loss_tea:0.04759314354431395  loss_houyan:1566.0006103515625\n",
      "Number:26 epoch: 541/1200 loss_tea:0.05836540582222521  loss_houyan:1555.9412841796875\n",
      "Number:26 epoch: 551/1200 loss_tea:0.050278085840238546  loss_houyan:1573.324951171875\n",
      "Number:26 epoch: 561/1200 loss_tea:0.051097642050832295  loss_houyan:1518.563720703125\n",
      "Number:26 epoch: 571/1200 loss_tea:0.05039533756708046  loss_houyan:1546.30517578125\n",
      "Number:26 epoch: 581/1200 loss_tea:0.04205876002674864  loss_houyan:1539.1639404296875\n",
      "Number:26 epoch: 591/1200 loss_tea:0.04567061752044698  loss_houyan:1649.57666015625\n",
      "Number:26 epoch: 601/1200 loss_tea:0.06747173702546899  loss_houyan:3965.47412109375\n",
      "Number:26 epoch: 611/1200 loss_tea:0.05467363394018137  loss_houyan:1567.83251953125\n",
      "Number:26 epoch: 621/1200 loss_tea:0.048504359913012364  loss_houyan:1815.1207275390625\n",
      "Number:26 epoch: 631/1200 loss_tea:0.05460430614398225  loss_houyan:1553.3426513671875\n",
      "Number:26 epoch: 641/1200 loss_tea:0.0583669798407558  loss_houyan:1516.7938232421875\n",
      "Number:26 epoch: 651/1200 loss_tea:0.07385776247938611  loss_houyan:1519.9351806640625\n",
      "Number:26 epoch: 661/1200 loss_tea:0.07293598792491737  loss_houyan:1797.6943359375\n",
      "Number:26 epoch: 671/1200 loss_tea:0.06547222680798759  loss_houyan:1651.1202392578125\n",
      "Number:26 epoch: 681/1200 loss_tea:0.05244325560415554  loss_houyan:1553.8070068359375\n",
      "Number:26 epoch: 691/1200 loss_tea:0.045124040329132774  loss_houyan:1579.2666015625\n",
      "Number:26 epoch: 701/1200 loss_tea:0.05495211298990423  loss_houyan:1739.0350341796875\n",
      "Number:26 epoch: 711/1200 loss_tea:0.06295599208694613  loss_houyan:1606.449951171875\n",
      "Number:26 epoch: 721/1200 loss_tea:0.039400179511064795  loss_houyan:2037.8037109375\n",
      "Number:26 epoch: 731/1200 loss_tea:0.058958838187672724  loss_houyan:1716.98291015625\n",
      "Number:26 epoch: 741/1200 loss_tea:0.060340771114678914  loss_houyan:1977.4466552734375\n",
      "Number:26 epoch: 751/1200 loss_tea:0.0767740780736286  loss_houyan:1549.4974365234375\n",
      "Number:26 epoch: 761/1200 loss_tea:0.06803644764681734  loss_houyan:1577.4505615234375\n",
      "Number:26 epoch: 771/1200 loss_tea:0.052913388242261813  loss_houyan:2159.72705078125\n",
      "Number:26 epoch: 781/1200 loss_tea:0.04599440042385996  loss_houyan:1600.5223388671875\n",
      "Number:26 epoch: 791/1200 loss_tea:0.040984139563234705  loss_houyan:1532.91064453125\n",
      "Number:26 epoch: 801/1200 loss_tea:0.051855868248486654  loss_houyan:1605.338623046875\n",
      "Number:26 epoch: 811/1200 loss_tea:0.043132758731237904  loss_houyan:1709.8270263671875\n",
      "Number:26 epoch: 821/1200 loss_tea:0.049831134565770686  loss_houyan:1558.9564208984375\n",
      "Number:26 epoch: 831/1200 loss_tea:0.05045085746047165  loss_houyan:1754.4471435546875\n",
      "Number:26 epoch: 841/1200 loss_tea:0.050560220275794855  loss_houyan:1576.87646484375\n",
      "Number:26 epoch: 851/1200 loss_tea:0.060519307833681805  loss_houyan:1794.3614501953125\n",
      "Number:26 epoch: 861/1200 loss_tea:0.048273579083746784  loss_houyan:1698.181640625\n",
      "Number:26 epoch: 871/1200 loss_tea:0.0523633934143936  loss_houyan:1652.9879150390625\n",
      "Number:26 epoch: 881/1200 loss_tea:0.06042016205974549  loss_houyan:1750.3641357421875\n",
      "Number:26 epoch: 891/1200 loss_tea:0.07844022458393926  loss_houyan:1569.428466796875\n",
      "Number:26 epoch: 901/1200 loss_tea:0.05141974544933216  loss_houyan:1910.8251953125\n",
      "Number:26 epoch: 911/1200 loss_tea:0.04747361941679977  loss_houyan:1554.9488525390625\n",
      "Number:26 epoch: 921/1200 loss_tea:0.05124542520962337  loss_houyan:1514.963134765625\n",
      "Number:26 epoch: 931/1200 loss_tea:0.07971884610562617  loss_houyan:2233.7763671875\n",
      "Number:26 epoch: 941/1200 loss_tea:0.047173196956566625  loss_houyan:1579.8916015625\n",
      "Number:26 epoch: 951/1200 loss_tea:0.038922330775445886  loss_houyan:1734.4893798828125\n",
      "Number:26 epoch: 961/1200 loss_tea:0.06701895148158134  loss_houyan:1553.7154541015625\n",
      "Number:26 epoch: 971/1200 loss_tea:0.04605676430408255  loss_houyan:1663.7913818359375\n",
      "Number:26 epoch: 981/1200 loss_tea:0.07157024166646553  loss_houyan:1804.7142333984375\n",
      "Number:26 epoch: 991/1200 loss_tea:0.057447256797693365  loss_houyan:1553.7567138671875\n",
      "Number:26 epoch: 1001/1200 loss_tea:0.04847734297252399  loss_houyan:1546.156982421875\n",
      "Number:26 epoch: 1011/1200 loss_tea:0.06013735570126708  loss_houyan:1523.0230712890625\n",
      "Number:26 epoch: 1021/1200 loss_tea:0.061750366743552144  loss_houyan:1541.810302734375\n",
      "Number:26 epoch: 1031/1200 loss_tea:0.05989055918951809  loss_houyan:1762.5341796875\n",
      "Number:26 epoch: 1041/1200 loss_tea:0.053731394772083385  loss_houyan:1558.62548828125\n",
      "Number:26 epoch: 1051/1200 loss_tea:0.04408175633088017  loss_houyan:1774.3758544921875\n",
      "Number:26 epoch: 1061/1200 loss_tea:0.0564656845043612  loss_houyan:1842.1978759765625\n",
      "Number:26 epoch: 1071/1200 loss_tea:0.052338009774126525  loss_houyan:1619.03662109375\n",
      "Number:26 epoch: 1081/1200 loss_tea:0.05140328162959261  loss_houyan:1520.927001953125\n",
      "Number:26 epoch: 1091/1200 loss_tea:0.06717694326118465  loss_houyan:1533.5357666015625\n",
      "Number:26 epoch: 1101/1200 loss_tea:0.046667234602759476  loss_houyan:1496.11328125\n",
      "Number:26 epoch: 1111/1200 loss_tea:0.0571198167993559  loss_houyan:1634.072265625\n",
      "Number:26 epoch: 1121/1200 loss_tea:0.07340318305617478  loss_houyan:1784.4208984375\n",
      "Number:26 epoch: 1131/1200 loss_tea:0.056426028950233825  loss_houyan:1618.7684326171875\n",
      "Number:26 epoch: 1141/1200 loss_tea:0.05160062198680145  loss_houyan:1507.5931396484375\n",
      "Number:26 epoch: 1151/1200 loss_tea:0.043376425152890884  loss_houyan:2069.527099609375\n",
      "Number:26 epoch: 1161/1200 loss_tea:0.04094059766319833  loss_houyan:1633.7528076171875\n",
      "Number:26 epoch: 1171/1200 loss_tea:0.08610567542580377  loss_houyan:1533.7894287109375\n",
      "Number:26 epoch: 1181/1200 loss_tea:0.05933232506458301  loss_houyan:1551.9580078125\n",
      "Number:26 epoch: 1191/1200 loss_tea:0.053103949443027994  loss_houyan:1602.2479248046875\n",
      "finished training number 26 techer!\n",
      "start training number 27 techer!\n",
      "Number:27 epoch: 1/1200 loss_tea:0.06234099327045002  loss_houyan:1743.11865234375\n",
      "Number:27 epoch: 11/1200 loss_tea:0.04928146592698462  loss_houyan:1580.271728515625\n",
      "Number:27 epoch: 21/1200 loss_tea:0.0429441915628445  loss_houyan:1674.4390869140625\n",
      "Number:27 epoch: 31/1200 loss_tea:0.06460410925690428  loss_houyan:1534.8421630859375\n",
      "Number:27 epoch: 41/1200 loss_tea:0.04950046271888597  loss_houyan:1577.550537109375\n",
      "Number:27 epoch: 51/1200 loss_tea:0.04226072835238907  loss_houyan:1578.2384033203125\n",
      "Number:27 epoch: 61/1200 loss_tea:0.054259446487117446  loss_houyan:1519.3021240234375\n",
      "Number:27 epoch: 71/1200 loss_tea:0.058643596723346736  loss_houyan:1801.775390625\n",
      "Number:27 epoch: 81/1200 loss_tea:0.059590831003571564  loss_houyan:1562.4202880859375\n",
      "Number:27 epoch: 91/1200 loss_tea:0.055882416718373636  loss_houyan:1552.7010498046875\n",
      "Number:27 epoch: 101/1200 loss_tea:0.07046098821959275  loss_houyan:1533.272216796875\n",
      "Number:27 epoch: 111/1200 loss_tea:0.04921865550786444  loss_houyan:1651.8851318359375\n",
      "Number:27 epoch: 121/1200 loss_tea:0.07321406197438954  loss_houyan:1524.826171875\n",
      "Number:27 epoch: 131/1200 loss_tea:0.06872132805066479  loss_houyan:1949.19677734375\n",
      "Number:27 epoch: 141/1200 loss_tea:0.0642809505391989  loss_houyan:1724.10693359375\n",
      "Number:27 epoch: 151/1200 loss_tea:0.03786113951430675  loss_houyan:1767.408203125\n",
      "Number:27 epoch: 161/1200 loss_tea:0.05800726482445095  loss_houyan:1559.415283203125\n",
      "Number:27 epoch: 171/1200 loss_tea:0.07133224565729707  loss_houyan:1641.504638671875\n",
      "Number:27 epoch: 181/1200 loss_tea:0.06627044209247239  loss_houyan:1685.344970703125\n",
      "Number:27 epoch: 191/1200 loss_tea:0.05225752480743094  loss_houyan:1544.8447265625\n",
      "Number:27 epoch: 201/1200 loss_tea:0.05965311301606904  loss_houyan:1522.6571044921875\n",
      "Number:27 epoch: 211/1200 loss_tea:0.0649647966426425  loss_houyan:1562.2442626953125\n",
      "Number:27 epoch: 221/1200 loss_tea:0.045019220857333384  loss_houyan:1721.4697265625\n",
      "Number:27 epoch: 231/1200 loss_tea:0.052146087958427824  loss_houyan:1628.413818359375\n",
      "Number:27 epoch: 241/1200 loss_tea:0.04605497852426588  loss_houyan:1669.0369873046875\n",
      "Number:27 epoch: 251/1200 loss_tea:0.06482710273465164  loss_houyan:1544.4774169921875\n",
      "Number:27 epoch: 261/1200 loss_tea:0.05135905637081538  loss_houyan:1585.20703125\n",
      "Number:27 epoch: 271/1200 loss_tea:0.05751772353820637  loss_houyan:1555.86474609375\n",
      "Number:27 epoch: 281/1200 loss_tea:0.061628819489125325  loss_houyan:1633.6865234375\n",
      "Number:27 epoch: 291/1200 loss_tea:0.04852836818734427  loss_houyan:1531.4287109375\n",
      "Number:27 epoch: 301/1200 loss_tea:0.0513422340916759  loss_houyan:1557.76611328125\n",
      "Number:27 epoch: 311/1200 loss_tea:0.05452974461711731  loss_houyan:1743.92919921875\n",
      "Number:27 epoch: 321/1200 loss_tea:0.05548681908226554  loss_houyan:1932.531982421875\n",
      "Number:27 epoch: 331/1200 loss_tea:0.05590890429827977  loss_houyan:1567.7138671875\n",
      "Number:27 epoch: 341/1200 loss_tea:0.05457907061248939  loss_houyan:1546.604736328125\n",
      "Number:27 epoch: 351/1200 loss_tea:0.05400343592444098  loss_houyan:1622.137451171875\n",
      "Number:27 epoch: 361/1200 loss_tea:0.0456489246909812  loss_houyan:1771.6031494140625\n",
      "Number:27 epoch: 371/1200 loss_tea:0.04479857353791247  loss_houyan:1761.645263671875\n",
      "Number:27 epoch: 381/1200 loss_tea:0.05193561650940902  loss_houyan:1553.564453125\n",
      "Number:27 epoch: 391/1200 loss_tea:0.06964020717022075  loss_houyan:1537.0626220703125\n",
      "Number:27 epoch: 401/1200 loss_tea:0.057138505301417335  loss_houyan:1556.5517578125\n",
      "Number:27 epoch: 411/1200 loss_tea:0.04832414777393715  loss_houyan:1575.85693359375\n",
      "Number:27 epoch: 421/1200 loss_tea:0.04268442302548393  loss_houyan:1570.7490234375\n",
      "Number:27 epoch: 431/1200 loss_tea:0.05270714784772598  loss_houyan:1749.878173828125\n",
      "Number:27 epoch: 441/1200 loss_tea:0.06778552308293799  loss_houyan:1619.577392578125\n",
      "Number:27 epoch: 451/1200 loss_tea:0.052049687795125206  loss_houyan:1546.925537109375\n",
      "Number:27 epoch: 461/1200 loss_tea:0.04999899349884874  loss_houyan:1623.0712890625\n",
      "Number:27 epoch: 471/1200 loss_tea:0.04827246272131719  loss_houyan:1590.615966796875\n",
      "Number:27 epoch: 481/1200 loss_tea:0.07101174373031025  loss_houyan:1957.9140625\n",
      "Number:27 epoch: 491/1200 loss_tea:0.05533523359806619  loss_houyan:1741.2620849609375\n",
      "Number:27 epoch: 501/1200 loss_tea:0.06187692782220049  loss_houyan:1515.418701171875\n",
      "Number:27 epoch: 511/1200 loss_tea:0.05569783050068144  loss_houyan:1537.1204833984375\n",
      "Number:27 epoch: 521/1200 loss_tea:0.052364972825804434  loss_houyan:1911.2626953125\n",
      "Number:27 epoch: 531/1200 loss_tea:0.05327308456307243  loss_houyan:1564.103759765625\n",
      "Number:27 epoch: 541/1200 loss_tea:0.045846208149417464  loss_houyan:1780.7366943359375\n",
      "Number:27 epoch: 551/1200 loss_tea:0.04888084615190816  loss_houyan:1698.118896484375\n",
      "Number:27 epoch: 561/1200 loss_tea:0.061641082680352746  loss_houyan:1730.3890380859375\n",
      "Number:27 epoch: 571/1200 loss_tea:0.05354713401814716  loss_houyan:1536.807373046875\n",
      "Number:27 epoch: 581/1200 loss_tea:0.05208006371487991  loss_houyan:1612.8841552734375\n",
      "Number:27 epoch: 591/1200 loss_tea:0.05556728420275436  loss_houyan:1638.5975341796875\n",
      "Number:27 epoch: 601/1200 loss_tea:0.059737392621790564  loss_houyan:1603.4761962890625\n",
      "Number:27 epoch: 611/1200 loss_tea:0.04902312906294101  loss_houyan:1549.5997314453125\n",
      "Number:27 epoch: 621/1200 loss_tea:0.046287906792860704  loss_houyan:1801.04150390625\n",
      "Number:27 epoch: 631/1200 loss_tea:0.052949854970907956  loss_houyan:1582.9647216796875\n",
      "Number:27 epoch: 641/1200 loss_tea:0.051756254070039844  loss_houyan:2077.21240234375\n",
      "Number:27 epoch: 651/1200 loss_tea:0.050327920263034384  loss_houyan:1524.7181396484375\n",
      "Number:27 epoch: 661/1200 loss_tea:0.04252252814748376  loss_houyan:2106.871826171875\n",
      "Number:27 epoch: 671/1200 loss_tea:0.050693102746881984  loss_houyan:2024.199951171875\n",
      "Number:27 epoch: 681/1200 loss_tea:0.0530756256353138  loss_houyan:1629.9342041015625\n",
      "Number:27 epoch: 691/1200 loss_tea:0.05134213949480664  loss_houyan:1704.865234375\n",
      "Number:27 epoch: 701/1200 loss_tea:0.05793753707981771  loss_houyan:1664.3544921875\n",
      "Number:27 epoch: 711/1200 loss_tea:0.07713818401460443  loss_houyan:1761.95703125\n",
      "Number:27 epoch: 721/1200 loss_tea:0.05127875936376631  loss_houyan:1532.80517578125\n",
      "Number:27 epoch: 731/1200 loss_tea:0.043308227701976225  loss_houyan:1545.4010009765625\n",
      "Number:27 epoch: 741/1200 loss_tea:0.06790797031943509  loss_houyan:1545.7247314453125\n",
      "Number:27 epoch: 751/1200 loss_tea:0.07152802442216138  loss_houyan:2116.0537109375\n",
      "Number:27 epoch: 761/1200 loss_tea:0.049472037007089374  loss_houyan:1570.329833984375\n",
      "Number:27 epoch: 771/1200 loss_tea:0.058789025467731994  loss_houyan:1617.10302734375\n",
      "Number:27 epoch: 781/1200 loss_tea:0.059413723074124604  loss_houyan:1513.8861083984375\n",
      "Number:27 epoch: 791/1200 loss_tea:0.0454097494142685  loss_houyan:1863.1749267578125\n",
      "Number:27 epoch: 801/1200 loss_tea:0.07238482168528723  loss_houyan:1718.34228515625\n",
      "Number:27 epoch: 811/1200 loss_tea:0.049691098089676436  loss_houyan:2083.445556640625\n",
      "Number:27 epoch: 821/1200 loss_tea:0.05689048841791837  loss_houyan:1555.140625\n",
      "Number:27 epoch: 831/1200 loss_tea:0.0645486903662669  loss_houyan:1624.9566650390625\n",
      "Number:27 epoch: 841/1200 loss_tea:0.041309006269572235  loss_houyan:1725.667236328125\n",
      "Number:27 epoch: 851/1200 loss_tea:0.0645997775199806  loss_houyan:1655.5791015625\n",
      "Number:27 epoch: 861/1200 loss_tea:0.04793438661020307  loss_houyan:1840.8756103515625\n",
      "Number:27 epoch: 871/1200 loss_tea:0.057483993953590766  loss_houyan:1930.81396484375\n",
      "Number:27 epoch: 881/1200 loss_tea:0.05017977450718299  loss_houyan:1800.6578369140625\n",
      "Number:27 epoch: 891/1200 loss_tea:0.05770587675078741  loss_houyan:1714.94970703125\n",
      "Number:27 epoch: 901/1200 loss_tea:0.06880558754759072  loss_houyan:1928.149169921875\n",
      "Number:27 epoch: 911/1200 loss_tea:0.06499985836488541  loss_houyan:1656.0130615234375\n",
      "Number:27 epoch: 921/1200 loss_tea:0.058839566346898554  loss_houyan:1743.823486328125\n",
      "Number:27 epoch: 931/1200 loss_tea:0.05571851189685346  loss_houyan:1570.402099609375\n",
      "Number:27 epoch: 941/1200 loss_tea:0.07291637951723592  loss_houyan:1506.4285888671875\n",
      "Number:27 epoch: 951/1200 loss_tea:0.05116020709559916  loss_houyan:1579.2047119140625\n",
      "Number:27 epoch: 961/1200 loss_tea:0.06143134198698023  loss_houyan:1768.4346923828125\n",
      "Number:27 epoch: 971/1200 loss_tea:0.05466298034751563  loss_houyan:1618.2296142578125\n",
      "Number:27 epoch: 981/1200 loss_tea:0.06269389681830469  loss_houyan:1640.5230712890625\n",
      "Number:27 epoch: 991/1200 loss_tea:0.050487259227811915  loss_houyan:1638.49951171875\n",
      "Number:27 epoch: 1001/1200 loss_tea:0.05749151434169104  loss_houyan:1640.5147705078125\n",
      "Number:27 epoch: 1011/1200 loss_tea:0.03737969214467534  loss_houyan:1848.2900390625\n",
      "Number:27 epoch: 1021/1200 loss_tea:0.05127845342213117  loss_houyan:1684.7974853515625\n",
      "Number:27 epoch: 1031/1200 loss_tea:0.07451936613760039  loss_houyan:1559.7752685546875\n",
      "Number:27 epoch: 1041/1200 loss_tea:0.06353139843062058  loss_houyan:2159.65576171875\n",
      "Number:27 epoch: 1051/1200 loss_tea:0.04417847525513266  loss_houyan:1774.708251953125\n",
      "Number:27 epoch: 1061/1200 loss_tea:0.055477180919450184  loss_houyan:1557.6070556640625\n",
      "Number:27 epoch: 1071/1200 loss_tea:0.050281748746072615  loss_houyan:1719.567626953125\n",
      "Number:27 epoch: 1081/1200 loss_tea:0.05921720021666726  loss_houyan:1693.2115478515625\n",
      "Number:27 epoch: 1091/1200 loss_tea:0.042782443325611194  loss_houyan:1585.1624755859375\n",
      "Number:27 epoch: 1101/1200 loss_tea:0.05862472693199857  loss_houyan:1703.82177734375\n",
      "Number:27 epoch: 1111/1200 loss_tea:0.05951169232788998  loss_houyan:1804.77978515625\n",
      "Number:27 epoch: 1121/1200 loss_tea:0.044257257140513394  loss_houyan:1531.899169921875\n",
      "Number:27 epoch: 1131/1200 loss_tea:0.053758982840622874  loss_houyan:1561.05078125\n",
      "Number:27 epoch: 1141/1200 loss_tea:0.051257741639603  loss_houyan:1706.92041015625\n",
      "Number:27 epoch: 1151/1200 loss_tea:0.048231614006961285  loss_houyan:1565.596923828125\n",
      "Number:27 epoch: 1161/1200 loss_tea:0.048153897465567176  loss_houyan:1666.8472900390625\n",
      "Number:27 epoch: 1171/1200 loss_tea:0.05091368065117365  loss_houyan:1674.4150390625\n",
      "Number:27 epoch: 1181/1200 loss_tea:0.04783561805077161  loss_houyan:1563.7142333984375\n",
      "Number:27 epoch: 1191/1200 loss_tea:0.06557157705335186  loss_houyan:1597.560791015625\n",
      "finished training number 27 techer!\n",
      "start training number 28 techer!\n",
      "Number:28 epoch: 1/1200 loss_tea:0.0552676237541605  loss_houyan:1540.810546875\n",
      "Number:28 epoch: 11/1200 loss_tea:0.06115444857119331  loss_houyan:1732.7215576171875\n",
      "Number:28 epoch: 21/1200 loss_tea:0.049227682856251676  loss_houyan:2078.547119140625\n",
      "Number:28 epoch: 31/1200 loss_tea:0.06255227426194894  loss_houyan:1596.5067138671875\n",
      "Number:28 epoch: 41/1200 loss_tea:0.044108730638035644  loss_houyan:1522.4310302734375\n",
      "Number:28 epoch: 51/1200 loss_tea:0.052246050713351797  loss_houyan:1586.658935546875\n",
      "Number:28 epoch: 61/1200 loss_tea:0.05307376812951078  loss_houyan:1529.0440673828125\n",
      "Number:28 epoch: 71/1200 loss_tea:0.052993384821663776  loss_houyan:1957.1546630859375\n",
      "Number:28 epoch: 81/1200 loss_tea:0.06655226013716933  loss_houyan:1546.8211669921875\n",
      "Number:28 epoch: 91/1200 loss_tea:0.04534083247186262  loss_houyan:1510.0594482421875\n",
      "Number:28 epoch: 101/1200 loss_tea:0.051041333310054046  loss_houyan:1551.2762451171875\n",
      "Number:28 epoch: 111/1200 loss_tea:0.06352660360245997  loss_houyan:1755.1514892578125\n",
      "Number:28 epoch: 121/1200 loss_tea:0.0758577456883822  loss_houyan:1890.687744140625\n",
      "Number:28 epoch: 131/1200 loss_tea:0.07997351572451766  loss_houyan:1545.56396484375\n",
      "Number:28 epoch: 141/1200 loss_tea:0.06152586039576015  loss_houyan:1557.5096435546875\n",
      "Number:28 epoch: 151/1200 loss_tea:0.05613013456488986  loss_houyan:1504.615234375\n",
      "Number:28 epoch: 161/1200 loss_tea:0.0578539319251903  loss_houyan:1596.2498779296875\n",
      "Number:28 epoch: 171/1200 loss_tea:0.057670432698462254  loss_houyan:1676.0877685546875\n",
      "Number:28 epoch: 181/1200 loss_tea:0.05076996991831522  loss_houyan:2098.661865234375\n",
      "Number:28 epoch: 191/1200 loss_tea:0.05595810029105617  loss_houyan:2409.2822265625\n",
      "Number:28 epoch: 201/1200 loss_tea:0.05818886417538646  loss_houyan:1789.512939453125\n",
      "Number:28 epoch: 211/1200 loss_tea:0.05744634573407289  loss_houyan:2159.605712890625\n",
      "Number:28 epoch: 221/1200 loss_tea:0.04468669234296801  loss_houyan:1607.5294189453125\n",
      "Number:28 epoch: 231/1200 loss_tea:0.048151644988444646  loss_houyan:1736.2432861328125\n",
      "Number:28 epoch: 241/1200 loss_tea:0.03800380672578124  loss_houyan:1671.6414794921875\n",
      "Number:28 epoch: 251/1200 loss_tea:0.045453629754702525  loss_houyan:1596.54541015625\n",
      "Number:28 epoch: 261/1200 loss_tea:0.06123278205974734  loss_houyan:1760.1151123046875\n",
      "Number:28 epoch: 271/1200 loss_tea:0.04863429213791983  loss_houyan:1788.8973388671875\n",
      "Number:28 epoch: 281/1200 loss_tea:0.08282088697656871  loss_houyan:1533.5758056640625\n",
      "Number:28 epoch: 291/1200 loss_tea:0.057258801047699835  loss_houyan:1574.257080078125\n",
      "Number:28 epoch: 301/1200 loss_tea:0.06628257717732744  loss_houyan:1605.4677734375\n",
      "Number:28 epoch: 311/1200 loss_tea:0.05040328930901182  loss_houyan:1529.6890869140625\n",
      "Number:28 epoch: 321/1200 loss_tea:0.05446881352225271  loss_houyan:1656.4111328125\n",
      "Number:28 epoch: 331/1200 loss_tea:0.05596718260350982  loss_houyan:1548.8389892578125\n",
      "Number:28 epoch: 341/1200 loss_tea:0.0607800928286111  loss_houyan:1610.0382080078125\n",
      "Number:28 epoch: 351/1200 loss_tea:0.06553016752849737  loss_houyan:1562.3465576171875\n",
      "Number:28 epoch: 361/1200 loss_tea:0.043749347422705843  loss_houyan:1548.80419921875\n",
      "Number:28 epoch: 371/1200 loss_tea:0.04664421642487025  loss_houyan:1553.7554931640625\n",
      "Number:28 epoch: 381/1200 loss_tea:0.05682316114976987  loss_houyan:1554.9012451171875\n",
      "Number:28 epoch: 391/1200 loss_tea:0.049203772041233586  loss_houyan:2214.992919921875\n",
      "Number:28 epoch: 401/1200 loss_tea:0.0456971455918516  loss_houyan:1547.0299072265625\n",
      "Number:28 epoch: 411/1200 loss_tea:0.05346989650743341  loss_houyan:1927.452880859375\n",
      "Number:28 epoch: 421/1200 loss_tea:0.04230518993695774  loss_houyan:1556.1885986328125\n",
      "Number:28 epoch: 431/1200 loss_tea:0.045422131914388125  loss_houyan:1636.871826171875\n",
      "Number:28 epoch: 441/1200 loss_tea:0.05297353330690995  loss_houyan:1728.3973388671875\n",
      "Number:28 epoch: 451/1200 loss_tea:0.036563743585152846  loss_houyan:1569.520751953125\n",
      "Number:28 epoch: 461/1200 loss_tea:0.07138884150990354  loss_houyan:2027.69384765625\n",
      "Number:28 epoch: 471/1200 loss_tea:0.05875676578329188  loss_houyan:1505.086669921875\n",
      "Number:28 epoch: 481/1200 loss_tea:0.07402715604229389  loss_houyan:1696.7371826171875\n",
      "Number:28 epoch: 491/1200 loss_tea:0.043448431484125055  loss_houyan:1533.592041015625\n",
      "Number:28 epoch: 501/1200 loss_tea:0.05235825781460496  loss_houyan:1812.755615234375\n",
      "Number:28 epoch: 511/1200 loss_tea:0.0806746719194154  loss_houyan:1562.3009033203125\n",
      "Number:28 epoch: 521/1200 loss_tea:0.0609573743662444  loss_houyan:1700.5750732421875\n",
      "Number:28 epoch: 531/1200 loss_tea:0.06906303783159205  loss_houyan:1481.697998046875\n",
      "Number:28 epoch: 541/1200 loss_tea:0.07243359244866411  loss_houyan:1564.0963134765625\n",
      "Number:28 epoch: 551/1200 loss_tea:0.06621377512809597  loss_houyan:2097.1435546875\n",
      "Number:28 epoch: 561/1200 loss_tea:0.0476828467720357  loss_houyan:1589.9156494140625\n",
      "Number:28 epoch: 571/1200 loss_tea:0.05360924906263546  loss_houyan:1971.447265625\n",
      "Number:28 epoch: 581/1200 loss_tea:0.05325251429479253  loss_houyan:1538.7528076171875\n",
      "Number:28 epoch: 591/1200 loss_tea:0.03944220556127704  loss_houyan:1611.4757080078125\n",
      "Number:28 epoch: 601/1200 loss_tea:0.047999413916416704  loss_houyan:1581.341064453125\n",
      "Number:28 epoch: 611/1200 loss_tea:0.055930872932785744  loss_houyan:1583.1251220703125\n",
      "Number:28 epoch: 621/1200 loss_tea:0.07347843005531877  loss_houyan:1702.62548828125\n",
      "Number:28 epoch: 631/1200 loss_tea:0.05442577510534908  loss_houyan:1631.9251708984375\n",
      "Number:28 epoch: 641/1200 loss_tea:0.06639556407004879  loss_houyan:2353.19189453125\n",
      "Number:28 epoch: 651/1200 loss_tea:0.06109297042231534  loss_houyan:1810.45947265625\n",
      "Number:28 epoch: 661/1200 loss_tea:0.040128210801325614  loss_houyan:1767.4871826171875\n",
      "Number:28 epoch: 671/1200 loss_tea:0.051603411639542  loss_houyan:1619.41748046875\n",
      "Number:28 epoch: 681/1200 loss_tea:0.046340102305326814  loss_houyan:1546.007568359375\n",
      "Number:28 epoch: 691/1200 loss_tea:0.07913182241963258  loss_houyan:1618.00634765625\n",
      "Number:28 epoch: 701/1200 loss_tea:0.048152816662040826  loss_houyan:1538.4837646484375\n",
      "Number:28 epoch: 711/1200 loss_tea:0.055140137487367104  loss_houyan:1620.6397705078125\n",
      "Number:28 epoch: 721/1200 loss_tea:0.06366603205035953  loss_houyan:1737.90478515625\n",
      "Number:28 epoch: 731/1200 loss_tea:0.041619530350953816  loss_houyan:1622.2520751953125\n",
      "Number:28 epoch: 741/1200 loss_tea:0.040882472705314074  loss_houyan:1752.7109375\n",
      "Number:28 epoch: 751/1200 loss_tea:0.0423953998981035  loss_houyan:1589.321044921875\n",
      "Number:28 epoch: 761/1200 loss_tea:0.06126850726320986  loss_houyan:1645.4923095703125\n",
      "Number:28 epoch: 771/1200 loss_tea:0.08241767567862184  loss_houyan:1555.720458984375\n",
      "Number:28 epoch: 781/1200 loss_tea:0.0517836539250215  loss_houyan:1566.7703857421875\n",
      "Number:28 epoch: 791/1200 loss_tea:0.05065209185978119  loss_houyan:1601.2178955078125\n",
      "Number:28 epoch: 801/1200 loss_tea:0.05752281536602264  loss_houyan:1659.389892578125\n",
      "Number:28 epoch: 811/1200 loss_tea:0.03969546450622318  loss_houyan:1574.1182861328125\n",
      "Number:28 epoch: 821/1200 loss_tea:0.05359491550189861  loss_houyan:1745.005126953125\n",
      "Number:28 epoch: 831/1200 loss_tea:0.0744200035901761  loss_houyan:1506.7076416015625\n",
      "Number:28 epoch: 841/1200 loss_tea:0.051212787447078086  loss_houyan:1499.91650390625\n",
      "Number:28 epoch: 851/1200 loss_tea:0.05325808880048192  loss_houyan:1577.162353515625\n",
      "Number:28 epoch: 861/1200 loss_tea:0.06545513879159925  loss_houyan:1715.5841064453125\n",
      "Number:28 epoch: 871/1200 loss_tea:0.06416820877578569  loss_houyan:1964.0107421875\n",
      "Number:28 epoch: 881/1200 loss_tea:0.04970592496879301  loss_houyan:1631.2596435546875\n",
      "Number:28 epoch: 891/1200 loss_tea:0.0576540084051318  loss_houyan:1662.656005859375\n",
      "Number:28 epoch: 901/1200 loss_tea:0.04950037222383523  loss_houyan:1754.474853515625\n",
      "Number:28 epoch: 911/1200 loss_tea:0.04347363285869752  loss_houyan:2126.34521484375\n",
      "Number:28 epoch: 921/1200 loss_tea:0.04563724757046721  loss_houyan:1714.984619140625\n",
      "Number:28 epoch: 931/1200 loss_tea:0.045609571980619955  loss_houyan:1777.934814453125\n",
      "Number:28 epoch: 941/1200 loss_tea:0.07544256134061696  loss_houyan:1642.580810546875\n",
      "Number:28 epoch: 951/1200 loss_tea:0.061938035117561505  loss_houyan:1577.4921875\n",
      "Number:28 epoch: 961/1200 loss_tea:0.055525102391341015  loss_houyan:1549.9720458984375\n",
      "Number:28 epoch: 971/1200 loss_tea:0.05542898714860815  loss_houyan:1544.4742431640625\n",
      "Number:28 epoch: 981/1200 loss_tea:0.07817900227146977  loss_houyan:1524.556640625\n",
      "Number:28 epoch: 991/1200 loss_tea:0.06278026280843563  loss_houyan:1821.1484375\n",
      "Number:28 epoch: 1001/1200 loss_tea:0.053636743593715765  loss_houyan:2169.1611328125\n",
      "Number:28 epoch: 1011/1200 loss_tea:0.056554120729688885  loss_houyan:1552.453125\n",
      "Number:28 epoch: 1021/1200 loss_tea:0.04809684880079348  loss_houyan:1528.72314453125\n",
      "Number:28 epoch: 1031/1200 loss_tea:0.05716611523877664  loss_houyan:1872.9404296875\n",
      "Number:28 epoch: 1041/1200 loss_tea:0.07712928163095575  loss_houyan:1607.00830078125\n",
      "Number:28 epoch: 1051/1200 loss_tea:0.03694488043793667  loss_houyan:1673.624755859375\n",
      "Number:28 epoch: 1061/1200 loss_tea:0.06273988602052509  loss_houyan:1567.6827392578125\n",
      "Number:28 epoch: 1071/1200 loss_tea:0.05405542688172236  loss_houyan:1793.6741943359375\n",
      "Number:28 epoch: 1081/1200 loss_tea:0.08301243407537465  loss_houyan:1805.0418701171875\n",
      "Number:28 epoch: 1091/1200 loss_tea:0.06121576208706529  loss_houyan:1547.1109619140625\n",
      "Number:28 epoch: 1101/1200 loss_tea:0.05154543506320742  loss_houyan:1719.9547119140625\n",
      "Number:28 epoch: 1111/1200 loss_tea:0.04596139100418536  loss_houyan:2116.59765625\n",
      "Number:28 epoch: 1121/1200 loss_tea:0.05224215802720546  loss_houyan:1640.2886962890625\n",
      "Number:28 epoch: 1131/1200 loss_tea:0.0449967226554942  loss_houyan:1669.8770751953125\n",
      "Number:28 epoch: 1141/1200 loss_tea:0.04378428520679667  loss_houyan:1628.403076171875\n",
      "Number:28 epoch: 1151/1200 loss_tea:0.057137349961902395  loss_houyan:1926.200439453125\n",
      "Number:28 epoch: 1161/1200 loss_tea:0.054558771012921746  loss_houyan:1603.6141357421875\n",
      "Number:28 epoch: 1171/1200 loss_tea:0.051385300556536936  loss_houyan:1556.442626953125\n",
      "Number:28 epoch: 1181/1200 loss_tea:0.05119897838223915  loss_houyan:1554.4776611328125\n",
      "Number:28 epoch: 1191/1200 loss_tea:0.058399904901884285  loss_houyan:1597.58447265625\n",
      "finished training number 28 techer!\n",
      "start training number 29 techer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangyipeng/anaconda3/envs/ysy/lib/python3.9/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([47])) that is different to the input size (torch.Size([47, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number:29 epoch: 1/1200 loss_tea:0.06403865759107154  loss_houyan:1836.3433837890625\n",
      "Number:29 epoch: 11/1200 loss_tea:0.05566530260408187  loss_houyan:1567.64404296875\n",
      "Number:29 epoch: 21/1200 loss_tea:0.04472710460733916  loss_houyan:1577.908203125\n",
      "Number:29 epoch: 31/1200 loss_tea:0.04285888714824692  loss_houyan:1731.61669921875\n",
      "Number:29 epoch: 41/1200 loss_tea:0.048589587571925094  loss_houyan:1626.5797119140625\n",
      "Number:29 epoch: 51/1200 loss_tea:0.05322413196864232  loss_houyan:1655.7801513671875\n",
      "Number:29 epoch: 61/1200 loss_tea:0.05461052886578853  loss_houyan:1536.333251953125\n",
      "Number:29 epoch: 71/1200 loss_tea:0.047637589071025624  loss_houyan:1634.7811279296875\n",
      "Number:29 epoch: 81/1200 loss_tea:0.048521042773562666  loss_houyan:1520.2374267578125\n",
      "Number:29 epoch: 91/1200 loss_tea:0.05753397679582581  loss_houyan:1631.0162353515625\n",
      "Number:29 epoch: 101/1200 loss_tea:0.051994918038295986  loss_houyan:1670.576904296875\n",
      "Number:29 epoch: 111/1200 loss_tea:0.04577639999903164  loss_houyan:1538.1875\n",
      "Number:29 epoch: 121/1200 loss_tea:0.04599081086022527  loss_houyan:1689.6502685546875\n",
      "Number:29 epoch: 131/1200 loss_tea:0.07092581660532461  loss_houyan:1878.175537109375\n",
      "Number:29 epoch: 141/1200 loss_tea:0.03876181849475925  loss_houyan:1552.9608154296875\n",
      "Number:29 epoch: 151/1200 loss_tea:0.05725066606474976  loss_houyan:1584.034423828125\n",
      "Number:29 epoch: 161/1200 loss_tea:0.0642675544783652  loss_houyan:1550.209228515625\n",
      "Number:29 epoch: 171/1200 loss_tea:0.04943804753892316  loss_houyan:1650.226806640625\n",
      "Number:29 epoch: 181/1200 loss_tea:0.05201948508377523  loss_houyan:1595.6214599609375\n",
      "Number:29 epoch: 191/1200 loss_tea:0.04988901493105619  loss_houyan:1564.70947265625\n",
      "Number:29 epoch: 201/1200 loss_tea:0.06533476159908067  loss_houyan:2448.42138671875\n",
      "Number:29 epoch: 211/1200 loss_tea:0.055930425338045056  loss_houyan:1562.37451171875\n",
      "Number:29 epoch: 221/1200 loss_tea:0.051936384865445585  loss_houyan:1546.23095703125\n",
      "Number:29 epoch: 231/1200 loss_tea:0.06262385400212775  loss_houyan:1550.345458984375\n",
      "Number:29 epoch: 241/1200 loss_tea:0.057914160569017326  loss_houyan:1656.011962890625\n",
      "Number:29 epoch: 251/1200 loss_tea:0.06075527532641743  loss_houyan:1674.822998046875\n",
      "Number:29 epoch: 261/1200 loss_tea:0.045363362681335305  loss_houyan:1676.107421875\n",
      "Number:29 epoch: 271/1200 loss_tea:0.06164414772843118  loss_houyan:1567.841796875\n",
      "Number:29 epoch: 281/1200 loss_tea:0.0477854055778251  loss_houyan:1600.954833984375\n",
      "Number:29 epoch: 291/1200 loss_tea:0.042566219134234626  loss_houyan:1550.1378173828125\n",
      "Number:29 epoch: 301/1200 loss_tea:0.05430977699837818  loss_houyan:1531.9969482421875\n",
      "Number:29 epoch: 311/1200 loss_tea:0.057843048736146596  loss_houyan:1549.6500244140625\n",
      "Number:29 epoch: 321/1200 loss_tea:0.06066707406527543  loss_houyan:1644.9774169921875\n",
      "Number:29 epoch: 331/1200 loss_tea:0.04223899858543087  loss_houyan:1546.484375\n",
      "Number:29 epoch: 341/1200 loss_tea:0.05343771434795454  loss_houyan:1558.25927734375\n",
      "Number:29 epoch: 351/1200 loss_tea:0.054377551273048545  loss_houyan:1594.0140380859375\n",
      "Number:29 epoch: 361/1200 loss_tea:0.04902318947592182  loss_houyan:1556.2557373046875\n",
      "Number:29 epoch: 371/1200 loss_tea:0.04979942388175656  loss_houyan:1677.3837890625\n",
      "Number:29 epoch: 381/1200 loss_tea:0.0499020820933771  loss_houyan:2111.047119140625\n",
      "Number:29 epoch: 391/1200 loss_tea:0.06758801073952524  loss_houyan:1663.2271728515625\n",
      "Number:29 epoch: 401/1200 loss_tea:0.050562652349441704  loss_houyan:2093.2802734375\n",
      "Number:29 epoch: 411/1200 loss_tea:0.04947683694930772  loss_houyan:1741.457275390625\n",
      "Number:29 epoch: 421/1200 loss_tea:0.05365753506608588  loss_houyan:1696.9117431640625\n",
      "Number:29 epoch: 431/1200 loss_tea:0.06002028297076908  loss_houyan:1613.1527099609375\n",
      "Number:29 epoch: 441/1200 loss_tea:0.06978870049571098  loss_houyan:1575.6461181640625\n",
      "Number:29 epoch: 451/1200 loss_tea:0.06817885067118959  loss_houyan:1779.53466796875\n",
      "Number:29 epoch: 461/1200 loss_tea:0.07160548840459217  loss_houyan:1723.7421875\n",
      "Number:29 epoch: 471/1200 loss_tea:0.06411119364922654  loss_houyan:1560.79052734375\n",
      "Number:29 epoch: 481/1200 loss_tea:0.04349487568485994  loss_houyan:1735.939697265625\n",
      "Number:29 epoch: 491/1200 loss_tea:0.058725175046673184  loss_houyan:1906.905029296875\n",
      "Number:29 epoch: 501/1200 loss_tea:0.05297053171269788  loss_houyan:1682.2635498046875\n",
      "Number:29 epoch: 511/1200 loss_tea:0.04588476799214549  loss_houyan:1572.15185546875\n",
      "Number:29 epoch: 521/1200 loss_tea:0.045257570583474196  loss_houyan:1560.4810791015625\n",
      "Number:29 epoch: 531/1200 loss_tea:0.05568562280155325  loss_houyan:1609.7674560546875\n",
      "Number:29 epoch: 541/1200 loss_tea:0.051400956812115  loss_houyan:1609.5958251953125\n",
      "Number:29 epoch: 551/1200 loss_tea:0.04903031515977432  loss_houyan:1718.94921875\n",
      "Number:29 epoch: 561/1200 loss_tea:0.06784722984073799  loss_houyan:1550.903564453125\n",
      "Number:29 epoch: 571/1200 loss_tea:0.057630785185835365  loss_houyan:1643.7799072265625\n",
      "Number:29 epoch: 581/1200 loss_tea:0.05992347612063721  loss_houyan:1535.7266845703125\n",
      "Number:29 epoch: 591/1200 loss_tea:0.05620110051403388  loss_houyan:1627.8023681640625\n",
      "Number:29 epoch: 601/1200 loss_tea:0.04598911738391443  loss_houyan:1643.4642333984375\n",
      "Number:29 epoch: 611/1200 loss_tea:0.04742459272705982  loss_houyan:1735.27685546875\n",
      "Number:29 epoch: 621/1200 loss_tea:0.0683289432044404  loss_houyan:1688.669189453125\n",
      "Number:29 epoch: 631/1200 loss_tea:0.055176341452950056  loss_houyan:1959.4603271484375\n",
      "Number:29 epoch: 641/1200 loss_tea:0.06321838427038427  loss_houyan:1653.5235595703125\n",
      "Number:29 epoch: 651/1200 loss_tea:0.050368203336839105  loss_houyan:1533.6151123046875\n",
      "Number:29 epoch: 661/1200 loss_tea:0.044090139498125146  loss_houyan:1550.4381103515625\n",
      "Number:29 epoch: 671/1200 loss_tea:0.038688581072763475  loss_houyan:1543.4342041015625\n",
      "Number:29 epoch: 681/1200 loss_tea:0.06529004636919078  loss_houyan:1520.330322265625\n",
      "Number:29 epoch: 691/1200 loss_tea:0.06503418448540993  loss_houyan:1625.010009765625\n",
      "Number:29 epoch: 701/1200 loss_tea:0.09681994208242616  loss_houyan:1575.344970703125\n",
      "Number:29 epoch: 711/1200 loss_tea:0.059911650014391526  loss_houyan:1665.133544921875\n",
      "Number:29 epoch: 721/1200 loss_tea:0.06999362304630723  loss_houyan:1595.2694091796875\n",
      "Number:29 epoch: 731/1200 loss_tea:0.05643266011458069  loss_houyan:1535.7918701171875\n",
      "Number:29 epoch: 741/1200 loss_tea:0.049193448356554335  loss_houyan:1567.1021728515625\n",
      "Number:29 epoch: 751/1200 loss_tea:0.05022223521349411  loss_houyan:1533.443603515625\n",
      "Number:29 epoch: 761/1200 loss_tea:0.04629653776597404  loss_houyan:1545.167236328125\n",
      "Number:29 epoch: 771/1200 loss_tea:0.05200746083054468  loss_houyan:1695.9420166015625\n",
      "Number:29 epoch: 781/1200 loss_tea:0.03895128409565406  loss_houyan:1552.3436279296875\n",
      "Number:29 epoch: 791/1200 loss_tea:0.05139005880380352  loss_houyan:1574.864990234375\n",
      "Number:29 epoch: 801/1200 loss_tea:0.06730607198977988  loss_houyan:1595.60888671875\n",
      "Number:29 epoch: 811/1200 loss_tea:0.04728995464746735  loss_houyan:1663.4833984375\n",
      "Number:29 epoch: 821/1200 loss_tea:0.041829051644465653  loss_houyan:1569.6666259765625\n",
      "Number:29 epoch: 831/1200 loss_tea:0.08184617430402558  loss_houyan:1573.307373046875\n",
      "Number:29 epoch: 841/1200 loss_tea:0.049390513944488706  loss_houyan:1628.476318359375\n",
      "Number:29 epoch: 851/1200 loss_tea:0.05051045917767999  loss_houyan:1564.3828125\n",
      "Number:29 epoch: 861/1200 loss_tea:0.06135792018797509  loss_houyan:1717.4287109375\n",
      "Number:29 epoch: 871/1200 loss_tea:0.043086490086942604  loss_houyan:1714.4351806640625\n",
      "Number:29 epoch: 881/1200 loss_tea:0.06765681620618218  loss_houyan:1555.5782470703125\n",
      "Number:29 epoch: 891/1200 loss_tea:0.0478705823970676  loss_houyan:1694.771240234375\n",
      "Number:29 epoch: 901/1200 loss_tea:0.053482971284436104  loss_houyan:1616.690673828125\n",
      "Number:29 epoch: 911/1200 loss_tea:0.07195832356476803  loss_houyan:1534.729248046875\n",
      "Number:29 epoch: 921/1200 loss_tea:0.051462251156708105  loss_houyan:1552.720703125\n",
      "Number:29 epoch: 931/1200 loss_tea:0.04841561642244255  loss_houyan:1590.1800537109375\n",
      "Number:29 epoch: 941/1200 loss_tea:0.04574294267000406  loss_houyan:1648.7977294921875\n",
      "Number:29 epoch: 951/1200 loss_tea:0.04503022139538114  loss_houyan:1600.301025390625\n",
      "Number:29 epoch: 961/1200 loss_tea:0.04369518062813999  loss_houyan:1548.118896484375\n",
      "Number:29 epoch: 971/1200 loss_tea:0.059144582600160034  loss_houyan:1585.5421142578125\n",
      "Number:29 epoch: 981/1200 loss_tea:0.05567516073678618  loss_houyan:1559.036865234375\n",
      "Number:29 epoch: 991/1200 loss_tea:0.056236360802399944  loss_houyan:1627.9775390625\n",
      "Number:29 epoch: 1001/1200 loss_tea:0.054219306937590125  loss_houyan:1541.529296875\n",
      "Number:29 epoch: 1011/1200 loss_tea:0.05553056960089147  loss_houyan:1683.2183837890625\n",
      "Number:29 epoch: 1021/1200 loss_tea:0.04765001267219022  loss_houyan:1796.6014404296875\n",
      "Number:29 epoch: 1031/1200 loss_tea:0.05700114546971393  loss_houyan:1549.453369140625\n",
      "Number:29 epoch: 1041/1200 loss_tea:0.05915288496637352  loss_houyan:1663.2371826171875\n",
      "Number:29 epoch: 1051/1200 loss_tea:0.06166534970841226  loss_houyan:1651.539306640625\n",
      "Number:29 epoch: 1061/1200 loss_tea:0.07696103228161666  loss_houyan:1664.100341796875\n",
      "Number:29 epoch: 1071/1200 loss_tea:0.0463731078970726  loss_houyan:1550.7489013671875\n",
      "Number:29 epoch: 1081/1200 loss_tea:0.05487413602558556  loss_houyan:1543.5179443359375\n",
      "Number:29 epoch: 1091/1200 loss_tea:0.048166668037609904  loss_houyan:1568.9512939453125\n",
      "Number:29 epoch: 1101/1200 loss_tea:0.05974403862042853  loss_houyan:2082.082763671875\n",
      "Number:29 epoch: 1111/1200 loss_tea:0.06324709813212746  loss_houyan:1539.6324462890625\n",
      "Number:29 epoch: 1121/1200 loss_tea:0.04914721209806948  loss_houyan:1611.6212158203125\n",
      "Number:29 epoch: 1131/1200 loss_tea:0.06166003519834921  loss_houyan:1576.3115234375\n",
      "Number:29 epoch: 1141/1200 loss_tea:0.050372234760889624  loss_houyan:2123.48486328125\n",
      "Number:29 epoch: 1151/1200 loss_tea:0.06167226952427251  loss_houyan:1619.648681640625\n",
      "Number:29 epoch: 1161/1200 loss_tea:0.06358668211156496  loss_houyan:1626.412109375\n",
      "Number:29 epoch: 1171/1200 loss_tea:0.05758282940472927  loss_houyan:1552.81787109375\n",
      "Number:29 epoch: 1181/1200 loss_tea:0.050290110818563834  loss_houyan:1792.07958984375\n",
      "Number:29 epoch: 1191/1200 loss_tea:0.05872899573808337  loss_houyan:1565.8916015625\n",
      "finished training number 29 techer!\n"
     ]
    }
   ],
   "source": [
    "#training teacher models\n",
    "loss_func=nn.MSELoss()\n",
    "train_loss_all=[]\n",
    "# loss_all=[[0]*num_epochs]*n_teachers\n",
    "for tea_num in range(n_teachers):\n",
    "    print(f'start training number {tea_num} techer!')\n",
    "    minloss =float ('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        train_num=0\n",
    "        teachers_model[tea_num].train()\n",
    "        for b_x,b_y in teacher_data_loader[tea_num]:\n",
    "            teacher_optimizers[tea_num].zero_grad()\n",
    "            output=teachers_model[tea_num].forward(b_x.to(device))\n",
    "            loss=loss_func(output,b_y.to(device))\n",
    "            loss.backward()\n",
    "            teacher_optimizers[tea_num].step()\n",
    "            \n",
    "            teacher_optimizers[tea_num].zero_grad()\n",
    "            loss_houyan = teachers_model[tea_num].sample_elbo(b_x.to(device),b_y.to(device),1)\n",
    "            loss_houyan.backward()\n",
    "            teacher_optimizers[tea_num].step()\n",
    "\n",
    "            train_loss+=loss.item() * b_x.size(0)\n",
    "            train_num += b_x.size(0)\n",
    "            loss_tea = train_loss/train_num\n",
    "            \n",
    "            if loss_tea< minloss:\n",
    "                minloss = loss_tea\n",
    "            # if loss_houyan< minloss:\n",
    "            #     minloss = loss_houyan\n",
    "                if os.path.exists(f'/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_44_gauss/teacher{tea_num}/best.pth'):\n",
    "                    os.remove(f'/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_44_gauss/teacher{tea_num}/best.pth')\n",
    "                torch.save(teachers_model[tea_num], f'/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_44_gauss/teacher{tea_num}/best.pth')\n",
    "            \n",
    "        \n",
    "        # loss_all[tea_num][epoch]+=loss\n",
    "        if epoch%10== 0:\n",
    "            print(f'Number:{tea_num}','epoch: {}/{}'.format(epoch+1,num_epochs),f'loss_tea:{loss_tea}', f' loss_houyan:{loss_houyan}')\n",
    "        train_loss_all.append(loss_tea)\n",
    "    print(f'finished training number {tea_num} techer!')\n",
    "    # 20*1500 1400mins\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_loss_all,\"ro-\",label=\"Train loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()      \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower:[-0.32501178],upper:[0.39964756]\n"
     ]
    }
   ],
   "source": [
    "#confidence interval \n",
    "#获取样本和标签之间的置信区间关系 找到上下界  判断样本是否在区间内 在阈值内 即打上标签\n",
    "# samples = 13542\n",
    "y_samp = np.zeros((samples,1))\n",
    "for s in range(samples):\n",
    "    for b_x,b_y  in student_train_loader:\n",
    "        y = net (b_x[s]).cpu().detach().numpy()\n",
    "        y_samp[s] = y.reshape(-1)\n",
    "# print(y_samp)\n",
    "lower = np.percentile(y_samp, 2.5, axis = 0)\n",
    "upper = np.percentile(y_samp, 97.5, axis = 0)\n",
    "print(f'lower:{lower},upper:{upper}')\n",
    "#97.5-2.5////99-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.32501178] [0.39964756]\n",
      "8186 0.004983590616263522 41\n"
     ]
    }
   ],
   "source": [
    "print(lower,upper)\n",
    "count = 0 \n",
    "for i in range(samples):\n",
    "    for b_x,b_y in student_train_loader:\n",
    "    #     print(max(b_y),min(b_y)) tensor(5.0000) tensor(0.1500)\n",
    "    #     print(b_y)\n",
    "        if lower<=b_y[i].item()<=upper:\n",
    "            count+=1\n",
    "per = 1-count/samples\n",
    "print (count,per,samples-count)\n",
    "#其实只丢了134条数据\n",
    "# [-0.54881748] [0.61468787]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3250], device='cuda:2', dtype=torch.float64)\n",
      "tensor([0.3996], device='cuda:2', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# for b_x,b_y in student_train_loader:\n",
    "#     print(b_y[0].item())\n",
    "#     b_y[0]=1\n",
    "#     print(b_y[0])\n",
    "# lower:[-0.54156185],upper:[0.62291579]\n",
    "# lower=-0.54156185\n",
    "# upper=0.62291579\n",
    "upper=torch.tensor(upper).to(device)\n",
    "lower= torch.tensor(lower).to(device)\n",
    "print(lower)\n",
    "print(upper)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1897, 0.9316, 1.2974,  ..., 1.1745, 0.9431, 1.0438],\n",
      "        [1.1355, 0.8397, 1.1340,  ..., 1.0000, 1.0000, 0.7667],\n",
      "        [1.2027, 1.0000, 1.1505,  ..., 0.9418, 1.2588, 0.8202],\n",
      "        ...,\n",
      "        [1.1390, 1.0000, 1.2686,  ..., 1.3390, 1.0403, 0.7194],\n",
      "        [1.0454, 0.7234, 1.3664,  ..., 1.1757, 0.8762, 1.0133],\n",
      "        [1.1723, 0.7054, 1.2255,  ..., 1.2990, 1.1482, 0.9328]],\n",
      "       device='cuda:2') 0.08248855394838135\n",
      "tensor([[0.0411],\n",
      "        [0.0322],\n",
      "        [0.0303],\n",
      "        ...,\n",
      "        [0.0258],\n",
      "        [0.0258],\n",
      "        [0.0258]], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "#聚合标签\n",
    "from http.cookiejar import LoadError\n",
    "import random\n",
    "import numpy as np\n",
    "mu = 0\n",
    "sigma = 0.05\n",
    "# sigma =0.1\n",
    "# sigma = 0.15\n",
    "# sigma =0.2\n",
    "# sigma =0.3\n",
    "# sigma =0.5\n",
    "# sigma =1\n",
    "#teahcer labeling\n",
    "teacher_best_models =[torch.load(f'/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_44_gauss/teacher{tea_num}/best.pth')  for  tea_num in  range(n_teachers)]\n",
    "\n",
    "pos= torch.ones(samples,n_teachers).to(device)\n",
    "#用0标记无效值 数据集已ln处理 无ln0\n",
    "# 记录每个老师 针对拿到的数据 预测出来的如果符合置信区间内，那就记录在内，如果不在 则剔除该值\n",
    "ccc = 0\n",
    "sss=0\n",
    "for tea in range(n_teachers):\n",
    "    for x,y in student_train_loader:\n",
    "            teacher_best_models[tea].eval().to(device)\n",
    "            pred=teacher_best_models[tea].forward(x)\n",
    "            \n",
    "            # pred+=random.gauss(mu,sigma)\n",
    "            for i in range(samples):\n",
    "                # if lower <= pred[i].item() <= upper :\n",
    "\n",
    "                if lower.item()<= pred[i].item() <= upper.item():\n",
    "                    #如果在内 记录\n",
    "                    pos[i][tea]+=pred[i].item()\n",
    "                    sss+=1\n",
    "                else:\n",
    "                    ccc+=1\n",
    "print(pos,ccc/(sss+ccc))            \n",
    "\n",
    "#针对pos中有效值（非0数）进行处理  →  每个老师打出标签做均值求和 \n",
    "res= torch.zeros(samples,1).to(device)\n",
    "count_data=0\n",
    "count_sum=0\n",
    " \n",
    "for i in range(samples):\n",
    "    for j in range(n_teachers):\n",
    "        if pos[i][j]!=1:\n",
    "            count_data+=1\n",
    "            count_sum+=pos[i][j]\n",
    "            # 有效\n",
    "    # if count_data!=0:\n",
    "    #     cur=count_sum/count_data #data1 +....datan//K\n",
    "    # else:\n",
    "    #     cur = count_sum/1\n",
    "    #     zero+=1#表明n个老师打出的值均无效//避免除以0//零值  但标准化后出现0值 需注意\n",
    "    if count_data==0:#表明n个老师打出的值均无效//避免除以0//零值\n",
    "        print('False')\n",
    "    cur=count_sum/count_data #data1 +....datan//K\n",
    "    res[i]+=cur#有效的均值标签\n",
    "    res[i]-=1\n",
    "print(res)\n",
    "\n",
    "                 \n",
    "#     fz1=(fz1+fz)\n",
    "# # fz1 = fz1/n_teachers\n",
    "# print(fz1)\n",
    "\n",
    "# for i in range(samples):\n",
    "#     for x,y in student_train_loader:\n",
    "#         fz1[i]= torch.where(fz1[i]>upper,y[i],fz1[i])\n",
    "#         fz1[i]= torch.where(fz1[i]<lower,y[i],fz1[i])\n",
    "          \n",
    "# print(fz1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregate finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_215213/2792088638.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  student_trainset =TensorDataset(torch.tensor(student_x[0],device=device,dtype=torch.float),torch.tensor(res,device=device,dtype=torch.float))\n"
     ]
    }
   ],
   "source": [
    "#student_label\n",
    "\n",
    "# torch.as_tensor(torch.from_numpy(student_x[0]), dtype=torch.float)\n",
    "\n",
    "student_trainset =TensorDataset(torch.tensor(student_x[0],device=device,dtype=torch.float),torch.tensor(res,device=device,dtype=torch.float))\n",
    "student_train_loader1 = DataLoader(student_trainset,batch_size=batch_size)\n",
    "print('aggregate finished')\n",
    "\n",
    "# # res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#creating student  folders\n",
    "def  mkdir_if_missing(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "mkdir_if_missing(f'/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_44_gauss/stu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden1): Sequential(\n",
      "    (0): Linear(in_features=81, out_features=810, bias=True)\n",
      "    (1): Dropout(p=0.7, inplace=False)\n",
      "    (2): Tanh()\n",
      "  )\n",
      "  (hidden2): Sequential(\n",
      "    (0): Linear(in_features=810, out_features=162, bias=True)\n",
      "    (1): Dropout(p=0.7, inplace=False)\n",
      "    (2): Tanh()\n",
      "  )\n",
      "  (hidden3): Sequential(\n",
      "    (0): Linear(in_features=162, out_features=54, bias=True)\n",
      "    (1): Dropout(p=0.6, inplace=False)\n",
      "    (2): Tanh()\n",
      "  )\n",
      "  (predict): Sequential(\n",
      "    (0): Linear(in_features=54, out_features=1, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# # # neural network model mlp\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.hidden1 = nn.Sequential(nn.Linear(in_features=44, out_features=880, bias=True),\n",
    "#                                   nn.Dropout(0.7),\n",
    "#                                   nn.Tanh()\n",
    "#                                   )\n",
    "        \n",
    "#         self.hidden2 = nn.Sequential(nn.Linear(in_features=880, out_features=220, bias=True),\n",
    "#                                   nn.Dropout(0.7),\n",
    "#                                   nn.Tanh()\n",
    "#                                   )\n",
    "        \n",
    "#         self.hidden3 = nn.Sequential(nn.Linear(in_features=220, out_features=44, bias=True),\n",
    "#                                   nn.Dropout(0.5),\n",
    "#                                   nn.Tanh()\n",
    "#                                   )\n",
    "        \n",
    "#         self.predict = nn.Sequential(nn.Linear(in_features=44, out_features=1, bias=True),\n",
    "#                                         nn.Tanh())\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.hidden1(x)\n",
    "#         x = self.hidden2(x)\n",
    "#         x = self.hidden3(x)\n",
    "#         output = self.predict(x)\n",
    "#         return output[:, 0]\n",
    "\n",
    "\n",
    "# mlpreg=MLP().to(device)\n",
    "# print(mlpreg)\n",
    "# #initialzing student model\n",
    "# student_model=mlpreg\n",
    "# student_optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-5) \n",
    "\n",
    "\n",
    "# neural network model mlp\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden1 = nn.Sequential(nn.Linear(in_features=81, out_features=810, bias=True),\n",
    "                                  nn.Dropout(0.7),\n",
    "                                  nn.Tanh()\n",
    "                                  )\n",
    "        \n",
    "        self.hidden2 = nn.Sequential(nn.Linear(in_features=810, out_features=162, bias=True),\n",
    "                                  nn.Dropout(0.7),\n",
    "                                  nn.Tanh()\n",
    "                                  )\n",
    "        \n",
    "        self.hidden3 = nn.Sequential(nn.Linear(in_features=162, out_features=54, bias=True),\n",
    "                                  nn.Dropout(0.6),\n",
    "                                  nn.Tanh()\n",
    "                                  )\n",
    "        \n",
    "        self.predict = nn.Sequential(nn.Linear(in_features=54, out_features=1, bias=True),\n",
    "                                        nn.Tanh())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.hidden3(x)\n",
    "        output = self.predict(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "mlpreg=MLP().to(device)\n",
    "print(mlpreg)\n",
    "#initialzing student model\n",
    "student_model=mlpreg\n",
    "student_optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "student train: Epo:0  Loss_stu:0.011383323930203915\n",
      "student train: Epo:1  Loss_stu:0.012851214036345482\n",
      "student train: Epo:2  Loss_stu:0.007528847549110651\n",
      "student train: Epo:3  Loss_stu:0.007017587777227163\n",
      "student train: Epo:4  Loss_stu:0.00289433472789824\n",
      "student train: Epo:5  Loss_stu:0.004872066434472799\n",
      "student train: Epo:6  Loss_stu:0.0062877340242266655\n",
      "student train: Epo:7  Loss_stu:0.0031189662404358387\n",
      "student train: Epo:8  Loss_stu:0.0019492170540615916\n",
      "student train: Epo:9  Loss_stu:0.0028707075398415327\n",
      "student train: Epo:10  Loss_stu:0.0030575059354305267\n",
      "student train: Epo:11  Loss_stu:0.0016751771327108145\n",
      "student train: Epo:12  Loss_stu:0.002438274910673499\n",
      "student train: Epo:13  Loss_stu:0.0013168059522286057\n",
      "student train: Epo:14  Loss_stu:0.0014418085338547826\n",
      "student train: Epo:15  Loss_stu:0.0017759638139978051\n",
      "student train: Epo:16  Loss_stu:0.0014437134377658367\n",
      "student train: Epo:17  Loss_stu:0.0012162983184680343\n",
      "student train: Epo:18  Loss_stu:0.0010718825506046414\n",
      "student train: Epo:19  Loss_stu:0.0010617139050737023\n",
      "student train: Epo:20  Loss_stu:0.0006542857736349106\n",
      "student train: Epo:21  Loss_stu:0.000729747349396348\n",
      "student train: Epo:22  Loss_stu:0.0005775080062448978\n",
      "student train: Epo:23  Loss_stu:0.00046295582433231175\n",
      "student train: Epo:24  Loss_stu:0.00043530180118978024\n",
      "student train: Epo:25  Loss_stu:0.0004121338133700192\n",
      "student train: Epo:26  Loss_stu:0.00014102525892667472\n",
      "student train: Epo:27  Loss_stu:0.00022313426597975194\n",
      "student train: Epo:28  Loss_stu:0.0002159872674383223\n",
      "student train: Epo:29  Loss_stu:0.00018981078756041825\n",
      "student train: Epo:30  Loss_stu:0.00010324923641746864\n",
      "student train: Epo:31  Loss_stu:8.217905269702896e-05\n",
      "student train: Epo:32  Loss_stu:8.823082316666842e-05\n",
      "student train: Epo:33  Loss_stu:0.00012119191524107009\n",
      "student train: Epo:34  Loss_stu:8.342832734342664e-05\n",
      "student train: Epo:35  Loss_stu:6.219562783371657e-05\n",
      "student train: Epo:36  Loss_stu:5.827881977893412e-05\n",
      "student train: Epo:37  Loss_stu:6.358081009238958e-05\n",
      "student train: Epo:38  Loss_stu:3.457898128544912e-05\n",
      "student train: Epo:39  Loss_stu:3.210270733688958e-05\n",
      "student train: Epo:40  Loss_stu:2.4866814783308655e-05\n",
      "student train: Epo:41  Loss_stu:3.420048233238049e-05\n",
      "student train: Epo:42  Loss_stu:1.6999363651848398e-05\n",
      "student train: Epo:43  Loss_stu:1.460719886381412e-05\n",
      "student train: Epo:44  Loss_stu:1.1884982086485252e-05\n",
      "student train: Epo:45  Loss_stu:9.86115264822729e-06\n",
      "student train: Epo:46  Loss_stu:9.772760677151382e-06\n",
      "student train: Epo:47  Loss_stu:5.300597422319697e-06\n",
      "student train: Epo:48  Loss_stu:4.030587206216296e-06\n",
      "student train: Epo:49  Loss_stu:3.851536803267663e-06\n",
      "student train: Epo:50  Loss_stu:2.536030933697475e-06\n",
      "student train: Epo:51  Loss_stu:2.0570103060890688e-06\n",
      "student train: Epo:52  Loss_stu:2.6790937681653304e-06\n",
      "student train: Epo:53  Loss_stu:1.4545040585289826e-06\n",
      "student train: Epo:54  Loss_stu:9.749313676366e-07\n",
      "student train: Epo:55  Loss_stu:6.344735083985142e-07\n",
      "student train: Epo:56  Loss_stu:5.693348725799297e-07\n",
      "student train: Epo:57  Loss_stu:4.574692979986139e-07\n",
      "student train: Epo:58  Loss_stu:2.0188350902117236e-07\n",
      "student train: Epo:59  Loss_stu:2.0887543428216304e-07\n",
      "student train: Epo:60  Loss_stu:1.1906593755384165e-07\n",
      "student train: Epo:61  Loss_stu:7.538540813811778e-08\n",
      "student train: Epo:62  Loss_stu:1.0510012771192123e-07\n",
      "student train: Epo:63  Loss_stu:3.598982445396359e-08\n",
      "student train: Epo:64  Loss_stu:4.276677501025006e-08\n",
      "student train: Epo:65  Loss_stu:4.915565199326011e-08\n",
      "student train: Epo:66  Loss_stu:2.930578091309144e-08\n",
      "student train: Epo:67  Loss_stu:2.8008575014837334e-08\n",
      "student train: Epo:68  Loss_stu:1.2564012941140845e-08\n",
      "student train: Epo:69  Loss_stu:1.0228867530770458e-08\n",
      "student train: Epo:70  Loss_stu:2.1375603509454777e-08\n",
      "student train: Epo:71  Loss_stu:1.816754569006207e-08\n",
      "student train: Epo:72  Loss_stu:1.2895967849146928e-08\n",
      "student train: Epo:73  Loss_stu:1.3203212745338533e-08\n",
      "student train: Epo:74  Loss_stu:1.646555780610015e-08\n",
      "student train: Epo:75  Loss_stu:1.6050769602315995e-08\n",
      "student train: Epo:76  Loss_stu:1.5085216631405274e-08\n",
      "student train: Epo:77  Loss_stu:1.5763154337378182e-08\n",
      "student train: Epo:78  Loss_stu:1.055733012123028e-08\n",
      "student train: Epo:79  Loss_stu:1.4687472571495164e-08\n",
      "student train: Epo:80  Loss_stu:3.4032581197607215e-08\n",
      "student train: Epo:81  Loss_stu:1.4571331696799916e-08\n",
      "student train: Epo:82  Loss_stu:1.7402486562900776e-08\n",
      "student train: Epo:83  Loss_stu:1.766092161403776e-08\n",
      "student train: Epo:84  Loss_stu:2.6411512976665108e-08\n",
      "student train: Epo:85  Loss_stu:1.630147217213107e-08\n",
      "student train: Epo:86  Loss_stu:2.6320645218902428e-08\n",
      "student train: Epo:87  Loss_stu:1.2276904826080681e-08\n",
      "student train: Epo:88  Loss_stu:1.4376416501704625e-08\n",
      "student train: Epo:89  Loss_stu:1.9166346731935846e-08\n",
      "student train: Epo:90  Loss_stu:2.4297404976891812e-08\n",
      "student train: Epo:91  Loss_stu:1.9697436570709215e-08\n",
      "student train: Epo:92  Loss_stu:1.9063847389588773e-08\n",
      "student train: Epo:93  Loss_stu:1.3749074767588354e-08\n",
      "student train: Epo:94  Loss_stu:1.36898803404506e-08\n",
      "student train: Epo:95  Loss_stu:2.4316587854400495e-08\n",
      "student train: Epo:96  Loss_stu:1.5667685815401455e-08\n",
      "student train: Epo:97  Loss_stu:1.4863657860075818e-08\n",
      "student train: Epo:98  Loss_stu:1.5714542556111155e-08\n",
      "student train: Epo:99  Loss_stu:1.2777451097178982e-08\n",
      "student train: Epo:100  Loss_stu:2.3450480668429918e-08\n",
      "student train: Epo:101  Loss_stu:2.1060154509200402e-08\n",
      "student train: Epo:102  Loss_stu:1.9265771200593917e-08\n",
      "student train: Epo:103  Loss_stu:1.6975246097672425e-08\n",
      "student train: Epo:104  Loss_stu:1.7179662137323248e-08\n",
      "student train: Epo:105  Loss_stu:2.002734333927947e-08\n",
      "student train: Epo:106  Loss_stu:2.306573598787054e-08\n",
      "student train: Epo:107  Loss_stu:3.361748568408984e-08\n",
      "student train: Epo:108  Loss_stu:2.773671781142184e-08\n",
      "student train: Epo:109  Loss_stu:2.0719980398098414e-08\n",
      "student train: Epo:110  Loss_stu:3.077516907978861e-08\n",
      "student train: Epo:111  Loss_stu:1.7481946557040828e-08\n",
      "student train: Epo:112  Loss_stu:1.5438565981185093e-08\n",
      "student train: Epo:113  Loss_stu:2.362537365740991e-08\n",
      "student train: Epo:114  Loss_stu:2.165838530743258e-08\n",
      "student train: Epo:115  Loss_stu:2.3835271534267122e-08\n",
      "student train: Epo:116  Loss_stu:2.2074697625384943e-08\n",
      "student train: Epo:117  Loss_stu:2.151758060620068e-08\n",
      "student train: Epo:118  Loss_stu:2.2003495914191262e-08\n",
      "student train: Epo:119  Loss_stu:1.7288428466599726e-08\n",
      "student train: Epo:120  Loss_stu:2.4743497917256718e-08\n",
      "student train: Epo:121  Loss_stu:2.0952487744807513e-08\n",
      "student train: Epo:122  Loss_stu:2.873998106167619e-08\n",
      "student train: Epo:123  Loss_stu:1.9110361293428468e-08\n",
      "student train: Epo:124  Loss_stu:2.2508807262511255e-08\n",
      "student train: Epo:125  Loss_stu:2.5389727653646332e-08\n",
      "student train: Epo:126  Loss_stu:2.5225185495969527e-08\n",
      "student train: Epo:127  Loss_stu:3.0839043319019765e-08\n",
      "student train: Epo:128  Loss_stu:2.353284678235923e-08\n",
      "student train: Epo:129  Loss_stu:2.540164345532503e-08\n",
      "student train: Epo:130  Loss_stu:2.3520716041502965e-08\n",
      "student train: Epo:131  Loss_stu:2.890921457776585e-08\n",
      "student train: Epo:132  Loss_stu:2.5885476873099833e-08\n",
      "student train: Epo:133  Loss_stu:1.8502806398146276e-08\n",
      "student train: Epo:134  Loss_stu:2.237642249269811e-08\n",
      "student train: Epo:135  Loss_stu:1.8484687558384394e-08\n",
      "student train: Epo:136  Loss_stu:1.8742451146636085e-08\n",
      "student train: Epo:137  Loss_stu:2.536290288901455e-08\n",
      "student train: Epo:138  Loss_stu:2.3685444716647908e-08\n",
      "student train: Epo:139  Loss_stu:1.8924435352118962e-08\n",
      "student train: Epo:140  Loss_stu:2.3811026039766148e-08\n",
      "student train: Epo:141  Loss_stu:2.0061200700638437e-08\n",
      "student train: Epo:142  Loss_stu:1.889165091029099e-08\n",
      "student train: Epo:143  Loss_stu:1.8243836663600632e-08\n",
      "student train: Epo:144  Loss_stu:2.0632095143469087e-08\n",
      "student train: Epo:145  Loss_stu:1.7000992613702692e-08\n",
      "student train: Epo:146  Loss_stu:1.5896256755354443e-08\n",
      "student train: Epo:147  Loss_stu:1.7842765487330325e-08\n",
      "student train: Epo:148  Loss_stu:1.6996283491721442e-08\n",
      "student train: Epo:149  Loss_stu:1.4282639959617427e-08\n",
      "student train: Epo:150  Loss_stu:1.7316738265549247e-08\n",
      "student train: Epo:151  Loss_stu:2.8629902004695396e-08\n",
      "student train: Epo:152  Loss_stu:2.354200034915266e-08\n",
      "student train: Epo:153  Loss_stu:1.9250235183676523e-08\n",
      "student train: Epo:154  Loss_stu:1.3850396385350905e-08\n",
      "student train: Epo:155  Loss_stu:2.5496278865944078e-08\n",
      "student train: Epo:156  Loss_stu:1.6828481719244337e-08\n",
      "student train: Epo:157  Loss_stu:1.4609554455091711e-08\n",
      "student train: Epo:158  Loss_stu:1.944694005828751e-08\n",
      "student train: Epo:159  Loss_stu:1.9057681654999215e-08\n",
      "student train: Epo:160  Loss_stu:1.8243204280565806e-08\n",
      "student train: Epo:161  Loss_stu:1.8589512151834242e-08\n",
      "student train: Epo:162  Loss_stu:1.4217444999076179e-08\n",
      "student train: Epo:163  Loss_stu:1.8059251871704873e-08\n",
      "student train: Epo:164  Loss_stu:1.1695260759836401e-08\n",
      "student train: Epo:165  Loss_stu:1.3411039390121005e-08\n",
      "student train: Epo:166  Loss_stu:1.5639528783140122e-08\n",
      "student train: Epo:167  Loss_stu:1.4190955965887042e-08\n",
      "student train: Epo:168  Loss_stu:1.281430250799076e-08\n",
      "student train: Epo:169  Loss_stu:1.732419541156105e-08\n",
      "student train: Epo:170  Loss_stu:1.5774135775359355e-08\n",
      "student train: Epo:171  Loss_stu:1.4135792092417887e-08\n",
      "student train: Epo:172  Loss_stu:1.4388341185167519e-08\n",
      "student train: Epo:173  Loss_stu:1.5251838902941017e-08\n",
      "student train: Epo:174  Loss_stu:1.8999577022782432e-08\n",
      "student train: Epo:175  Loss_stu:1.4776274426253622e-08\n",
      "student train: Epo:176  Loss_stu:1.7124012430258517e-08\n",
      "student train: Epo:177  Loss_stu:1.9865190381551656e-08\n",
      "student train: Epo:178  Loss_stu:1.3544204868765064e-08\n",
      "student train: Epo:179  Loss_stu:1.457111498126551e-08\n",
      "student train: Epo:180  Loss_stu:1.0295721608599706e-08\n",
      "student train: Epo:181  Loss_stu:1.0547449136311116e-08\n",
      "student train: Epo:182  Loss_stu:1.5112126661165348e-08\n",
      "student train: Epo:183  Loss_stu:1.452398912249464e-08\n",
      "student train: Epo:184  Loss_stu:1.071928146245682e-08\n",
      "student train: Epo:185  Loss_stu:7.409648450362738e-09\n",
      "student train: Epo:186  Loss_stu:1.0377560144547715e-08\n",
      "student train: Epo:187  Loss_stu:9.57639922916087e-09\n",
      "student train: Epo:188  Loss_stu:1.3365298201506448e-08\n",
      "student train: Epo:189  Loss_stu:1.804667526528192e-08\n",
      "student train: Epo:190  Loss_stu:1.1577305336629706e-08\n",
      "student train: Epo:191  Loss_stu:8.45967207396825e-09\n",
      "student train: Epo:192  Loss_stu:9.080060259236689e-09\n",
      "student train: Epo:193  Loss_stu:7.132257451303303e-09\n",
      "student train: Epo:194  Loss_stu:1.0331138611263668e-08\n",
      "student train: Epo:195  Loss_stu:1.0391913995988489e-08\n",
      "student train: Epo:196  Loss_stu:5.895410382095179e-09\n",
      "student train: Epo:197  Loss_stu:9.183581006766417e-09\n",
      "student train: Epo:198  Loss_stu:8.873708878809339e-09\n",
      "student train: Epo:199  Loss_stu:7.433302862125402e-09\n",
      "student train: Epo:200  Loss_stu:1.1268276978171343e-08\n",
      "student train: Epo:201  Loss_stu:1.1560575607916235e-08\n",
      "student train: Epo:202  Loss_stu:8.99339180904235e-09\n",
      "student train: Epo:203  Loss_stu:1.1153472811997744e-08\n",
      "student train: Epo:204  Loss_stu:6.9744916508795995e-09\n",
      "student train: Epo:205  Loss_stu:8.998406464399977e-09\n",
      "student train: Epo:206  Loss_stu:6.783297923362852e-09\n",
      "student train: Epo:207  Loss_stu:8.092564840467276e-09\n",
      "student train: Epo:208  Loss_stu:1.1499478702603483e-08\n",
      "student train: Epo:209  Loss_stu:8.226626491136813e-09\n",
      "student train: Epo:210  Loss_stu:6.314876621615895e-09\n",
      "student train: Epo:211  Loss_stu:9.129080602576778e-09\n",
      "student train: Epo:212  Loss_stu:5.485551568540359e-09\n",
      "student train: Epo:213  Loss_stu:1.4163083150720013e-08\n",
      "student train: Epo:214  Loss_stu:7.911592270204437e-09\n",
      "student train: Epo:215  Loss_stu:6.23675688871117e-09\n",
      "student train: Epo:216  Loss_stu:6.497445692588144e-09\n",
      "student train: Epo:217  Loss_stu:8.161671338768883e-09\n",
      "student train: Epo:218  Loss_stu:9.143046320048143e-09\n",
      "student train: Epo:219  Loss_stu:9.607519224630323e-09\n",
      "student train: Epo:220  Loss_stu:6.492284043702057e-09\n",
      "student train: Epo:221  Loss_stu:7.084727471351471e-09\n",
      "student train: Epo:222  Loss_stu:6.972360022672319e-09\n",
      "student train: Epo:223  Loss_stu:5.7276081655288635e-09\n",
      "student train: Epo:224  Loss_stu:7.373470278793093e-09\n",
      "student train: Epo:225  Loss_stu:4.726991686965221e-09\n",
      "student train: Epo:226  Loss_stu:6.635870963833668e-09\n",
      "student train: Epo:227  Loss_stu:5.70682612277551e-09\n",
      "student train: Epo:228  Loss_stu:8.088949954299096e-09\n",
      "student train: Epo:229  Loss_stu:4.274579357144148e-09\n",
      "student train: Epo:230  Loss_stu:5.867863084318969e-09\n",
      "student train: Epo:231  Loss_stu:9.076539519980997e-09\n",
      "student train: Epo:232  Loss_stu:8.620537172987497e-09\n",
      "student train: Epo:233  Loss_stu:1.2045969555174452e-08\n",
      "student train: Epo:234  Loss_stu:1.0241188341808538e-08\n",
      "student train: Epo:235  Loss_stu:5.46946621327038e-09\n",
      "student train: Epo:236  Loss_stu:8.073664403696057e-09\n",
      "student train: Epo:237  Loss_stu:6.508217964551477e-09\n",
      "student train: Epo:238  Loss_stu:8.58516191470926e-09\n",
      "student train: Epo:239  Loss_stu:7.066713880732323e-09\n",
      "student train: Epo:240  Loss_stu:7.996289852485461e-09\n",
      "student train: Epo:241  Loss_stu:5.009584747028839e-09\n",
      "student train: Epo:242  Loss_stu:8.646803273393289e-09\n",
      "student train: Epo:243  Loss_stu:8.010339058728277e-09\n",
      "student train: Epo:244  Loss_stu:1.0147715556740877e-08\n",
      "student train: Epo:245  Loss_stu:7.682698033306679e-09\n",
      "student train: Epo:246  Loss_stu:9.552252322464483e-09\n",
      "student train: Epo:247  Loss_stu:6.574392141800445e-09\n",
      "student train: Epo:248  Loss_stu:6.541549524285983e-09\n",
      "student train: Epo:249  Loss_stu:5.401526337323048e-09\n",
      "student train: Epo:250  Loss_stu:5.176546746810118e-09\n",
      "student train: Epo:251  Loss_stu:7.830403880859649e-09\n",
      "student train: Epo:252  Loss_stu:6.1123914818494995e-09\n",
      "student train: Epo:253  Loss_stu:1.1057134763348131e-08\n",
      "student train: Epo:254  Loss_stu:7.945456736990764e-09\n",
      "student train: Epo:255  Loss_stu:3.978255058711966e-09\n",
      "student train: Epo:256  Loss_stu:6.3988214726862225e-09\n",
      "student train: Epo:257  Loss_stu:7.525506440231311e-09\n",
      "student train: Epo:258  Loss_stu:7.682592340074734e-09\n",
      "student train: Epo:259  Loss_stu:7.450690731047871e-09\n",
      "student train: Epo:260  Loss_stu:6.618857018025892e-09\n",
      "student train: Epo:261  Loss_stu:7.853818040359783e-09\n",
      "student train: Epo:262  Loss_stu:7.799992651769116e-09\n",
      "student train: Epo:263  Loss_stu:9.722117333410552e-09\n",
      "student train: Epo:264  Loss_stu:5.220843313225032e-09\n",
      "student train: Epo:265  Loss_stu:3.718862995327754e-09\n",
      "student train: Epo:266  Loss_stu:5.8851807871462825e-09\n",
      "student train: Epo:267  Loss_stu:7.489746600697345e-09\n",
      "student train: Epo:268  Loss_stu:4.429826283569582e-09\n",
      "student train: Epo:269  Loss_stu:8.552569319419945e-09\n",
      "student train: Epo:270  Loss_stu:3.980235696587897e-09\n",
      "student train: Epo:271  Loss_stu:6.396169371924998e-09\n",
      "student train: Epo:272  Loss_stu:5.9827196530193305e-09\n",
      "student train: Epo:273  Loss_stu:9.177674620275411e-09\n",
      "student train: Epo:274  Loss_stu:8.448833632712649e-09\n",
      "student train: Epo:275  Loss_stu:5.791882529138093e-09\n",
      "student train: Epo:276  Loss_stu:7.691537184939534e-09\n",
      "student train: Epo:277  Loss_stu:4.12935641236345e-09\n",
      "student train: Epo:278  Loss_stu:5.871616082231412e-09\n",
      "student train: Epo:279  Loss_stu:5.8860929463833145e-09\n",
      "student train: Epo:280  Loss_stu:8.244112059685449e-09\n",
      "student train: Epo:281  Loss_stu:6.2970264558259714e-09\n",
      "student train: Epo:282  Loss_stu:5.739066111232205e-09\n",
      "student train: Epo:283  Loss_stu:3.2277081007947572e-09\n",
      "student train: Epo:284  Loss_stu:4.161870847951832e-09\n",
      "student train: Epo:285  Loss_stu:5.703285399505376e-09\n",
      "student train: Epo:286  Loss_stu:4.227780348031729e-09\n",
      "student train: Epo:287  Loss_stu:7.96291299565155e-09\n",
      "student train: Epo:288  Loss_stu:5.099564770461029e-09\n",
      "student train: Epo:289  Loss_stu:6.7098016032218766e-09\n",
      "student train: Epo:290  Loss_stu:6.971881294504101e-09\n",
      "student train: Epo:291  Loss_stu:4.881705262249625e-09\n",
      "student train: Epo:292  Loss_stu:6.761188497961257e-09\n",
      "student train: Epo:293  Loss_stu:5.423808069338065e-09\n",
      "student train: Epo:294  Loss_stu:6.716092126879403e-09\n",
      "student train: Epo:295  Loss_stu:7.795253331721597e-09\n",
      "student train: Epo:296  Loss_stu:5.729107854790527e-09\n",
      "student train: Epo:297  Loss_stu:7.37981808995869e-09\n",
      "student train: Epo:298  Loss_stu:7.836401749727884e-09\n",
      "student train: Epo:299  Loss_stu:6.6257541675440734e-09\n",
      "student train: Epo:300  Loss_stu:3.589369690359945e-09\n",
      "student train: Epo:301  Loss_stu:5.283334658656713e-09\n",
      "student train: Epo:302  Loss_stu:6.607039804151782e-09\n",
      "student train: Epo:303  Loss_stu:4.074928394715016e-09\n",
      "student train: Epo:304  Loss_stu:8.332198930816048e-09\n",
      "student train: Epo:305  Loss_stu:5.335933916938984e-09\n",
      "student train: Epo:306  Loss_stu:4.546330867327697e-09\n",
      "student train: Epo:307  Loss_stu:5.219490617491829e-09\n",
      "student train: Epo:308  Loss_stu:8.196636258617218e-09\n",
      "student train: Epo:309  Loss_stu:6.6884382476928295e-09\n",
      "student train: Epo:310  Loss_stu:7.027558535099843e-09\n",
      "student train: Epo:311  Loss_stu:6.767021165643428e-09\n",
      "student train: Epo:312  Loss_stu:6.1282405816598384e-09\n",
      "student train: Epo:313  Loss_stu:5.4262092596957245e-09\n",
      "student train: Epo:314  Loss_stu:5.437545524955567e-09\n",
      "student train: Epo:315  Loss_stu:5.573948858028643e-09\n",
      "student train: Epo:316  Loss_stu:5.725224738739598e-09\n",
      "student train: Epo:317  Loss_stu:8.473850066081923e-09\n",
      "student train: Epo:318  Loss_stu:6.973048805036797e-09\n",
      "student train: Epo:319  Loss_stu:5.281934001288846e-09\n",
      "student train: Epo:320  Loss_stu:7.285280823055018e-09\n",
      "student train: Epo:321  Loss_stu:6.455417089767934e-09\n",
      "student train: Epo:322  Loss_stu:5.8632245725220855e-09\n",
      "student train: Epo:323  Loss_stu:5.7331281944073e-09\n",
      "student train: Epo:324  Loss_stu:4.848030865645114e-09\n",
      "student train: Epo:325  Loss_stu:5.878037612205844e-09\n",
      "student train: Epo:326  Loss_stu:4.632122241332581e-09\n",
      "student train: Epo:327  Loss_stu:6.874341096363423e-09\n",
      "student train: Epo:328  Loss_stu:7.50676143468354e-09\n",
      "student train: Epo:329  Loss_stu:5.7082734095104115e-09\n",
      "student train: Epo:330  Loss_stu:7.53723750079871e-09\n",
      "student train: Epo:331  Loss_stu:4.316332180565041e-09\n",
      "student train: Epo:332  Loss_stu:9.33823862681038e-09\n",
      "student train: Epo:333  Loss_stu:5.987543350016722e-09\n",
      "student train: Epo:334  Loss_stu:6.317309786396663e-09\n",
      "student train: Epo:335  Loss_stu:9.235361808634934e-09\n",
      "student train: Epo:336  Loss_stu:6.665785701187588e-09\n",
      "student train: Epo:337  Loss_stu:4.888673466041382e-09\n",
      "student train: Epo:338  Loss_stu:5.622970533636362e-09\n",
      "student train: Epo:339  Loss_stu:8.353007174832783e-09\n",
      "student train: Epo:340  Loss_stu:5.82038284235864e-09\n",
      "student train: Epo:341  Loss_stu:4.813885734478163e-09\n",
      "student train: Epo:342  Loss_stu:8.39468672353405e-09\n",
      "student train: Epo:343  Loss_stu:1.0064876931892286e-08\n",
      "student train: Epo:344  Loss_stu:5.41496891770521e-09\n",
      "student train: Epo:345  Loss_stu:7.839920712626736e-09\n",
      "student train: Epo:346  Loss_stu:5.2493880353665645e-09\n",
      "student train: Epo:347  Loss_stu:3.4273910376469985e-09\n",
      "student train: Epo:348  Loss_stu:7.07791825149684e-09\n",
      "student train: Epo:349  Loss_stu:6.149359244034258e-09\n",
      "student train: Epo:350  Loss_stu:4.802735542597247e-09\n",
      "student train: Epo:351  Loss_stu:3.881261534388614e-09\n",
      "student train: Epo:352  Loss_stu:7.410897229220836e-09\n",
      "student train: Epo:353  Loss_stu:5.200921027181948e-09\n",
      "student train: Epo:354  Loss_stu:4.4765124940227e-09\n",
      "student train: Epo:355  Loss_stu:8.426304098918536e-09\n",
      "student train: Epo:356  Loss_stu:5.611092479540503e-09\n",
      "student train: Epo:357  Loss_stu:4.0544430035538426e-09\n",
      "student train: Epo:358  Loss_stu:5.958116666704427e-09\n",
      "student train: Epo:359  Loss_stu:6.247692141414518e-09\n",
      "student train: Epo:360  Loss_stu:7.515989608464224e-09\n",
      "student train: Epo:361  Loss_stu:5.3360600382745815e-09\n",
      "student train: Epo:362  Loss_stu:7.611871133406112e-09\n",
      "student train: Epo:363  Loss_stu:5.922708101735452e-09\n",
      "student train: Epo:364  Loss_stu:9.08097330665214e-09\n",
      "student train: Epo:365  Loss_stu:6.857367562673744e-09\n",
      "student train: Epo:366  Loss_stu:7.648795374848305e-09\n",
      "student train: Epo:367  Loss_stu:4.71023042791785e-09\n",
      "student train: Epo:368  Loss_stu:6.739486302365094e-09\n",
      "student train: Epo:369  Loss_stu:8.06248756646255e-09\n",
      "student train: Epo:370  Loss_stu:8.542053286930695e-09\n",
      "student train: Epo:371  Loss_stu:1.0371403291742354e-08\n",
      "student train: Epo:372  Loss_stu:9.397647993125702e-09\n",
      "student train: Epo:373  Loss_stu:6.1523555139331165e-09\n",
      "student train: Epo:374  Loss_stu:8.99532004439152e-09\n",
      "student train: Epo:375  Loss_stu:8.477919699600989e-09\n",
      "student train: Epo:376  Loss_stu:4.9610893171347925e-09\n",
      "student train: Epo:377  Loss_stu:4.567671574307042e-09\n",
      "student train: Epo:378  Loss_stu:7.869271456684146e-09\n",
      "student train: Epo:379  Loss_stu:7.906090004894395e-09\n",
      "student train: Epo:380  Loss_stu:6.208070058022486e-09\n",
      "student train: Epo:381  Loss_stu:2.7001465507936473e-09\n",
      "student train: Epo:382  Loss_stu:5.553956849979613e-09\n",
      "student train: Epo:383  Loss_stu:7.904101373412686e-09\n",
      "student train: Epo:384  Loss_stu:4.278403409330167e-09\n",
      "student train: Epo:385  Loss_stu:5.735500963055529e-09\n",
      "student train: Epo:386  Loss_stu:6.761805337873739e-09\n",
      "student train: Epo:387  Loss_stu:5.545186532174284e-09\n",
      "student train: Epo:388  Loss_stu:3.5355396388325744e-09\n",
      "student train: Epo:389  Loss_stu:7.517253486355457e-09\n",
      "student train: Epo:390  Loss_stu:4.020280108818497e-09\n",
      "student train: Epo:391  Loss_stu:3.431642969786708e-09\n",
      "student train: Epo:392  Loss_stu:5.460080387820199e-09\n",
      "student train: Epo:393  Loss_stu:5.438800965151813e-09\n",
      "student train: Epo:394  Loss_stu:2.5770174882921992e-09\n",
      "student train: Epo:395  Loss_stu:6.038952449216595e-09\n",
      "student train: Epo:396  Loss_stu:6.7852128360357256e-09\n",
      "student train: Epo:397  Loss_stu:7.124344225672985e-09\n",
      "student train: Epo:398  Loss_stu:3.4562126494108725e-09\n",
      "student train: Epo:399  Loss_stu:4.4194830017829645e-09\n",
      "student train: Epo:400  Loss_stu:5.442204908945314e-09\n",
      "student train: Epo:401  Loss_stu:8.0359212617509e-09\n",
      "student train: Epo:402  Loss_stu:8.271309859253506e-09\n",
      "student train: Epo:403  Loss_stu:4.101384121213414e-09\n",
      "student train: Epo:404  Loss_stu:5.34586730438491e-09\n",
      "student train: Epo:405  Loss_stu:3.311377394510373e-09\n",
      "student train: Epo:406  Loss_stu:5.1359942965234495e-09\n",
      "student train: Epo:407  Loss_stu:2.9085693853403427e-09\n",
      "student train: Epo:408  Loss_stu:5.290907711952286e-09\n",
      "student train: Epo:409  Loss_stu:3.816257088118391e-09\n",
      "student train: Epo:410  Loss_stu:6.130296714701444e-09\n",
      "student train: Epo:411  Loss_stu:8.918011218383981e-09\n",
      "student train: Epo:412  Loss_stu:7.767919640855325e-09\n",
      "student train: Epo:413  Loss_stu:4.9189812223460194e-09\n",
      "student train: Epo:414  Loss_stu:4.733275105195389e-09\n",
      "student train: Epo:415  Loss_stu:3.895757938465749e-09\n",
      "student train: Epo:416  Loss_stu:2.7174051897560503e-09\n",
      "student train: Epo:417  Loss_stu:4.246488050085873e-09\n",
      "student train: Epo:418  Loss_stu:5.56009638330579e-09\n",
      "student train: Epo:419  Loss_stu:5.510682132836564e-09\n",
      "student train: Epo:420  Loss_stu:4.5255075242778275e-09\n",
      "student train: Epo:421  Loss_stu:3.582562912995968e-09\n",
      "student train: Epo:422  Loss_stu:4.727092051126647e-09\n",
      "student train: Epo:423  Loss_stu:5.90707571745952e-09\n",
      "student train: Epo:424  Loss_stu:3.935460846093974e-09\n",
      "student train: Epo:425  Loss_stu:4.425141586494874e-09\n",
      "student train: Epo:426  Loss_stu:6.128149987461029e-09\n",
      "student train: Epo:427  Loss_stu:4.614019832871463e-09\n",
      "student train: Epo:428  Loss_stu:2.5387794089226645e-09\n",
      "student train: Epo:429  Loss_stu:3.41582095941817e-09\n",
      "student train: Epo:430  Loss_stu:3.3438938285001996e-09\n",
      "student train: Epo:431  Loss_stu:5.011000947519051e-09\n",
      "student train: Epo:432  Loss_stu:3.6111849066600143e-09\n",
      "student train: Epo:433  Loss_stu:4.473800441218145e-09\n",
      "student train: Epo:434  Loss_stu:3.1407298983765486e-09\n",
      "student train: Epo:435  Loss_stu:6.085688841750425e-09\n",
      "student train: Epo:436  Loss_stu:4.969227251905295e-09\n",
      "student train: Epo:437  Loss_stu:3.724010877448336e-09\n",
      "student train: Epo:438  Loss_stu:4.353070792717517e-09\n",
      "student train: Epo:439  Loss_stu:2.8643314387011287e-09\n",
      "student train: Epo:440  Loss_stu:6.014128306475186e-09\n",
      "student train: Epo:441  Loss_stu:3.3471765359394112e-09\n",
      "student train: Epo:442  Loss_stu:4.957892763002292e-09\n",
      "student train: Epo:443  Loss_stu:4.807461539968472e-09\n",
      "student train: Epo:444  Loss_stu:6.195922885865457e-09\n",
      "student train: Epo:445  Loss_stu:7.026843995561194e-09\n",
      "student train: Epo:446  Loss_stu:3.9704692866848745e-09\n",
      "student train: Epo:447  Loss_stu:3.164705164593329e-09\n",
      "student train: Epo:448  Loss_stu:3.815752602776001e-09\n",
      "student train: Epo:449  Loss_stu:3.3516034392278016e-09\n",
      "student train: Epo:450  Loss_stu:5.245511580653783e-09\n",
      "student train: Epo:451  Loss_stu:3.643899404437434e-09\n",
      "student train: Epo:452  Loss_stu:5.728742369370821e-09\n",
      "student train: Epo:453  Loss_stu:3.933909198394758e-09\n",
      "student train: Epo:454  Loss_stu:3.5100222728345898e-09\n",
      "student train: Epo:455  Loss_stu:4.506579553975598e-09\n",
      "student train: Epo:456  Loss_stu:2.573013802020796e-09\n",
      "student train: Epo:457  Loss_stu:4.434081102289156e-09\n",
      "student train: Epo:458  Loss_stu:5.145281090079834e-09\n",
      "student train: Epo:459  Loss_stu:6.483059866724261e-09\n",
      "student train: Epo:460  Loss_stu:4.813799137082242e-09\n",
      "student train: Epo:461  Loss_stu:5.689357873706058e-09\n",
      "student train: Epo:462  Loss_stu:4.615765991644594e-09\n",
      "student train: Epo:463  Loss_stu:2.6869475533430887e-09\n",
      "student train: Epo:464  Loss_stu:3.975786810883619e-09\n",
      "student train: Epo:465  Loss_stu:4.8804240648792074e-09\n",
      "student train: Epo:466  Loss_stu:5.61379565056086e-09\n",
      "student train: Epo:467  Loss_stu:8.521801930783113e-09\n",
      "student train: Epo:468  Loss_stu:6.563051435648504e-09\n",
      "student train: Epo:469  Loss_stu:4.287929566970661e-09\n",
      "student train: Epo:470  Loss_stu:5.833321381487622e-09\n",
      "student train: Epo:471  Loss_stu:5.967875971180092e-09\n",
      "student train: Epo:472  Loss_stu:4.797911401510646e-09\n",
      "student train: Epo:473  Loss_stu:1.6354902987458786e-09\n",
      "student train: Epo:474  Loss_stu:4.104064199594859e-09\n",
      "student train: Epo:475  Loss_stu:3.982561391779882e-09\n",
      "student train: Epo:476  Loss_stu:5.8565188254533496e-09\n",
      "student train: Epo:477  Loss_stu:3.6055447516503136e-09\n",
      "student train: Epo:478  Loss_stu:5.023838234308187e-09\n",
      "student train: Epo:479  Loss_stu:6.0046194683138765e-09\n",
      "student train: Epo:480  Loss_stu:6.525917584099261e-09\n",
      "student train: Epo:481  Loss_stu:5.9640838934171825e-09\n",
      "student train: Epo:482  Loss_stu:6.55399912119492e-09\n",
      "student train: Epo:483  Loss_stu:4.530229968935373e-09\n",
      "student train: Epo:484  Loss_stu:6.41396402656369e-09\n",
      "student train: Epo:485  Loss_stu:6.9183263562422326e-09\n",
      "student train: Epo:486  Loss_stu:2.692760903144631e-09\n",
      "student train: Epo:487  Loss_stu:4.334100189851142e-09\n",
      "student train: Epo:488  Loss_stu:5.294150895451821e-09\n",
      "student train: Epo:489  Loss_stu:6.256146267702434e-09\n",
      "student train: Epo:490  Loss_stu:3.6534411052002724e-09\n",
      "student train: Epo:491  Loss_stu:4.120525254336371e-09\n",
      "student train: Epo:492  Loss_stu:3.91992527326579e-09\n",
      "student train: Epo:493  Loss_stu:4.787812812878656e-09\n",
      "student train: Epo:494  Loss_stu:2.9785491850731205e-09\n",
      "student train: Epo:495  Loss_stu:3.246086732744402e-09\n",
      "student train: Epo:496  Loss_stu:7.263889045816541e-09\n",
      "student train: Epo:497  Loss_stu:5.389601653860154e-09\n",
      "student train: Epo:498  Loss_stu:3.6750245069328003e-09\n",
      "student train: Epo:499  Loss_stu:2.7920936673808683e-09\n",
      "student train: Epo:500  Loss_stu:8.720389743643864e-09\n",
      "student train: Epo:501  Loss_stu:5.653617130008115e-09\n",
      "student train: Epo:502  Loss_stu:3.2503704172626158e-09\n",
      "student train: Epo:503  Loss_stu:4.047065793599813e-09\n",
      "student train: Epo:504  Loss_stu:4.6242667472995436e-09\n",
      "student train: Epo:505  Loss_stu:5.37877342665638e-09\n",
      "student train: Epo:506  Loss_stu:5.47657341698482e-09\n",
      "student train: Epo:507  Loss_stu:5.684527959459729e-09\n",
      "student train: Epo:508  Loss_stu:7.4289250306947e-09\n",
      "student train: Epo:509  Loss_stu:7.090588116653862e-09\n",
      "student train: Epo:510  Loss_stu:4.766869121652917e-09\n",
      "student train: Epo:511  Loss_stu:4.073572590357344e-09\n",
      "student train: Epo:512  Loss_stu:6.238366268007667e-09\n",
      "student train: Epo:513  Loss_stu:5.624770871293094e-09\n",
      "student train: Epo:514  Loss_stu:4.339450132562206e-09\n",
      "student train: Epo:515  Loss_stu:4.735248637643963e-09\n",
      "student train: Epo:516  Loss_stu:6.28968432891952e-09\n",
      "student train: Epo:517  Loss_stu:5.181595152947693e-09\n",
      "student train: Epo:518  Loss_stu:5.221771459673619e-09\n",
      "student train: Epo:519  Loss_stu:5.184963125515196e-09\n",
      "student train: Epo:520  Loss_stu:4.1233740866175594e-09\n",
      "student train: Epo:521  Loss_stu:2.621970418559272e-09\n",
      "student train: Epo:522  Loss_stu:4.961732802399865e-09\n",
      "student train: Epo:523  Loss_stu:5.581967776890906e-09\n",
      "student train: Epo:524  Loss_stu:7.0977632660174095e-09\n",
      "student train: Epo:525  Loss_stu:5.913610490182464e-09\n",
      "student train: Epo:526  Loss_stu:7.0376060534727e-09\n",
      "student train: Epo:527  Loss_stu:4.424937749547553e-09\n",
      "student train: Epo:528  Loss_stu:9.414137913665854e-09\n",
      "student train: Epo:529  Loss_stu:6.588826817477411e-09\n",
      "student train: Epo:530  Loss_stu:3.8334877494605735e-09\n",
      "student train: Epo:531  Loss_stu:6.4958927126212984e-09\n",
      "student train: Epo:532  Loss_stu:7.835367910047353e-09\n",
      "student train: Epo:533  Loss_stu:6.093593185596546e-09\n",
      "student train: Epo:534  Loss_stu:5.388209878276484e-09\n",
      "student train: Epo:535  Loss_stu:4.772821249332537e-09\n",
      "student train: Epo:536  Loss_stu:3.1534919120446148e-09\n",
      "student train: Epo:537  Loss_stu:5.640299782783131e-09\n",
      "student train: Epo:538  Loss_stu:3.5467548897827328e-09\n",
      "student train: Epo:539  Loss_stu:5.065059482944889e-09\n",
      "student train: Epo:540  Loss_stu:5.607488695602569e-09\n",
      "student train: Epo:541  Loss_stu:4.511691464870182e-09\n",
      "student train: Epo:542  Loss_stu:3.63070196129911e-09\n",
      "student train: Epo:543  Loss_stu:5.93672355719832e-09\n",
      "student train: Epo:544  Loss_stu:5.242537071126208e-09\n",
      "student train: Epo:545  Loss_stu:6.112942152469714e-09\n",
      "student train: Epo:546  Loss_stu:7.710845295605395e-09\n",
      "student train: Epo:547  Loss_stu:4.099859118866789e-09\n",
      "student train: Epo:548  Loss_stu:8.923897176771334e-09\n",
      "student train: Epo:549  Loss_stu:5.848063366897804e-09\n",
      "student train: Epo:550  Loss_stu:4.032614242532873e-09\n",
      "student train: Epo:551  Loss_stu:4.24009716226692e-09\n",
      "student train: Epo:552  Loss_stu:5.399483971046948e-09\n",
      "student train: Epo:553  Loss_stu:8.717594646157067e-09\n",
      "student train: Epo:554  Loss_stu:7.712423588657202e-09\n",
      "student train: Epo:555  Loss_stu:6.628850801604358e-09\n",
      "student train: Epo:556  Loss_stu:3.785599389516392e-09\n",
      "student train: Epo:557  Loss_stu:7.43324823915259e-09\n",
      "student train: Epo:558  Loss_stu:6.986453193746911e-09\n",
      "student train: Epo:559  Loss_stu:5.04959718483633e-09\n",
      "student train: Epo:560  Loss_stu:5.152205329039816e-09\n",
      "student train: Epo:561  Loss_stu:7.024864245863682e-09\n",
      "student train: Epo:562  Loss_stu:4.49096626553569e-09\n",
      "student train: Epo:563  Loss_stu:4.098514416739363e-09\n",
      "student train: Epo:564  Loss_stu:6.885825243330146e-09\n",
      "student train: Epo:565  Loss_stu:6.6539356247119485e-09\n",
      "student train: Epo:566  Loss_stu:4.903337735839841e-09\n",
      "student train: Epo:567  Loss_stu:5.456856300156687e-09\n",
      "student train: Epo:568  Loss_stu:4.268948750052459e-09\n",
      "student train: Epo:569  Loss_stu:7.608089269695029e-09\n",
      "student train: Epo:570  Loss_stu:4.201910819290333e-09\n",
      "student train: Epo:571  Loss_stu:4.286788257701346e-09\n",
      "student train: Epo:572  Loss_stu:5.265255786923717e-09\n",
      "student train: Epo:573  Loss_stu:3.479831534036748e-09\n",
      "student train: Epo:574  Loss_stu:3.687870231416923e-09\n",
      "student train: Epo:575  Loss_stu:3.6621641275047523e-09\n",
      "student train: Epo:576  Loss_stu:2.444331625994778e-09\n",
      "student train: Epo:577  Loss_stu:4.672730646859691e-09\n",
      "student train: Epo:578  Loss_stu:4.925423624513314e-09\n",
      "student train: Epo:579  Loss_stu:5.351028509181788e-09\n",
      "student train: Epo:580  Loss_stu:5.559321003545392e-09\n",
      "student train: Epo:581  Loss_stu:7.956873382397589e-09\n",
      "student train: Epo:582  Loss_stu:5.49511369740685e-09\n",
      "student train: Epo:583  Loss_stu:4.15781942209037e-09\n",
      "student train: Epo:584  Loss_stu:4.852747981232142e-09\n",
      "student train: Epo:585  Loss_stu:5.092570365405891e-09\n",
      "student train: Epo:586  Loss_stu:5.544187331452122e-09\n",
      "student train: Epo:587  Loss_stu:8.391206840485665e-09\n",
      "student train: Epo:588  Loss_stu:6.440326050238809e-09\n",
      "student train: Epo:589  Loss_stu:4.570893885613714e-09\n",
      "student train: Epo:590  Loss_stu:6.201137825456726e-09\n",
      "student train: Epo:591  Loss_stu:4.452596957804644e-09\n",
      "student train: Epo:592  Loss_stu:6.619372605598528e-09\n",
      "student train: Epo:593  Loss_stu:8.058085754214517e-09\n",
      "student train: Epo:594  Loss_stu:6.646392325393435e-09\n",
      "student train: Epo:595  Loss_stu:5.4393067827618324e-09\n",
      "student train: Epo:596  Loss_stu:3.2216718182098703e-09\n",
      "student train: Epo:597  Loss_stu:7.359730158640332e-09\n",
      "student train: Epo:598  Loss_stu:7.52470974418884e-09\n",
      "student train: Epo:599  Loss_stu:5.699661631552999e-09\n",
      "student train: Epo:600  Loss_stu:4.5667718495678855e-09\n",
      "student train: Epo:601  Loss_stu:4.059885760909765e-09\n",
      "student train: Epo:602  Loss_stu:5.237129396817863e-09\n",
      "student train: Epo:603  Loss_stu:5.625293120203878e-09\n",
      "student train: Epo:604  Loss_stu:5.869853492157517e-09\n",
      "student train: Epo:605  Loss_stu:9.47167144715877e-09\n",
      "student train: Epo:606  Loss_stu:4.219472327093854e-09\n",
      "student train: Epo:607  Loss_stu:4.619018945106745e-09\n",
      "student train: Epo:608  Loss_stu:8.10810174556309e-09\n",
      "student train: Epo:609  Loss_stu:6.1265699180523825e-09\n",
      "student train: Epo:610  Loss_stu:2.5977560103029873e-09\n",
      "student train: Epo:611  Loss_stu:3.6419725013558946e-09\n",
      "student train: Epo:612  Loss_stu:3.991091013233472e-09\n",
      "student train: Epo:613  Loss_stu:5.910213651816321e-09\n",
      "student train: Epo:614  Loss_stu:4.006758480556982e-09\n",
      "student train: Epo:615  Loss_stu:4.228466909950157e-09\n",
      "student train: Epo:616  Loss_stu:4.180618073945652e-09\n",
      "student train: Epo:617  Loss_stu:6.696455390198253e-09\n",
      "student train: Epo:618  Loss_stu:8.335704571038605e-09\n",
      "student train: Epo:619  Loss_stu:5.744874798097044e-09\n",
      "student train: Epo:620  Loss_stu:7.364743037641119e-09\n",
      "student train: Epo:621  Loss_stu:8.779115212576016e-09\n",
      "student train: Epo:622  Loss_stu:5.58811574791207e-09\n",
      "student train: Epo:623  Loss_stu:4.151436083787985e-09\n",
      "student train: Epo:624  Loss_stu:5.305116790310649e-09\n",
      "student train: Epo:625  Loss_stu:8.669053030985197e-09\n",
      "student train: Epo:626  Loss_stu:7.1145254132432e-09\n",
      "student train: Epo:627  Loss_stu:8.873847434642812e-09\n",
      "student train: Epo:628  Loss_stu:6.719321987702642e-09\n",
      "student train: Epo:629  Loss_stu:6.6363217143816655e-09\n",
      "student train: Epo:630  Loss_stu:6.340815428274027e-09\n",
      "student train: Epo:631  Loss_stu:5.965248295325409e-09\n",
      "student train: Epo:632  Loss_stu:6.088875181831099e-09\n",
      "student train: Epo:633  Loss_stu:6.187277801217306e-09\n",
      "student train: Epo:634  Loss_stu:4.972892764243397e-09\n",
      "student train: Epo:635  Loss_stu:5.6263846914816895e-09\n",
      "student train: Epo:636  Loss_stu:5.7283369159222275e-09\n",
      "student train: Epo:637  Loss_stu:6.580293643310142e-09\n",
      "student train: Epo:638  Loss_stu:5.67566704745559e-09\n",
      "student train: Epo:639  Loss_stu:6.353262804736914e-09\n",
      "student train: Epo:640  Loss_stu:4.981892232081009e-09\n",
      "student train: Epo:641  Loss_stu:5.082732457140082e-09\n",
      "student train: Epo:642  Loss_stu:4.587918933651736e-09\n",
      "student train: Epo:643  Loss_stu:4.7359369759192305e-09\n",
      "student train: Epo:644  Loss_stu:6.4766907392765916e-09\n",
      "student train: Epo:645  Loss_stu:8.373227444735676e-09\n",
      "student train: Epo:646  Loss_stu:8.761023018166725e-09\n",
      "student train: Epo:647  Loss_stu:9.163071190698702e-09\n",
      "student train: Epo:648  Loss_stu:5.783410195192573e-09\n",
      "student train: Epo:649  Loss_stu:6.222967474656116e-09\n",
      "student train: Epo:650  Loss_stu:5.310193618157655e-09\n",
      "student train: Epo:651  Loss_stu:1.1248952880293928e-08\n",
      "student train: Epo:652  Loss_stu:5.532665880991772e-09\n",
      "student train: Epo:653  Loss_stu:5.5028821499547576e-09\n",
      "student train: Epo:654  Loss_stu:7.630142739856183e-09\n",
      "student train: Epo:655  Loss_stu:9.533343003909067e-09\n",
      "student train: Epo:656  Loss_stu:6.0083364950003215e-09\n",
      "student train: Epo:657  Loss_stu:8.042279731057533e-09\n",
      "student train: Epo:658  Loss_stu:5.130864177971262e-09\n",
      "student train: Epo:659  Loss_stu:7.574651128550158e-09\n",
      "student train: Epo:660  Loss_stu:6.20308249210666e-09\n",
      "student train: Epo:661  Loss_stu:7.769597409890139e-09\n",
      "student train: Epo:662  Loss_stu:6.1781944005190326e-09\n",
      "student train: Epo:663  Loss_stu:6.177722333688962e-09\n",
      "student train: Epo:664  Loss_stu:4.345921400528141e-09\n",
      "student train: Epo:665  Loss_stu:5.766732869005864e-09\n",
      "student train: Epo:666  Loss_stu:7.3198767047699675e-09\n",
      "student train: Epo:667  Loss_stu:7.815319058579462e-09\n",
      "student train: Epo:668  Loss_stu:1.037985608576264e-08\n",
      "student train: Epo:669  Loss_stu:3.2820859363624777e-09\n",
      "student train: Epo:670  Loss_stu:6.659716333956567e-09\n",
      "student train: Epo:671  Loss_stu:6.125478790863781e-09\n",
      "student train: Epo:672  Loss_stu:4.9129624812849215e-09\n",
      "student train: Epo:673  Loss_stu:6.867181046033011e-09\n",
      "student train: Epo:674  Loss_stu:6.900812810073376e-09\n",
      "student train: Epo:675  Loss_stu:7.0412902175576164e-09\n",
      "student train: Epo:676  Loss_stu:1.1742656624846859e-08\n",
      "student train: Epo:677  Loss_stu:3.657093960995894e-09\n",
      "student train: Epo:678  Loss_stu:6.761021520418353e-09\n",
      "student train: Epo:679  Loss_stu:4.81792161721728e-09\n",
      "student train: Epo:680  Loss_stu:8.294241737871744e-09\n",
      "student train: Epo:681  Loss_stu:6.569570221159893e-09\n",
      "student train: Epo:682  Loss_stu:5.8675491132476054e-09\n",
      "student train: Epo:683  Loss_stu:7.821932435092549e-09\n",
      "student train: Epo:684  Loss_stu:3.7293523824644126e-09\n",
      "student train: Epo:685  Loss_stu:6.037450539508882e-09\n",
      "student train: Epo:686  Loss_stu:7.842198002094847e-09\n",
      "student train: Epo:687  Loss_stu:5.8861848728497534e-09\n",
      "student train: Epo:688  Loss_stu:6.201613000911266e-09\n",
      "student train: Epo:689  Loss_stu:7.172869853633301e-09\n",
      "student train: Epo:690  Loss_stu:7.501375520746478e-09\n",
      "student train: Epo:691  Loss_stu:7.560094772429693e-09\n",
      "student train: Epo:692  Loss_stu:7.226353737621594e-09\n",
      "student train: Epo:693  Loss_stu:7.315731131996017e-09\n",
      "student train: Epo:694  Loss_stu:4.769712624863587e-09\n",
      "student train: Epo:695  Loss_stu:5.58198509637009e-09\n",
      "student train: Epo:696  Loss_stu:4.803285325039042e-09\n",
      "student train: Epo:697  Loss_stu:6.192386603487421e-09\n",
      "student train: Epo:698  Loss_stu:5.579564366087197e-09\n",
      "student train: Epo:699  Loss_stu:7.27573823411376e-09\n",
      "student train: Epo:700  Loss_stu:6.255466811211363e-09\n",
      "student train: Epo:701  Loss_stu:4.99509900109274e-09\n",
      "student train: Epo:702  Loss_stu:6.823600795513585e-09\n",
      "student train: Epo:703  Loss_stu:6.576172051353524e-09\n",
      "student train: Epo:704  Loss_stu:6.052807588474707e-09\n",
      "student train: Epo:705  Loss_stu:9.619768981394827e-09\n",
      "student train: Epo:706  Loss_stu:9.888494467702458e-09\n",
      "student train: Epo:707  Loss_stu:4.285285903904423e-09\n",
      "student train: Epo:708  Loss_stu:5.990433482594426e-09\n",
      "student train: Epo:709  Loss_stu:4.919747276233011e-09\n",
      "student train: Epo:710  Loss_stu:7.024221648777029e-09\n",
      "student train: Epo:711  Loss_stu:6.5019079009687175e-09\n",
      "student train: Epo:712  Loss_stu:4.814947107689704e-09\n",
      "student train: Epo:713  Loss_stu:7.315098304871981e-09\n",
      "student train: Epo:714  Loss_stu:6.207508285172025e-09\n",
      "student train: Epo:715  Loss_stu:5.241165723646191e-09\n",
      "student train: Epo:716  Loss_stu:7.575645000201803e-09\n",
      "student train: Epo:717  Loss_stu:5.5592845882301845e-09\n",
      "student train: Epo:718  Loss_stu:7.980976768351411e-09\n",
      "student train: Epo:719  Loss_stu:8.40422487158321e-09\n",
      "student train: Epo:720  Loss_stu:6.9927428292260174e-09\n",
      "student train: Epo:721  Loss_stu:9.789603794274626e-09\n",
      "student train: Epo:722  Loss_stu:9.979745918542449e-09\n",
      "student train: Epo:723  Loss_stu:8.88884255090261e-09\n",
      "student train: Epo:724  Loss_stu:8.575547383316007e-09\n",
      "student train: Epo:725  Loss_stu:5.431652461140857e-09\n",
      "student train: Epo:726  Loss_stu:5.430879745915718e-09\n",
      "student train: Epo:727  Loss_stu:6.461486901088165e-09\n",
      "student train: Epo:728  Loss_stu:3.671121850956638e-09\n",
      "student train: Epo:729  Loss_stu:5.638917777162078e-09\n",
      "student train: Epo:730  Loss_stu:5.585872209223908e-09\n",
      "student train: Epo:731  Loss_stu:6.8516703422005776e-09\n",
      "student train: Epo:732  Loss_stu:8.406034979202559e-09\n",
      "student train: Epo:733  Loss_stu:6.4278031786102474e-09\n",
      "student train: Epo:734  Loss_stu:5.581202611182334e-09\n",
      "student train: Epo:735  Loss_stu:6.990160450470739e-09\n",
      "student train: Epo:736  Loss_stu:8.490691705276276e-09\n",
      "student train: Epo:737  Loss_stu:4.584268964435978e-09\n",
      "student train: Epo:738  Loss_stu:5.83747317151051e-09\n",
      "student train: Epo:739  Loss_stu:6.39130659507714e-09\n",
      "student train: Epo:740  Loss_stu:5.9244280592452014e-09\n",
      "student train: Epo:741  Loss_stu:6.254524453908061e-09\n",
      "student train: Epo:742  Loss_stu:5.587944329477068e-09\n",
      "student train: Epo:743  Loss_stu:7.556034908873244e-09\n",
      "student train: Epo:744  Loss_stu:5.087678722759392e-09\n",
      "student train: Epo:745  Loss_stu:8.567847764595626e-09\n",
      "student train: Epo:746  Loss_stu:4.001324604985257e-09\n",
      "student train: Epo:747  Loss_stu:5.007071202101088e-09\n",
      "student train: Epo:748  Loss_stu:4.70857219880827e-09\n",
      "student train: Epo:749  Loss_stu:5.2935726913005965e-09\n",
      "student train: Epo:750  Loss_stu:7.140035229724617e-09\n",
      "student train: Epo:751  Loss_stu:4.636032002736101e-09\n",
      "student train: Epo:752  Loss_stu:8.717950805703367e-09\n",
      "student train: Epo:753  Loss_stu:6.7282095450593715e-09\n",
      "student train: Epo:754  Loss_stu:8.057170930442226e-09\n",
      "student train: Epo:755  Loss_stu:8.120527361654695e-09\n",
      "student train: Epo:756  Loss_stu:6.7811520843008566e-09\n",
      "student train: Epo:757  Loss_stu:6.5075620447885285e-09\n",
      "student train: Epo:758  Loss_stu:7.67381003186074e-09\n",
      "student train: Epo:759  Loss_stu:8.907425907977995e-09\n",
      "student train: Epo:760  Loss_stu:6.365132865226997e-09\n",
      "student train: Epo:761  Loss_stu:5.653524759452466e-09\n",
      "student train: Epo:762  Loss_stu:6.966540233577234e-09\n",
      "student train: Epo:763  Loss_stu:7.018439163175572e-09\n",
      "student train: Epo:764  Loss_stu:5.019012316864746e-09\n",
      "student train: Epo:765  Loss_stu:6.32371577324875e-09\n",
      "student train: Epo:766  Loss_stu:7.3042412118695665e-09\n",
      "student train: Epo:767  Loss_stu:5.881205300539705e-09\n",
      "student train: Epo:768  Loss_stu:7.581837380143952e-09\n",
      "student train: Epo:769  Loss_stu:8.660302697194311e-09\n",
      "student train: Epo:770  Loss_stu:4.782155560434376e-09\n",
      "student train: Epo:771  Loss_stu:5.697750715683014e-09\n",
      "student train: Epo:772  Loss_stu:5.3662292387457455e-09\n",
      "student train: Epo:773  Loss_stu:4.378135187721455e-09\n",
      "student train: Epo:774  Loss_stu:6.6717276148153815e-09\n",
      "student train: Epo:775  Loss_stu:4.035070944041763e-09\n",
      "student train: Epo:776  Loss_stu:8.97911700548093e-09\n",
      "student train: Epo:777  Loss_stu:5.477602815773253e-09\n",
      "student train: Epo:778  Loss_stu:5.793749924265512e-09\n",
      "student train: Epo:779  Loss_stu:6.53151799312468e-09\n",
      "student train: Epo:780  Loss_stu:4.384355989373034e-09\n",
      "student train: Epo:781  Loss_stu:1.1053793436133219e-08\n",
      "student train: Epo:782  Loss_stu:5.120898372013016e-09\n",
      "student train: Epo:783  Loss_stu:7.573999205590098e-09\n",
      "student train: Epo:784  Loss_stu:6.1721938671155385e-09\n",
      "student train: Epo:785  Loss_stu:7.594513462549912e-09\n",
      "student train: Epo:786  Loss_stu:5.805379288403856e-09\n",
      "student train: Epo:787  Loss_stu:4.667945585623556e-09\n",
      "student train: Epo:788  Loss_stu:7.649885169769277e-09\n",
      "student train: Epo:789  Loss_stu:5.407390979428328e-09\n",
      "student train: Epo:790  Loss_stu:4.1622865154522515e-09\n",
      "student train: Epo:791  Loss_stu:1.1051957571339699e-08\n",
      "student train: Epo:792  Loss_stu:7.73189068326019e-09\n",
      "student train: Epo:793  Loss_stu:5.3270947653061285e-09\n",
      "student train: Epo:794  Loss_stu:3.4738856236060656e-09\n",
      "student train: Epo:795  Loss_stu:7.641172139472019e-09\n",
      "student train: Epo:796  Loss_stu:5.360167865120502e-09\n",
      "student train: Epo:797  Loss_stu:3.906678980314382e-09\n",
      "student train: Epo:798  Loss_stu:6.220510773147225e-09\n",
      "student train: Epo:799  Loss_stu:5.002305236700977e-09\n",
      "student train: Epo:800  Loss_stu:8.807588436354763e-09\n",
      "student train: Epo:801  Loss_stu:8.393602257683597e-09\n",
      "student train: Epo:802  Loss_stu:5.263431024360443e-09\n",
      "student train: Epo:803  Loss_stu:6.220329140660397e-09\n",
      "student train: Epo:804  Loss_stu:5.499284139176552e-09\n",
      "student train: Epo:805  Loss_stu:7.747811281433314e-09\n",
      "student train: Epo:806  Loss_stu:7.659137324367293e-09\n",
      "student train: Epo:807  Loss_stu:3.890080257917816e-09\n",
      "student train: Epo:808  Loss_stu:5.165595506895215e-09\n",
      "student train: Epo:809  Loss_stu:5.8251030665701364e-09\n",
      "student train: Epo:810  Loss_stu:1.1319216675076404e-08\n",
      "student train: Epo:811  Loss_stu:5.529900981571245e-09\n",
      "student train: Epo:812  Loss_stu:6.001179997383588e-09\n",
      "student train: Epo:813  Loss_stu:9.515198406973013e-09\n",
      "student train: Epo:814  Loss_stu:9.142214096868884e-09\n",
      "student train: Epo:815  Loss_stu:6.4424283685582395e-09\n",
      "student train: Epo:816  Loss_stu:5.810989467391892e-09\n",
      "student train: Epo:817  Loss_stu:8.49509351752431e-09\n",
      "student train: Epo:818  Loss_stu:7.881155283939734e-09\n",
      "student train: Epo:819  Loss_stu:7.571633986458437e-09\n",
      "student train: Epo:820  Loss_stu:9.006893009200212e-09\n",
      "student train: Epo:821  Loss_stu:4.04017441724136e-09\n",
      "student train: Epo:822  Loss_stu:8.23196444343921e-09\n",
      "student train: Epo:823  Loss_stu:5.8929554569431275e-09\n",
      "student train: Epo:824  Loss_stu:9.123573008196217e-09\n",
      "student train: Epo:825  Loss_stu:8.18356671317133e-09\n",
      "student train: Epo:826  Loss_stu:8.751894320369047e-09\n",
      "student train: Epo:827  Loss_stu:4.898019323462677e-09\n",
      "student train: Epo:828  Loss_stu:5.424806381881808e-09\n",
      "student train: Epo:829  Loss_stu:8.285896413440241e-09\n",
      "student train: Epo:830  Loss_stu:1.0404635375493854e-08\n",
      "student train: Epo:831  Loss_stu:5.297871030762735e-09\n",
      "student train: Epo:832  Loss_stu:6.1554596975099685e-09\n",
      "student train: Epo:833  Loss_stu:9.779686394040255e-09\n",
      "student train: Epo:834  Loss_stu:4.218160043478747e-09\n",
      "student train: Epo:835  Loss_stu:5.8746096875950116e-09\n",
      "student train: Epo:836  Loss_stu:4.417787469179757e-09\n",
      "student train: Epo:837  Loss_stu:4.481846893611419e-09\n",
      "student train: Epo:838  Loss_stu:6.227483417831081e-09\n",
      "student train: Epo:839  Loss_stu:7.332638496393429e-09\n",
      "student train: Epo:840  Loss_stu:8.341769053288317e-09\n",
      "student train: Epo:841  Loss_stu:8.260143680161036e-09\n",
      "student train: Epo:842  Loss_stu:7.072351149162159e-09\n",
      "student train: Epo:843  Loss_stu:5.602268426940782e-09\n",
      "student train: Epo:844  Loss_stu:9.655181543166691e-09\n",
      "student train: Epo:845  Loss_stu:9.236990727856664e-09\n",
      "student train: Epo:846  Loss_stu:4.892616978224851e-09\n",
      "student train: Epo:847  Loss_stu:7.119242084741018e-09\n",
      "student train: Epo:848  Loss_stu:4.832337641147433e-09\n",
      "student train: Epo:849  Loss_stu:5.601116015441221e-09\n",
      "student train: Epo:850  Loss_stu:6.531071239379571e-09\n",
      "student train: Epo:851  Loss_stu:1.4386920099695999e-08\n",
      "student train: Epo:852  Loss_stu:5.354124699152862e-09\n",
      "student train: Epo:853  Loss_stu:4.427165301024161e-09\n",
      "student train: Epo:854  Loss_stu:3.7261624896700596e-09\n",
      "student train: Epo:855  Loss_stu:8.426315645237992e-09\n",
      "student train: Epo:856  Loss_stu:7.235658738835582e-09\n",
      "student train: Epo:857  Loss_stu:7.931255652238178e-09\n",
      "student train: Epo:858  Loss_stu:6.213546122069147e-09\n",
      "student train: Epo:859  Loss_stu:4.794399099949942e-09\n",
      "student train: Epo:860  Loss_stu:5.36953415064545e-09\n",
      "student train: Epo:861  Loss_stu:8.194222189672473e-09\n",
      "student train: Epo:862  Loss_stu:7.851367556099831e-09\n",
      "student train: Epo:863  Loss_stu:5.4107753832965955e-09\n",
      "student train: Epo:864  Loss_stu:6.66489041734053e-09\n",
      "student train: Epo:865  Loss_stu:5.602381225600084e-09\n",
      "student train: Epo:866  Loss_stu:4.422711086249365e-09\n",
      "student train: Epo:867  Loss_stu:3.8275551617061865e-09\n",
      "student train: Epo:868  Loss_stu:6.974909982915278e-09\n",
      "student train: Epo:869  Loss_stu:6.910223948608518e-09\n",
      "student train: Epo:870  Loss_stu:6.443673150613449e-09\n",
      "student train: Epo:871  Loss_stu:4.896392180597786e-09\n",
      "student train: Epo:872  Loss_stu:6.001096952701346e-09\n",
      "student train: Epo:873  Loss_stu:3.66095664894317e-09\n",
      "student train: Epo:874  Loss_stu:7.783926392335161e-09\n",
      "student train: Epo:875  Loss_stu:5.722066376279145e-09\n",
      "student train: Epo:876  Loss_stu:6.139145192207707e-09\n",
      "student train: Epo:877  Loss_stu:4.2677426037585064e-09\n",
      "student train: Epo:878  Loss_stu:6.098597182813137e-09\n",
      "student train: Epo:879  Loss_stu:3.2941227523508587e-09\n",
      "student train: Epo:880  Loss_stu:8.095745407388222e-09\n",
      "student train: Epo:881  Loss_stu:4.33395941357162e-09\n",
      "student train: Epo:882  Loss_stu:4.158949629129438e-09\n",
      "student train: Epo:883  Loss_stu:9.771883746623189e-09\n",
      "student train: Epo:884  Loss_stu:6.318795264803612e-09\n",
      "student train: Epo:885  Loss_stu:5.084074494732249e-09\n",
      "student train: Epo:886  Loss_stu:3.9943004459530584e-09\n",
      "student train: Epo:887  Loss_stu:4.42032677128168e-09\n",
      "student train: Epo:888  Loss_stu:7.222181519495052e-09\n",
      "student train: Epo:889  Loss_stu:6.985226175260095e-09\n",
      "student train: Epo:890  Loss_stu:5.0450061905849e-09\n",
      "student train: Epo:891  Loss_stu:6.087306214652699e-09\n",
      "student train: Epo:892  Loss_stu:6.861351486975309e-09\n",
      "student train: Epo:893  Loss_stu:7.367913390510239e-09\n",
      "student train: Epo:894  Loss_stu:4.579095325141225e-09\n",
      "student train: Epo:895  Loss_stu:8.403056916961305e-09\n",
      "student train: Epo:896  Loss_stu:8.320058419997167e-09\n",
      "student train: Epo:897  Loss_stu:6.612486558310593e-09\n",
      "student train: Epo:898  Loss_stu:6.47482822913048e-09\n",
      "student train: Epo:899  Loss_stu:7.041560667886415e-09\n",
      "student train: Epo:900  Loss_stu:5.298262273356613e-09\n",
      "student train: Epo:901  Loss_stu:6.362936844084288e-09\n",
      "student train: Epo:902  Loss_stu:3.635926004719181e-09\n",
      "student train: Epo:903  Loss_stu:4.752226612225741e-09\n",
      "student train: Epo:904  Loss_stu:6.688066100934975e-09\n",
      "student train: Epo:905  Loss_stu:3.734694331569699e-09\n",
      "student train: Epo:906  Loss_stu:5.473870245964463e-09\n",
      "student train: Epo:907  Loss_stu:6.501956750781801e-09\n",
      "student train: Epo:908  Loss_stu:9.506060827391138e-09\n",
      "student train: Epo:909  Loss_stu:4.756497418156869e-09\n",
      "student train: Epo:910  Loss_stu:1.0045742904196686e-08\n",
      "student train: Epo:911  Loss_stu:7.064078655361072e-09\n",
      "student train: Epo:912  Loss_stu:5.639873457141675e-09\n",
      "student train: Epo:913  Loss_stu:5.1671706913225535e-09\n",
      "student train: Epo:914  Loss_stu:1.076261479937557e-08\n",
      "student train: Epo:915  Loss_stu:4.8648090000824595e-09\n",
      "student train: Epo:916  Loss_stu:4.8893213921985534e-09\n",
      "student train: Epo:917  Loss_stu:5.155087912100953e-09\n",
      "student train: Epo:918  Loss_stu:7.12255276980045e-09\n",
      "student train: Epo:919  Loss_stu:8.268479234629922e-09\n",
      "student train: Epo:920  Loss_stu:6.0878293517419024e-09\n",
      "student train: Epo:921  Loss_stu:6.028479493380701e-09\n",
      "student train: Epo:922  Loss_stu:3.398472614435377e-09\n",
      "student train: Epo:923  Loss_stu:6.991305312453733e-09\n",
      "student train: Epo:924  Loss_stu:1.1577806269258417e-08\n",
      "student train: Epo:925  Loss_stu:6.815656039549367e-09\n",
      "student train: Epo:926  Loss_stu:5.981343420558005e-09\n",
      "student train: Epo:927  Loss_stu:7.3571486680634735e-09\n",
      "student train: Epo:928  Loss_stu:4.604264525198687e-09\n",
      "student train: Epo:929  Loss_stu:5.167433148045575e-09\n",
      "student train: Epo:930  Loss_stu:7.501936849507729e-09\n",
      "student train: Epo:931  Loss_stu:5.053641505270434e-09\n",
      "student train: Epo:932  Loss_stu:5.891164889249012e-09\n",
      "student train: Epo:933  Loss_stu:4.999666902705258e-09\n",
      "student train: Epo:934  Loss_stu:4.570899658773442e-09\n",
      "student train: Epo:935  Loss_stu:9.609297357826563e-09\n",
      "student train: Epo:936  Loss_stu:6.580584521742594e-09\n",
      "student train: Epo:937  Loss_stu:8.013128827144556e-09\n",
      "student train: Epo:938  Loss_stu:6.612826286556128e-09\n",
      "student train: Epo:939  Loss_stu:4.886058224684575e-09\n",
      "student train: Epo:940  Loss_stu:6.473400926410022e-09\n",
      "student train: Epo:941  Loss_stu:9.564742775580726e-09\n",
      "student train: Epo:942  Loss_stu:7.200368301596427e-09\n",
      "student train: Epo:943  Loss_stu:7.100182219943463e-09\n",
      "student train: Epo:944  Loss_stu:5.1155253366630404e-09\n",
      "student train: Epo:945  Loss_stu:4.1239127668291076e-09\n",
      "student train: Epo:946  Loss_stu:5.474711350927919e-09\n",
      "student train: Epo:947  Loss_stu:5.993518126246045e-09\n",
      "student train: Epo:948  Loss_stu:6.67427801914755e-09\n",
      "student train: Epo:949  Loss_stu:7.0321970468967265e-09\n",
      "student train: Epo:950  Loss_stu:9.475713547146825e-09\n",
      "student train: Epo:951  Loss_stu:4.697745747961335e-09\n",
      "student train: Epo:952  Loss_stu:1.0926557436619078e-08\n",
      "student train: Epo:953  Loss_stu:7.873461882468291e-09\n",
      "student train: Epo:954  Loss_stu:3.368508583179164e-09\n",
      "student train: Epo:955  Loss_stu:4.653789353881166e-09\n",
      "student train: Epo:956  Loss_stu:9.373003706514282e-09\n",
      "student train: Epo:957  Loss_stu:6.662553619918299e-09\n",
      "student train: Epo:958  Loss_stu:6.578749989216703e-09\n",
      "student train: Epo:959  Loss_stu:5.302524641592754e-09\n",
      "student train: Epo:960  Loss_stu:7.090879439175524e-09\n",
      "student train: Epo:961  Loss_stu:4.928062846687453e-09\n",
      "student train: Epo:962  Loss_stu:5.563227656324443e-09\n",
      "student train: Epo:963  Loss_stu:7.292970227723572e-09\n",
      "student train: Epo:964  Loss_stu:4.290694022301977e-09\n",
      "student train: Epo:965  Loss_stu:5.794335677933304e-09\n",
      "student train: Epo:966  Loss_stu:5.961507731910842e-09\n",
      "student train: Epo:967  Loss_stu:6.456905232710142e-09\n",
      "student train: Epo:968  Loss_stu:6.756802672924778e-09\n",
      "student train: Epo:969  Loss_stu:8.731029232933452e-09\n",
      "student train: Epo:970  Loss_stu:6.118305861946283e-09\n",
      "student train: Epo:971  Loss_stu:4.488646343503433e-09\n",
      "student train: Epo:972  Loss_stu:3.409947435528693e-09\n",
      "student train: Epo:973  Loss_stu:4.461886859985498e-09\n",
      "student train: Epo:974  Loss_stu:5.947028203223681e-09\n",
      "student train: Epo:975  Loss_stu:7.089720810427025e-09\n",
      "student train: Epo:976  Loss_stu:5.683712611670444e-09\n",
      "student train: Epo:977  Loss_stu:5.995950846937603e-09\n",
      "student train: Epo:978  Loss_stu:1.010230032960635e-08\n",
      "student train: Epo:979  Loss_stu:6.920062745052746e-09\n",
      "student train: Epo:980  Loss_stu:5.933178837125297e-09\n",
      "student train: Epo:981  Loss_stu:5.917080159179022e-09\n",
      "student train: Epo:982  Loss_stu:6.524904172522383e-09\n",
      "student train: Epo:983  Loss_stu:6.803067442717747e-09\n",
      "student train: Epo:984  Loss_stu:3.6235268119355624e-09\n",
      "student train: Epo:985  Loss_stu:9.995669181250832e-09\n",
      "student train: Epo:986  Loss_stu:6.208220604264625e-09\n",
      "student train: Epo:987  Loss_stu:4.339861803259737e-09\n",
      "student train: Epo:988  Loss_stu:6.724605317032228e-09\n",
      "student train: Epo:989  Loss_stu:8.162997389149496e-09\n",
      "student train: Epo:990  Loss_stu:4.874711745372906e-09\n",
      "student train: Epo:991  Loss_stu:8.820753905069978e-09\n",
      "student train: Epo:992  Loss_stu:6.055765222612308e-09\n",
      "student train: Epo:993  Loss_stu:5.828169502564151e-09\n",
      "student train: Epo:994  Loss_stu:6.468324542652226e-09\n",
      "student train: Epo:995  Loss_stu:6.601998059352354e-09\n",
      "student train: Epo:996  Loss_stu:6.231858584726524e-09\n",
      "student train: Epo:997  Loss_stu:7.1683019520207836e-09\n",
      "student train: Epo:998  Loss_stu:4.332771919024481e-09\n",
      "student train: Epo:999  Loss_stu:6.888470682753223e-09\n"
     ]
    }
   ],
   "source": [
    "#train student model\n",
    "loss_func=nn.MSELoss()\n",
    "print('start training')\n",
    "train_loss_all_stu=[]\n",
    "minloss=float('inf')\n",
    "for epoch in  range(1000):\n",
    "    train_loss = 0\n",
    "    train_num=0\n",
    "    student_model.train()\n",
    "    for b_x,b_y in  student_train_loader1:\n",
    "        student_optimizer.zero_grad()\n",
    "        output=student_model(b_x)\n",
    "        loss=loss_func(output,b_y)\n",
    "        loss.backward()\n",
    "        student_optimizer.step()\n",
    "        # train_loss+=loss.item() * b_x.size(0)\n",
    "        # train_num += b_x.size(0)\n",
    "        # loss_stu = train_loss/train_num\n",
    "        if loss < minloss:\n",
    "            minloss = loss\n",
    "            if os.path.exists(f'/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_44_gauss/stu/best2.pth'):\n",
    "                os.remove(f'/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_44_gauss/stu/best2.pth')\n",
    "            torch.save(student_model, f'/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_44_gauss/stu/best2.pth')\n",
    "    if epoch%1 == 0:\n",
    "        print(f'student train: Epo:{epoch}  Loss_stu:{loss}')\n",
    "    train_loss_all_stu.append(loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 保存\n",
    "# import numpy as np\n",
    "# d=np.array(train_loss_all_stu)\n",
    "# d1=np.array(train_loss_all_stu)\n",
    "# # d2=np.array(train_loss_all_stu)\n",
    "\n",
    "# # np.save('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.3).npy',d)   # 保存为.npy格式\n",
    "# # np.save('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.3)_1.npy',d1)   # 保存为.npy格式\n",
    "# # np.save('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.3)_2.npy',d2)   # 保存为.npy格式\n",
    "\n",
    "\n",
    "\n",
    "# # d=np.load('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.3).npy')\n",
    "# # d1=np.load('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.3)_1.npy')\n",
    "# # d2=np.load('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.3)_2.npy')\n",
    "\n",
    "# ###\n",
    "\n",
    "# # np.save('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.5).npy',d)   # 保存为.npy格式\n",
    "# d=np.load('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.5).npy')\n",
    "\n",
    "# np.save('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.5)_1.npy',d1)   # 保存为.npy格式\n",
    "# d1=np.load('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.5)_1.npy')\n",
    "\n",
    "# # d=(d+d1+d2)/3\n",
    "# d=(d+d1)/2\n",
    "\n",
    "# d\n",
    "# d=d.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # # 保存\n",
    "            # import numpy as np\n",
    "            # d=np.array(train_loss_all_stu)\n",
    "\n",
    "            # np.save('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.05).npy',d)   # 保存为.npy格式\n",
    "            # # np.save('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.1).npy',d)   # 保存为.npy格式\n",
    "            # # np.save('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.15).npy',d)   # 保存为.npy格式\n",
    "            # # np.save('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.2).npy',d)   # 保存为.npy格式\n",
    "            # # np.save('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.3).npy',d)   # 保存为.npy格式\n",
    "            # # np.save('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.5).npy',d)   # 保存为.npy格式\n",
    "            # # np.save('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,1).npy',d)   # 保存为.npy格式\n",
    "            # # 读取\n",
    "\n",
    "\n",
    "            # d=np.load('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.05).npy')\n",
    "            # # d=np.load('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.1).npy')\n",
    "            # # d=np.load('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.15).npy')\n",
    "            # # d=np.load('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.2).npy')\n",
    "            # # d=np.load('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.3).npy')\n",
    "            # # d=np.load('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,0.5).npy')\n",
    "            # # d=np.load('/home/ysy/ysy/Fed-ReKD/44_loss_gauss(0,1).npy')\n",
    "            # d=d.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "                # #读取\n",
    "                # c=np.load('/home/ysy/ysy/Fed-ReKD/44_loss_average.npy')\n",
    "                # # c=np.load('/home/ysy/ysy/Fed-ReKD/44_loss.npy')\n",
    "                # c=c.tolist()\n",
    "                # #svg\n",
    "                # import matplotlib.pyplot as plt\n",
    "                # from matplotlib.pyplot import figure\n",
    "                # import numpy as np\n",
    "                # import seaborn as sns\n",
    "                # from  mpl_toolkits.axisartist import axis_artist\n",
    "\n",
    "                # sns.set_style(\"ticks\")\n",
    "                # figure(num=None, figsize=(6, 4), dpi=600)\n",
    "                # # figsize的2.8和1.7指的是英寸，dpi指定图片分辨率。那么图片就是（2.8*300）*（1.7*300）像素大小\n",
    "                # plt.plot(c, 'royalblue', label='Without LDP')\n",
    "\n",
    "                # plt.plot(d, 'darkorange', label='With (0,0.05)gauss')\n",
    "                # # plt.plot(d, 'darkorange', label='With (0,0.1)gauss')\n",
    "                # # plt.plot(d, 'darkorange', label='With (0,0.15)gauss')\n",
    "                # # plt.plot(d, 'darkorange', label='With (0,0.2)gauss')\n",
    "                # # plt.plot(d, 'darkorange', label='With (0,0.3)gauss')\n",
    "                # # plt.plot(d, 'darkorange', label='With (0,0.5)gauss')\n",
    "                # # plt.plot(d, 'darkorange', label='With (0,1)gauss')\n",
    "                # # 画图，并指定颜色\n",
    "\n",
    "                # plt.xticks(fontproperties = 'Times New Roman', fontsize=8)\n",
    "                # plt.yticks(np.arange(0, 0.031, 0.005), fontproperties = 'Times New Roman', fontsize=8)\n",
    "                # # 指定横纵坐标的字体以及字体大小，记住是fontsize不是size。yticks上我还用numpy指定了坐标轴的变化范围。\n",
    "\n",
    "                # plt.legend(loc='upper right', prop={'family':'Times New Roman', 'size':12})\n",
    "                # # 图上的legend，记住字体是要用prop以字典形式设置的，而且字的大小是size不是fontsize，这个容易和xticks的命令弄混\n",
    "\n",
    "                # plt.title('Zillow Train Performance', fontdict={'family' : 'Times New Roman', 'size':12})\n",
    "                # # 指定图上标题的字体及大小\n",
    "\n",
    "                # plt.xlabel('Epochs', fontdict={'family' : 'Times New Roman', 'size':12})\n",
    "                # plt.ylabel('Loss', fontdict={'family' : 'Times New Roman', 'size':12})\n",
    "                # # 指定横纵坐标描述的字体及大小\n",
    "                # plt.xlim(-1,100)\n",
    "                # plt.ylim(0.00,0.03)\n",
    "\n",
    "                # plt.savefig('/home/ysy/ysy/Fed-ReKD/loss_44_(0,0.05).pdf', dpi=600, bbox_inches=\"tight\")\n",
    "                # # plt.savefig('/home/ysy/ysy/Fed-ReKD/loss_44_(0,0.1).pdf', dpi=600, bbox_inches=\"tight\")\n",
    "                # # plt.savefig('/home/ysy/ysy/Fed-ReKD/loss_44_(0,0.15).pdf', dpi=600, bbox_inches=\"tight\")\n",
    "                # # plt.savefig('/home/ysy/ysy/Fed-ReKD/loss_44_(0,0.2).pdf', dpi=600, bbox_inches=\"tight\")\n",
    "                # # plt.savefig('/home/ysy/ysy/Fed-ReKD/loss_44_(0,0.3).pdf', dpi=600, bbox_inches=\"tight\")\n",
    "                # # plt.savefig('/home/ysy/ysy/Fed-ReKD/loss_44_(0,0.5).pdf', dpi=600, bbox_inches=\"tight\")\n",
    "                # # plt.savefig('/home/ysy/ysy/Fed-ReKD/loss_44_(0,1).pdf', dpi=600, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "                # # 保存文件，dpi指定保存文件的分辨率\n",
    "                # # bbox_inches=\"tight\" 可以保存图上所有的信息，不会出现横纵坐标轴的描述存掉了的情况\n",
    "\n",
    "\n",
    "                # plt.show()\n",
    "                # # 记住，如果你要show()的话，一定要先savefig，再show。如果你先show了，存出来的就是一张白纸。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student test Loss:0.00695079006254673 \n",
      " predict:tensor([[0.0258],\n",
      "        [0.0259],\n",
      "        [0.0259],\n",
      "        ...,\n",
      "        [0.0258],\n",
      "        [0.0259],\n",
      "        [0.0259]], device='cuda:2', grad_fn=<TanhBackward0>) \n",
      " b_y:tensor([ 0.0328,  0.1337, -0.0440,  ...,  0.0711, -0.0356,  0.0421],\n",
      "       device='cuda:2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangyipeng/anaconda3/envs/ysy/lib/python3.9/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([8227])) that is different to the input size (torch.Size([8227, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "#eval  student model\n",
    "# teacher_best_models =[torch.load(f'/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_44_gauss/teacher{tea_num}/best.pth')  for  tea_num in  range(n_teachers)]\n",
    "student_best_model =torch.load('/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_44_gauss/stu/best2.pth')\n",
    "\n",
    "for s_x,s_y in  student_test_loader:\n",
    "    student_best_model.eval().to(device)\n",
    "    pred=student_best_model(s_x)\n",
    "    loss_test=loss_func(pred,s_y) \n",
    "print(f'student test Loss:{loss_test}',f'\\n predict:{pred}',f'\\n b_y:{s_y}')\n",
    "\n",
    "# loss_all=0\n",
    "# for i in range(samples):\n",
    "#     for s_x,s_y in student_test_loader:\n",
    "#         student_best_model.eval().to(device)\n",
    "#         pred[i]=student_best_model(s_x[i])\n",
    "#         loss_test=loss_func(pred[i],s_y[i])\n",
    "#         loss_all+=loss_test\n",
    "#         print(f'stu loss',f'\\n now menber{i} loss:{loss_test}')\n",
    "#     # print(b_x,b_y)\n",
    "# ip = loss_all/samples\n",
    "# print(f'average:{ip}')\n",
    "# array = y.cpu().numpy()\n",
    "# print(len(s_x))\n",
    "#找到置信区间 若在student_test_loader中的x的置信度符合该置信区间的 我就加入到新的测试集中，并加入对应标签。训练取loss\n",
    "# 给各个地方打上标签，对不熟的标签 进行剔除投票    关于隐私保护机制/// 拉普拉斯 针对数值型数据 加入独立同分布的高斯噪声  根据噪声与隐私预算 做调整 //指数机制  针对非数值型数据  设置质量函数 选择输出\n",
    "# 0.025 //0.16//0.037 //0.258"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('bnn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "be5910e786292a3dd74c1ecc18ef09548ddc10597f54e3d847c6ad036c3e5961"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
