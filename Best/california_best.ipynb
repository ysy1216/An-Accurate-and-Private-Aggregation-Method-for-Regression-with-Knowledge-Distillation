{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14448, 8), (14448,), (6192, 8), (6192,), 0.14999, 5.00001)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "housedata=fetch_california_housing()\n",
    "train_x,test_x,train_y,test_y=train_test_split(housedata.data,housedata.target,test_size=0.3,random_state=42)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale=StandardScaler()\n",
    "train_x=scale.fit_transform(train_x)\n",
    "test_x=scale.fit_transform(test_x)\n",
    "train_x.shape,train_y.shape,test_x.shape,test_y.shape,min(test_y),max(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (1445, 8) (1445,)\n",
      "1 (1445, 8) (1445,)\n",
      "2 (1445, 8) (1445,)\n",
      "3 (1445, 8) (1445,)\n",
      "4 (1445, 8) (1445,)\n",
      "5 (1445, 8) (1445,)\n",
      "6 (1445, 8) (1445,)\n",
      "7 (1445, 8) (1445,)\n",
      "8 (1445, 8) (1445,)\n",
      "9 (1443, 8) (1443,)\n",
      "preparing students dataset\n",
      "0 (3096, 8) (3096,)\n",
      "1 (3096, 8) (3096,)\n",
      "3096\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "def split_list_n_list(origin_list, n):\n",
    "    if len(origin_list) % n == 0:\n",
    "        cnt = len(origin_list) // n\n",
    "    else:\n",
    "        cnt = len(origin_list) // n + 1\n",
    " \n",
    "    for i in range(0, n):\n",
    "        yield origin_list[i*cnt:(i+1)*cnt]\n",
    "             \n",
    "#preparing teacher's datasets\n",
    "n_teachers=10\n",
    "teacher_x,teacher_y = [],[]\n",
    "teacher_datasets = []\n",
    "teacher_data_loader = []\n",
    "\n",
    "\n",
    "\n",
    "teacher_x_loder = split_list_n_list(train_x,n_teachers)\n",
    "teacher_y_loder = split_list_n_list(train_y,n_teachers)\n",
    "\n",
    "teacher_x.extend(iter(teacher_x_loder))\n",
    "teacher_y.extend(iter(teacher_y_loder))\n",
    "\n",
    "for i in range(n_teachers):\n",
    "    print(i,teacher_x[i].shape,teacher_y[i].shape)\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "teacher_datasets.extend(TensorDataset(torch.tensor(teacher_x[u],device=device,dtype=torch.float),\n",
    "                                       torch.tensor(teacher_y[u],device=device,dtype=torch.float))\n",
    "                        for u in  range(n_teachers))\n",
    "\n",
    "teacher_data_loader.extend(DataLoader(teacher_datasets[i],batch_size=32,shuffle=True)\n",
    "                           for i in range(n_teachers))\n",
    "# print(teacher_x)\n",
    "\n",
    "\n",
    "print ('preparing students dataset')\n",
    "student_x,student_y = [] ,[]\n",
    "student_datasets= []\n",
    " \n",
    "\n",
    "student_x_loder =  split_list_n_list(test_x,2)\n",
    "student_y_loder =  split_list_n_list(test_y,2)\n",
    "student_x.extend(iter(student_x_loder))\n",
    "student_y.extend(iter(student_y_loder))\n",
    "\n",
    "for i in range(2):\n",
    "    print(i,student_x[i].shape,student_y[i].shape)\n",
    "\n",
    "student_datasets.extend(\n",
    "                        TensorDataset(torch.tensor(student_x[u],device=device,dtype=torch.float),\n",
    "                                      torch.tensor(student_y[u],device=device,dtype=torch.float))\n",
    "                        for u in range(2)\n",
    ")\n",
    "# model_valid_loader = DataLoader(student_datasets[0], batch_size=len(student_datasets[0]))\n",
    "student_train_loader = DataLoader(student_datasets[0], batch_size=len(student_datasets[0]),shuffle=True)\n",
    "student_test_loader = DataLoader(student_datasets[1], batch_size=len(student_datasets[1]))\n",
    "samples= student_x[i].shape[0]\n",
    "print(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_BBB(\n",
      "  (hidden): Linear_BBB()\n",
      "  (out): Linear_BBB()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Normal\n",
    "from pyro.distributions import Categorical\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI\n",
    "from pyro.infer import Trace_ELBO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "\n",
    "class Linear_BBB(nn.Module):\n",
    "    \"\"\"\n",
    "        Layer of our BNN.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_features, output_features, prior_var=1.):\n",
    "        \"\"\"\n",
    "            Initialization of our layer : our prior is a normal distribution\n",
    "            centered in 0 and of variance 20.\n",
    "        \"\"\"\n",
    "        # initialize layers\n",
    "        super().__init__()\n",
    "        # set input and output dimensions\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        # initialize mu and rho parameters for the weights of the layer\n",
    "        self.w_mu = nn.Parameter(torch.zeros(output_features, input_features))\n",
    "        self.w_rho = nn.Parameter(torch.zeros(output_features, input_features))\n",
    "\n",
    "        #initialize mu and rho parameters for the layer's bias\n",
    "        self.b_mu =  nn.Parameter(torch.zeros(output_features))\n",
    "        self.b_rho = nn.Parameter(torch.zeros(output_features))        \n",
    "\n",
    "        #initialize weight samples (these will be calculated whenever the layer makes a prediction)\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "        # initialize prior distribution for all of the weights and biases\n",
    "        self.prior = torch.distributions.Normal(0,prior_var)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "          Optimization process\n",
    "        \"\"\"\n",
    "        # sample weights\n",
    "        w_epsilon = Normal(0,1).sample(self.w_mu.shape).to(device)\n",
    "        self.w = self.w_mu + torch.log(1+torch.exp(self.w_rho)) * w_epsilon\n",
    "\n",
    "        # sample bias\n",
    "        b_epsilon = Normal(0,1).sample(self.b_mu.shape).to(device)\n",
    "        self.b = self.b_mu + torch.log(1+torch.exp(self.b_rho)) * b_epsilon\n",
    "\n",
    "        # record log prior by evaluating log pdf of prior at sampled weight and bias\n",
    "        w_log_prior = self.prior.log_prob(self.w)\n",
    "        b_log_prior = self.prior.log_prob(self.b)\n",
    "        self.log_prior = torch.sum(w_log_prior) + torch.sum(b_log_prior)\n",
    "\n",
    "        # record log variational posterior by evaluating log pdf of normal distribution defined by parameters with respect at the sampled values\n",
    "        self.w_post = Normal(self.w_mu.data, torch.log(1+torch.exp(self.w_rho)).to(device))\n",
    "        self.b_post = Normal(self.b_mu.data, torch.log(1+torch.exp(self.b_rho)).to(device))\n",
    "        self.log_post = self.w_post.log_prob(self.w).sum() + self.b_post.log_prob(self.b).sum()\n",
    "\n",
    "        return F.linear(input, self.w, self.b)\n",
    "\n",
    "class MLP_BBB(nn.Module):\n",
    "    def __init__(self, hidden_units, noise_tol=.1,  prior_var=1.):\n",
    "\n",
    "        # initialize the network like you would with a standard multilayer perceptron, but using the BBB layer\n",
    "        super().__init__()\n",
    "        self.hidden = Linear_BBB(8,hidden_units, prior_var=prior_var)\n",
    "        self.out = Linear_BBB(hidden_units, 1, prior_var=prior_var)\n",
    "        self.noise_tol = noise_tol # we will use the noise tolerance to calculate our likelihood\n",
    "\n",
    "    def forward(self, x):\n",
    "        # again, this is equivalent to a standard multilayer perceptron\n",
    "        x = torch.sigmoid(self.hidden(x)).to(device)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "    def log_prior(self):\n",
    "        # calculate the log prior over all the layers\n",
    "        return self.hidden.log_prior + self.out.log_prior\n",
    "\n",
    "    def log_post(self):\n",
    "        # calculate the log posterior over all the layers\n",
    "        return self.hidden.log_post + self.out.log_post\n",
    "\n",
    "    def sample_elbo(self, input, target, samples):\n",
    "        # we calculate the negative elbo, which will be our loss function\n",
    "        #initialize tensors\n",
    "        outputs = torch.zeros(samples, target.shape[0]).to(device)\n",
    "        log_priors = torch.zeros(samples).to(device)\n",
    "        log_posts = torch.zeros(samples).to(device)\n",
    "        log_likes = torch.zeros(samples).to(device)\n",
    "        # make predictions and calculate prior, posterior, and likelihood for a given number of samples\n",
    "        for i in range(samples):\n",
    "            outputs[i] = self(input).reshape(-1).to(device) # make predictions\n",
    "            log_priors[i] = self.log_prior() # get log prior\n",
    "            log_posts[i] = self.log_post() # get log variational posterior\n",
    "            log_likes[i] = Normal(outputs[i], self.noise_tol).log_prob(target.reshape(-1)).sum() # calculate the log likelihood\n",
    "        # calculate monte carlo estimate of prior posterior and likelihood\n",
    "        log_prior = log_priors.mean()\n",
    "        log_post = log_posts.mean()\n",
    "        log_like = log_likes.mean()\n",
    "        # calculate the negative elbo (which is our loss function)\n",
    "        loss = log_post - log_prior - log_like\n",
    "        return loss\n",
    "\n",
    "BNN = MLP_BBB(40, prior_var=1).to(device)\n",
    "print(BNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "#training configs\n",
    "num_epochs=1200\n",
    "batch_size =32\n",
    "lr =1e-3\n",
    "#initializing  teachers model\n",
    "teachers_model = []\n",
    "\n",
    "for i in range(n_teachers):\n",
    "    net = BNN\n",
    "    teachers_model.append(net)\n",
    " \n",
    "teacher_optimizers = [torch.optim.Adam(teachers_model[i].parameters(), lr=lr) for i in range(n_teachers)]\n",
    "\n",
    "#creating teachers folders\n",
    "import os\n",
    "def  mkdir_if_missing(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "for i in  range(n_teachers):\n",
    "    mkdir_if_missing(f'/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_8_gauss/teacher{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training number 0 techer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ysy/anaconda3/envs/bnn/lib/python3.9/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/ysy/anaconda3/envs/bnn/lib/python3.9/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number:0 epoch: 1/1200 loss_tea:10.697375101861656  loss_houyan:11414.044921875\n",
      "Number:0 epoch: 11/1200 loss_tea:4.508857118217178  loss_houyan:2944.045166015625\n",
      "Number:0 epoch: 21/1200 loss_tea:6.486080382522002  loss_houyan:1905.168212890625\n",
      "Number:0 epoch: 31/1200 loss_tea:6.893526911653037  loss_houyan:385.7079772949219\n",
      "Number:0 epoch: 41/1200 loss_tea:4.539849673381727  loss_houyan:604.1119995117188\n",
      "Number:0 epoch: 51/1200 loss_tea:3.622980091233567  loss_houyan:922.5023803710938\n",
      "Number:0 epoch: 61/1200 loss_tea:3.7508307598041415  loss_houyan:119.32247924804688\n",
      "Number:0 epoch: 71/1200 loss_tea:3.2685846960255844  loss_houyan:369.863525390625\n",
      "Number:0 epoch: 81/1200 loss_tea:3.168438045871299  loss_houyan:272.14971923828125\n",
      "Number:0 epoch: 91/1200 loss_tea:2.932537368589619  loss_houyan:2759.9736328125\n",
      "Number:0 epoch: 101/1200 loss_tea:2.8283105368432704  loss_houyan:485.7212829589844\n",
      "Number:0 epoch: 111/1200 loss_tea:2.6928471519047825  loss_houyan:838.5130004882812\n",
      "Number:0 epoch: 121/1200 loss_tea:2.770944014717551  loss_houyan:519.0213623046875\n",
      "Number:0 epoch: 131/1200 loss_tea:2.603127153192012  loss_houyan:199.71234130859375\n",
      "Number:0 epoch: 141/1200 loss_tea:2.4259880847996906  loss_houyan:267.06597900390625\n",
      "Number:0 epoch: 151/1200 loss_tea:2.4939635664533992  loss_houyan:732.8457641601562\n",
      "Number:0 epoch: 161/1200 loss_tea:2.3750843858223885  loss_houyan:215.4278106689453\n",
      "Number:0 epoch: 171/1200 loss_tea:2.400450753670663  loss_houyan:243.57138061523438\n",
      "Number:0 epoch: 181/1200 loss_tea:2.3839409189653233  loss_houyan:414.21673583984375\n",
      "Number:0 epoch: 191/1200 loss_tea:2.240657691889568  loss_houyan:398.2828369140625\n",
      "Number:0 epoch: 201/1200 loss_tea:2.2681301289363716  loss_houyan:643.8760986328125\n",
      "Number:0 epoch: 211/1200 loss_tea:2.386017456846666  loss_houyan:330.02093505859375\n",
      "Number:0 epoch: 221/1200 loss_tea:2.300191062565081  loss_houyan:282.1318664550781\n",
      "Number:0 epoch: 231/1200 loss_tea:2.2093932996571684  loss_houyan:294.2683410644531\n",
      "Number:0 epoch: 241/1200 loss_tea:2.2943237100917986  loss_houyan:338.13818359375\n",
      "Number:0 epoch: 251/1200 loss_tea:2.219743642015028  loss_houyan:260.28009033203125\n",
      "Number:0 epoch: 261/1200 loss_tea:2.290297516572022  loss_houyan:445.66693115234375\n",
      "Number:0 epoch: 271/1200 loss_tea:2.309811498714566  loss_houyan:623.411865234375\n",
      "Number:0 epoch: 281/1200 loss_tea:2.272543630302984  loss_houyan:598.42138671875\n",
      "Number:0 epoch: 291/1200 loss_tea:2.317946219258655  loss_houyan:331.3271484375\n",
      "Number:0 epoch: 301/1200 loss_tea:2.2748540958318744  loss_houyan:367.92144775390625\n",
      "Number:0 epoch: 311/1200 loss_tea:2.270639968660876  loss_houyan:339.473388671875\n",
      "Number:0 epoch: 321/1200 loss_tea:2.296501741442301  loss_houyan:323.55401611328125\n",
      "Number:0 epoch: 331/1200 loss_tea:2.3388934145336746  loss_houyan:291.3072814941406\n",
      "Number:0 epoch: 341/1200 loss_tea:2.216468735962178  loss_houyan:374.4771423339844\n",
      "Number:0 epoch: 351/1200 loss_tea:2.223722814771131  loss_houyan:321.185546875\n",
      "Number:0 epoch: 361/1200 loss_tea:2.2773154088782603  loss_houyan:307.51019287109375\n",
      "Number:0 epoch: 371/1200 loss_tea:2.2709260610560644  loss_houyan:340.01129150390625\n",
      "Number:0 epoch: 381/1200 loss_tea:2.2931906125124764  loss_houyan:390.2495422363281\n",
      "Number:0 epoch: 391/1200 loss_tea:2.222422070239242  loss_houyan:290.77630615234375\n",
      "Number:0 epoch: 401/1200 loss_tea:2.2547313394018524  loss_houyan:347.18353271484375\n",
      "Number:0 epoch: 411/1200 loss_tea:2.26288585283352  loss_houyan:400.5744934082031\n",
      "Number:0 epoch: 421/1200 loss_tea:2.258376414635602  loss_houyan:311.3592834472656\n",
      "Number:0 epoch: 431/1200 loss_tea:2.2465183280331043  loss_houyan:306.3799133300781\n",
      "Number:0 epoch: 441/1200 loss_tea:2.276507489903988  loss_houyan:330.06292724609375\n",
      "Number:0 epoch: 451/1200 loss_tea:2.2492796752692095  loss_houyan:322.76116943359375\n",
      "Number:0 epoch: 461/1200 loss_tea:2.256857885807031  loss_houyan:254.07427978515625\n",
      "Number:0 epoch: 471/1200 loss_tea:2.319443162832293  loss_houyan:386.0010681152344\n",
      "Number:0 epoch: 481/1200 loss_tea:2.3279938014733337  loss_houyan:252.9033203125\n",
      "Number:0 epoch: 491/1200 loss_tea:2.2790326642742618  loss_houyan:293.99127197265625\n",
      "Number:0 epoch: 501/1200 loss_tea:2.246887181456939  loss_houyan:278.1898498535156\n",
      "Number:0 epoch: 511/1200 loss_tea:2.2799617067752824  loss_houyan:581.5452880859375\n",
      "Number:0 epoch: 521/1200 loss_tea:2.2340557378881116  loss_houyan:422.0960693359375\n",
      "Number:0 epoch: 531/1200 loss_tea:2.2454691296217764  loss_houyan:276.3797912597656\n",
      "Number:0 epoch: 541/1200 loss_tea:2.2932441628928  loss_houyan:273.5262145996094\n",
      "Number:0 epoch: 551/1200 loss_tea:2.3036089063103224  loss_houyan:350.12969970703125\n",
      "Number:0 epoch: 561/1200 loss_tea:2.251485002824592  loss_houyan:263.54681396484375\n",
      "Number:0 epoch: 571/1200 loss_tea:2.2605128121623532  loss_houyan:453.1163635253906\n",
      "Number:0 epoch: 581/1200 loss_tea:2.310192783316114  loss_houyan:462.78021240234375\n",
      "Number:0 epoch: 591/1200 loss_tea:2.283785044033222  loss_houyan:239.90830993652344\n",
      "Number:0 epoch: 601/1200 loss_tea:2.3075784867610074  loss_houyan:252.99337768554688\n",
      "Number:0 epoch: 611/1200 loss_tea:2.269522796947651  loss_houyan:236.255615234375\n",
      "Number:0 epoch: 621/1200 loss_tea:2.2980541279571693  loss_houyan:298.1566162109375\n",
      "Number:0 epoch: 631/1200 loss_tea:2.26884622167551  loss_houyan:278.5660400390625\n",
      "Number:0 epoch: 641/1200 loss_tea:2.313638146567097  loss_houyan:440.09063720703125\n",
      "Number:0 epoch: 651/1200 loss_tea:2.3131274576418126  loss_houyan:369.8524169921875\n",
      "Number:0 epoch: 661/1200 loss_tea:2.2440717580821685  loss_houyan:320.72210693359375\n",
      "Number:0 epoch: 671/1200 loss_tea:2.2645281336299274  loss_houyan:288.20965576171875\n",
      "Number:0 epoch: 681/1200 loss_tea:2.2898587290390964  loss_houyan:361.56103515625\n",
      "Number:0 epoch: 691/1200 loss_tea:2.2919671747511234  loss_houyan:325.4773254394531\n",
      "Number:0 epoch: 701/1200 loss_tea:2.318010366746711  loss_houyan:278.9595947265625\n",
      "Number:0 epoch: 711/1200 loss_tea:2.2877845736110913  loss_houyan:411.7857666015625\n",
      "Number:0 epoch: 721/1200 loss_tea:2.3209375080765327  loss_houyan:313.74737548828125\n",
      "Number:0 epoch: 731/1200 loss_tea:2.2967800424998193  loss_houyan:319.8251953125\n",
      "Number:0 epoch: 741/1200 loss_tea:2.2997445479396306  loss_houyan:388.40667724609375\n",
      "Number:0 epoch: 751/1200 loss_tea:2.2959566551921275  loss_houyan:395.8255310058594\n",
      "Number:0 epoch: 761/1200 loss_tea:2.2808053367278154  loss_houyan:359.27154541015625\n",
      "Number:0 epoch: 771/1200 loss_tea:2.341238815726706  loss_houyan:291.61065673828125\n",
      "Number:0 epoch: 781/1200 loss_tea:2.293934875623578  loss_houyan:338.57293701171875\n",
      "Number:0 epoch: 791/1200 loss_tea:2.2908668384420006  loss_houyan:529.7041015625\n",
      "Number:0 epoch: 801/1200 loss_tea:2.300150608480183  loss_houyan:258.8762512207031\n",
      "Number:0 epoch: 811/1200 loss_tea:2.268824417120858  loss_houyan:305.57818603515625\n",
      "Number:0 epoch: 821/1200 loss_tea:2.2746054433208847  loss_houyan:431.7359313964844\n",
      "Number:0 epoch: 831/1200 loss_tea:2.2409367691274333  loss_houyan:275.04156494140625\n",
      "Number:0 epoch: 841/1200 loss_tea:2.302509856265309  loss_houyan:330.10943603515625\n",
      "Number:0 epoch: 851/1200 loss_tea:2.3192425811166992  loss_houyan:316.69329833984375\n",
      "Number:0 epoch: 861/1200 loss_tea:2.3162016970888555  loss_houyan:1311.9769287109375\n",
      "Number:0 epoch: 871/1200 loss_tea:2.257386600269991  loss_houyan:376.2909851074219\n",
      "Number:0 epoch: 881/1200 loss_tea:2.3253059033704053  loss_houyan:294.41741943359375\n",
      "Number:0 epoch: 891/1200 loss_tea:2.260465543410357  loss_houyan:235.01930236816406\n",
      "Number:0 epoch: 901/1200 loss_tea:2.262416841992045  loss_houyan:273.49224853515625\n",
      "Number:0 epoch: 911/1200 loss_tea:2.30234918792355  loss_houyan:486.7411193847656\n",
      "Number:0 epoch: 921/1200 loss_tea:2.2742133526241077  loss_houyan:320.38702392578125\n",
      "Number:0 epoch: 931/1200 loss_tea:2.3161233517537894  loss_houyan:308.6817932128906\n",
      "Number:0 epoch: 941/1200 loss_tea:2.3329691047074474  loss_houyan:334.98883056640625\n",
      "Number:0 epoch: 951/1200 loss_tea:2.3153030885544617  loss_houyan:549.1444091796875\n",
      "Number:0 epoch: 961/1200 loss_tea:2.314946165711822  loss_houyan:284.4574890136719\n",
      "Number:0 epoch: 971/1200 loss_tea:2.2834077742685497  loss_houyan:365.07769775390625\n",
      "Number:0 epoch: 981/1200 loss_tea:2.314157579349399  loss_houyan:399.5625\n",
      "Number:0 epoch: 991/1200 loss_tea:2.2871313413534198  loss_houyan:350.692626953125\n",
      "Number:0 epoch: 1001/1200 loss_tea:2.271347514155827  loss_houyan:272.6781921386719\n",
      "Number:0 epoch: 1011/1200 loss_tea:2.2635116286756256  loss_houyan:281.8271179199219\n",
      "Number:0 epoch: 1021/1200 loss_tea:2.277410762532772  loss_houyan:235.41136169433594\n",
      "Number:0 epoch: 1031/1200 loss_tea:2.2902593155633206  loss_houyan:438.8184814453125\n",
      "Number:0 epoch: 1041/1200 loss_tea:2.320191528846648  loss_houyan:233.49655151367188\n",
      "Number:0 epoch: 1051/1200 loss_tea:2.3179390641232263  loss_houyan:250.0916290283203\n",
      "Number:0 epoch: 1061/1200 loss_tea:2.2809122511259825  loss_houyan:514.4829711914062\n",
      "Number:0 epoch: 1071/1200 loss_tea:2.2981138895952165  loss_houyan:494.72003173828125\n",
      "Number:0 epoch: 1081/1200 loss_tea:2.3165636017248294  loss_houyan:349.2201843261719\n",
      "Number:0 epoch: 1091/1200 loss_tea:2.3469743578491737  loss_houyan:287.8726806640625\n",
      "Number:0 epoch: 1101/1200 loss_tea:2.291771154832675  loss_houyan:315.10821533203125\n",
      "Number:0 epoch: 1111/1200 loss_tea:2.2875082758471215  loss_houyan:343.4384460449219\n",
      "Number:0 epoch: 1121/1200 loss_tea:2.3051349476340732  loss_houyan:357.3892822265625\n",
      "Number:0 epoch: 1131/1200 loss_tea:2.2818665965618146  loss_houyan:250.3168182373047\n",
      "Number:0 epoch: 1141/1200 loss_tea:2.300728535074676  loss_houyan:427.0853271484375\n",
      "Number:0 epoch: 1151/1200 loss_tea:2.3084680111762976  loss_houyan:302.53912353515625\n",
      "Number:0 epoch: 1161/1200 loss_tea:2.3130419454153848  loss_houyan:235.83517456054688\n",
      "Number:0 epoch: 1171/1200 loss_tea:2.3208039190117464  loss_houyan:227.05218505859375\n",
      "Number:0 epoch: 1181/1200 loss_tea:2.3004995435991913  loss_houyan:285.82489013671875\n",
      "Number:0 epoch: 1191/1200 loss_tea:2.3233737932356995  loss_houyan:275.3292236328125\n",
      "finished training number 0 techer!\n",
      "start training number 1 techer!\n",
      "Number:1 epoch: 1/1200 loss_tea:2.355247596638425  loss_houyan:299.33642578125\n",
      "Number:1 epoch: 11/1200 loss_tea:2.3278659982252288  loss_houyan:237.109130859375\n",
      "Number:1 epoch: 21/1200 loss_tea:2.3314986729704383  loss_houyan:247.65081787109375\n",
      "Number:1 epoch: 31/1200 loss_tea:2.3000025284744052  loss_houyan:239.09219360351562\n",
      "Number:1 epoch: 41/1200 loss_tea:2.319746617792387  loss_houyan:217.68212890625\n",
      "Number:1 epoch: 51/1200 loss_tea:2.3123893402851987  loss_houyan:247.61422729492188\n",
      "Number:1 epoch: 61/1200 loss_tea:2.3043217479151425  loss_houyan:271.5550842285156\n",
      "Number:1 epoch: 71/1200 loss_tea:2.32760556082412  loss_houyan:241.6554412841797\n",
      "Number:1 epoch: 81/1200 loss_tea:2.3180073418831744  loss_houyan:289.1746826171875\n",
      "Number:1 epoch: 91/1200 loss_tea:2.3057523958410773  loss_houyan:267.1005859375\n",
      "Number:1 epoch: 101/1200 loss_tea:2.3193767293514265  loss_houyan:428.52362060546875\n",
      "Number:1 epoch: 111/1200 loss_tea:2.3386478197203378  loss_houyan:280.35711669921875\n",
      "Number:1 epoch: 121/1200 loss_tea:2.3222608965573426  loss_houyan:380.2044372558594\n",
      "Number:1 epoch: 131/1200 loss_tea:2.3065206255467294  loss_houyan:361.7523498535156\n",
      "Number:1 epoch: 141/1200 loss_tea:2.3129797762240507  loss_houyan:313.8907470703125\n",
      "Number:1 epoch: 151/1200 loss_tea:2.2872605903338394  loss_houyan:271.4053039550781\n",
      "Number:1 epoch: 161/1200 loss_tea:2.27037811108114  loss_houyan:290.64947509765625\n",
      "Number:1 epoch: 171/1200 loss_tea:2.3174380274379955  loss_houyan:287.8280029296875\n",
      "Number:1 epoch: 181/1200 loss_tea:2.3020664563641002  loss_houyan:249.08074951171875\n",
      "Number:1 epoch: 191/1200 loss_tea:2.2939669303828043  loss_houyan:204.20199584960938\n",
      "Number:1 epoch: 201/1200 loss_tea:2.332776772522184  loss_houyan:248.50924682617188\n",
      "Number:1 epoch: 211/1200 loss_tea:2.3027990913721106  loss_houyan:248.9919891357422\n",
      "Number:1 epoch: 221/1200 loss_tea:2.2751021085725935  loss_houyan:263.5685729980469\n",
      "Number:1 epoch: 231/1200 loss_tea:2.3139082250710588  loss_houyan:268.07659912109375\n",
      "Number:1 epoch: 241/1200 loss_tea:2.3283120381378386  loss_houyan:246.66793823242188\n",
      "Number:1 epoch: 251/1200 loss_tea:2.3096560058709246  loss_houyan:267.2532958984375\n",
      "Number:1 epoch: 261/1200 loss_tea:2.3193443057446332  loss_houyan:710.3212280273438\n",
      "Number:1 epoch: 271/1200 loss_tea:2.3075848812050475  loss_houyan:218.75038146972656\n",
      "Number:1 epoch: 281/1200 loss_tea:2.302366316689752  loss_houyan:208.13758850097656\n",
      "Number:1 epoch: 291/1200 loss_tea:2.2882680249461664  loss_houyan:227.3525390625\n",
      "Number:1 epoch: 301/1200 loss_tea:2.320956492176518  loss_houyan:498.4231262207031\n",
      "Number:1 epoch: 311/1200 loss_tea:2.2782190234603354  loss_houyan:218.78466796875\n",
      "Number:1 epoch: 321/1200 loss_tea:2.3452133343706496  loss_houyan:272.27239990234375\n",
      "Number:1 epoch: 331/1200 loss_tea:2.3085526783161097  loss_houyan:276.2599182128906\n",
      "Number:1 epoch: 341/1200 loss_tea:2.3367495426257174  loss_houyan:304.14056396484375\n",
      "Number:1 epoch: 351/1200 loss_tea:2.300359114148625  loss_houyan:280.74481201171875\n",
      "Number:1 epoch: 361/1200 loss_tea:2.3296849650082705  loss_houyan:230.57186889648438\n",
      "Number:1 epoch: 371/1200 loss_tea:2.3050302075885982  loss_houyan:245.14865112304688\n",
      "Number:1 epoch: 381/1200 loss_tea:2.3337435330486627  loss_houyan:369.7474670410156\n",
      "Number:1 epoch: 391/1200 loss_tea:2.2941148039378922  loss_houyan:237.7210693359375\n",
      "Number:1 epoch: 401/1200 loss_tea:2.304444827257968  loss_houyan:221.44757080078125\n",
      "Number:1 epoch: 411/1200 loss_tea:2.2833128282561845  loss_houyan:201.69688415527344\n",
      "Number:1 epoch: 421/1200 loss_tea:2.325818300742179  loss_houyan:263.2102966308594\n",
      "Number:1 epoch: 431/1200 loss_tea:2.323870280374705  loss_houyan:341.85992431640625\n",
      "Number:1 epoch: 441/1200 loss_tea:2.2956493790174437  loss_houyan:545.5113525390625\n",
      "Number:1 epoch: 451/1200 loss_tea:2.306935466449566  loss_houyan:320.42962646484375\n",
      "Number:1 epoch: 461/1200 loss_tea:2.2770041893922746  loss_houyan:222.5138702392578\n",
      "Number:1 epoch: 471/1200 loss_tea:2.287606234368981  loss_houyan:544.1216430664062\n",
      "Number:1 epoch: 481/1200 loss_tea:2.2853088925041543  loss_houyan:202.4181671142578\n",
      "Number:1 epoch: 491/1200 loss_tea:2.3026101095866167  loss_houyan:793.7064819335938\n",
      "Number:1 epoch: 501/1200 loss_tea:2.300451867390669  loss_houyan:249.48825073242188\n",
      "Number:1 epoch: 511/1200 loss_tea:2.3357918895239647  loss_houyan:264.3362121582031\n",
      "Number:1 epoch: 521/1200 loss_tea:2.3231625958709983  loss_houyan:331.5787658691406\n",
      "Number:1 epoch: 531/1200 loss_tea:2.326968460347001  loss_houyan:258.97076416015625\n",
      "Number:1 epoch: 541/1200 loss_tea:2.3266423920034125  loss_houyan:214.88829040527344\n",
      "Number:1 epoch: 551/1200 loss_tea:2.318554642456213  loss_houyan:237.6204376220703\n",
      "Number:1 epoch: 561/1200 loss_tea:2.346271394775813  loss_houyan:307.518310546875\n",
      "Number:1 epoch: 571/1200 loss_tea:2.31307735599861  loss_houyan:229.31991577148438\n",
      "Number:1 epoch: 581/1200 loss_tea:2.319636026798235  loss_houyan:230.1848602294922\n",
      "Number:1 epoch: 591/1200 loss_tea:2.3143298909738403  loss_houyan:260.4366149902344\n",
      "Number:1 epoch: 601/1200 loss_tea:2.3415723284223087  loss_houyan:249.86337280273438\n",
      "Number:1 epoch: 611/1200 loss_tea:2.3307329579620624  loss_houyan:264.83355712890625\n",
      "Number:1 epoch: 621/1200 loss_tea:2.3295181273589085  loss_houyan:240.03109741210938\n",
      "Number:1 epoch: 631/1200 loss_tea:2.3305195261450375  loss_houyan:221.08506774902344\n",
      "Number:1 epoch: 641/1200 loss_tea:2.3197725647461045  loss_houyan:274.10003662109375\n",
      "Number:1 epoch: 651/1200 loss_tea:2.2965645147854894  loss_houyan:231.8262176513672\n",
      "Number:1 epoch: 661/1200 loss_tea:2.3085600206183727  loss_houyan:269.3329162597656\n",
      "Number:1 epoch: 671/1200 loss_tea:2.2937138826376837  loss_houyan:279.7410888671875\n",
      "Number:1 epoch: 681/1200 loss_tea:2.3022952792553752  loss_houyan:320.80450439453125\n",
      "Number:1 epoch: 691/1200 loss_tea:2.3366623748132516  loss_houyan:222.58126831054688\n",
      "Number:1 epoch: 701/1200 loss_tea:2.281878916532523  loss_houyan:561.892333984375\n",
      "Number:1 epoch: 711/1200 loss_tea:2.3083853637883407  loss_houyan:204.99635314941406\n",
      "Number:1 epoch: 721/1200 loss_tea:2.322946329636557  loss_houyan:212.3787078857422\n",
      "Number:1 epoch: 731/1200 loss_tea:2.312964296877178  loss_houyan:248.72998046875\n",
      "Number:1 epoch: 741/1200 loss_tea:2.3107470951278315  loss_houyan:223.61595153808594\n",
      "Number:1 epoch: 751/1200 loss_tea:2.3229233537165763  loss_houyan:319.07025146484375\n",
      "Number:1 epoch: 761/1200 loss_tea:2.3164329491272104  loss_houyan:303.3119201660156\n",
      "Number:1 epoch: 771/1200 loss_tea:2.3238479970648216  loss_houyan:210.211669921875\n",
      "Number:1 epoch: 781/1200 loss_tea:2.3256226897652175  loss_houyan:227.92291259765625\n",
      "Number:1 epoch: 791/1200 loss_tea:2.3295930685056536  loss_houyan:260.4383544921875\n",
      "Number:1 epoch: 801/1200 loss_tea:2.3199829308219435  loss_houyan:243.3451690673828\n",
      "Number:1 epoch: 811/1200 loss_tea:2.3409353698413677  loss_houyan:295.2758483886719\n",
      "Number:1 epoch: 821/1200 loss_tea:2.3132641832721275  loss_houyan:249.4305419921875\n",
      "Number:1 epoch: 831/1200 loss_tea:2.313894779624411  loss_houyan:266.91497802734375\n",
      "Number:1 epoch: 841/1200 loss_tea:2.3101331219128673  loss_houyan:296.0382080078125\n",
      "Number:1 epoch: 851/1200 loss_tea:2.3065633747404424  loss_houyan:212.89476013183594\n",
      "Number:1 epoch: 861/1200 loss_tea:2.301132995117082  loss_houyan:211.93276977539062\n",
      "Number:1 epoch: 871/1200 loss_tea:2.2791868564579314  loss_houyan:316.81817626953125\n",
      "Number:1 epoch: 881/1200 loss_tea:2.3051848651216105  loss_houyan:328.8724060058594\n",
      "Number:1 epoch: 891/1200 loss_tea:2.310395266522998  loss_houyan:346.0667724609375\n",
      "Number:1 epoch: 901/1200 loss_tea:2.3363610756026008  loss_houyan:562.1807250976562\n",
      "Number:1 epoch: 911/1200 loss_tea:2.3040790466288796  loss_houyan:217.4669647216797\n",
      "Number:1 epoch: 921/1200 loss_tea:2.3255687416631043  loss_houyan:287.9058837890625\n",
      "Number:1 epoch: 931/1200 loss_tea:2.3344537784484016  loss_houyan:211.7151336669922\n",
      "Number:1 epoch: 941/1200 loss_tea:2.3290986310239483  loss_houyan:375.8686828613281\n",
      "Number:1 epoch: 951/1200 loss_tea:2.318244310068837  loss_houyan:474.5201110839844\n",
      "Number:1 epoch: 961/1200 loss_tea:2.260269584127776  loss_houyan:225.49227905273438\n",
      "Number:1 epoch: 971/1200 loss_tea:2.3403847321507016  loss_houyan:334.180908203125\n",
      "Number:1 epoch: 981/1200 loss_tea:2.313241899054768  loss_houyan:416.711181640625\n",
      "Number:1 epoch: 991/1200 loss_tea:2.321905538903801  loss_houyan:301.49603271484375\n",
      "Number:1 epoch: 1001/1200 loss_tea:2.303901145367474  loss_houyan:464.30145263671875\n",
      "Number:1 epoch: 1011/1200 loss_tea:2.3027859781852644  loss_houyan:201.06394958496094\n",
      "Number:1 epoch: 1021/1200 loss_tea:2.283431169194746  loss_houyan:240.6136474609375\n",
      "Number:1 epoch: 1031/1200 loss_tea:2.296178823523868  loss_houyan:262.0362854003906\n",
      "Number:1 epoch: 1041/1200 loss_tea:2.3367366091190327  loss_houyan:243.26918029785156\n",
      "Number:1 epoch: 1051/1200 loss_tea:2.3031840807838835  loss_houyan:346.2297058105469\n",
      "Number:1 epoch: 1061/1200 loss_tea:2.284287590304048  loss_houyan:301.54718017578125\n",
      "Number:1 epoch: 1071/1200 loss_tea:2.334417729889233  loss_houyan:397.49664306640625\n",
      "Number:1 epoch: 1081/1200 loss_tea:2.329066917228039  loss_houyan:263.5350646972656\n",
      "Number:1 epoch: 1091/1200 loss_tea:2.299008571977846  loss_houyan:296.7065734863281\n",
      "Number:1 epoch: 1101/1200 loss_tea:2.2972648939459384  loss_houyan:245.17015075683594\n",
      "Number:1 epoch: 1111/1200 loss_tea:2.314039075168359  loss_houyan:229.54693603515625\n",
      "Number:1 epoch: 1121/1200 loss_tea:2.2903973315413846  loss_houyan:227.0032196044922\n",
      "Number:1 epoch: 1131/1200 loss_tea:2.3347022635713994  loss_houyan:206.74478149414062\n",
      "Number:1 epoch: 1141/1200 loss_tea:2.3318412319599138  loss_houyan:407.2903747558594\n",
      "Number:1 epoch: 1151/1200 loss_tea:2.3157404445859386  loss_houyan:223.43907165527344\n",
      "Number:1 epoch: 1161/1200 loss_tea:2.308511726253998  loss_houyan:253.87152099609375\n",
      "Number:1 epoch: 1171/1200 loss_tea:2.2912060498778795  loss_houyan:286.556396484375\n",
      "Number:1 epoch: 1181/1200 loss_tea:2.367923708523021  loss_houyan:298.9827880859375\n",
      "Number:1 epoch: 1191/1200 loss_tea:2.3034194744994485  loss_houyan:421.5977478027344\n",
      "finished training number 1 techer!\n",
      "start training number 2 techer!\n",
      "Number:2 epoch: 1/1200 loss_tea:2.0538488732902236  loss_houyan:216.07772827148438\n",
      "Number:2 epoch: 11/1200 loss_tea:2.090277519894306  loss_houyan:287.292724609375\n",
      "Number:2 epoch: 21/1200 loss_tea:2.05916150486593  loss_houyan:232.49978637695312\n",
      "Number:2 epoch: 31/1200 loss_tea:2.1037063490148236  loss_houyan:319.00750732421875\n",
      "Number:2 epoch: 41/1200 loss_tea:2.0872237124657547  loss_houyan:188.87608337402344\n",
      "Number:2 epoch: 51/1200 loss_tea:2.0494832406819485  loss_houyan:342.95733642578125\n",
      "Number:2 epoch: 61/1200 loss_tea:2.0647951696983258  loss_houyan:232.9458465576172\n",
      "Number:2 epoch: 71/1200 loss_tea:2.090985842972066  loss_houyan:494.0340270996094\n",
      "Number:2 epoch: 81/1200 loss_tea:2.039934529400202  loss_houyan:454.5607604980469\n",
      "Number:2 epoch: 91/1200 loss_tea:2.1045567835903496  loss_houyan:509.4046936035156\n",
      "Number:2 epoch: 101/1200 loss_tea:2.0649027270016784  loss_houyan:229.22982788085938\n",
      "Number:2 epoch: 111/1200 loss_tea:2.0901462630829597  loss_houyan:243.1329345703125\n",
      "Number:2 epoch: 121/1200 loss_tea:2.0725497826160444  loss_houyan:251.714111328125\n",
      "Number:2 epoch: 131/1200 loss_tea:2.097986624397621  loss_houyan:232.61367797851562\n",
      "Number:2 epoch: 141/1200 loss_tea:2.0744812958380754  loss_houyan:236.67091369628906\n",
      "Number:2 epoch: 151/1200 loss_tea:2.0896464299990645  loss_houyan:278.9062194824219\n",
      "Number:2 epoch: 161/1200 loss_tea:2.0579414532671336  loss_houyan:1211.20849609375\n",
      "Number:2 epoch: 171/1200 loss_tea:2.0577696911603933  loss_houyan:200.69036865234375\n",
      "Number:2 epoch: 181/1200 loss_tea:2.077613275603852  loss_houyan:243.33230590820312\n",
      "Number:2 epoch: 191/1200 loss_tea:2.056147922157829  loss_houyan:268.68841552734375\n",
      "Number:2 epoch: 201/1200 loss_tea:2.094610063344962  loss_houyan:191.6731414794922\n",
      "Number:2 epoch: 211/1200 loss_tea:2.06915305147534  loss_houyan:548.44287109375\n",
      "Number:2 epoch: 221/1200 loss_tea:2.094599826443154  loss_houyan:229.64617919921875\n",
      "Number:2 epoch: 231/1200 loss_tea:2.092194442249912  loss_houyan:200.78273010253906\n",
      "Number:2 epoch: 241/1200 loss_tea:2.0718227267677807  loss_houyan:520.88330078125\n",
      "Number:2 epoch: 251/1200 loss_tea:2.1023677362290223  loss_houyan:199.6819610595703\n",
      "Number:2 epoch: 261/1200 loss_tea:2.0538672225698056  loss_houyan:243.06570434570312\n",
      "Number:2 epoch: 271/1200 loss_tea:2.0922193806889147  loss_houyan:236.7210693359375\n",
      "Number:2 epoch: 281/1200 loss_tea:2.075864036767953  loss_houyan:227.66455078125\n",
      "Number:2 epoch: 291/1200 loss_tea:2.0701011191602396  loss_houyan:220.18612670898438\n",
      "Number:2 epoch: 301/1200 loss_tea:2.081892356410571  loss_houyan:209.32435607910156\n",
      "Number:2 epoch: 311/1200 loss_tea:2.1126627545868235  loss_houyan:255.92913818359375\n",
      "Number:2 epoch: 321/1200 loss_tea:2.063627289653237  loss_houyan:211.26939392089844\n",
      "Number:2 epoch: 331/1200 loss_tea:2.0938108269318576  loss_houyan:226.60525512695312\n",
      "Number:2 epoch: 341/1200 loss_tea:2.0940802281084356  loss_houyan:273.28497314453125\n",
      "Number:2 epoch: 351/1200 loss_tea:2.0666920306979577  loss_houyan:240.6628875732422\n",
      "Number:2 epoch: 361/1200 loss_tea:2.085440758602842  loss_houyan:215.20693969726562\n",
      "Number:2 epoch: 371/1200 loss_tea:2.08449770785533  loss_houyan:200.37205505371094\n",
      "Number:2 epoch: 381/1200 loss_tea:2.059714746805211  loss_houyan:247.52133178710938\n",
      "Number:2 epoch: 391/1200 loss_tea:2.0815335503911476  loss_houyan:268.1341552734375\n",
      "Number:2 epoch: 401/1200 loss_tea:2.0843698769704693  loss_houyan:213.74032592773438\n",
      "Number:2 epoch: 411/1200 loss_tea:2.104521070467147  loss_houyan:291.1820983886719\n",
      "Number:2 epoch: 421/1200 loss_tea:2.0980284043249373  loss_houyan:323.3320007324219\n",
      "Number:2 epoch: 431/1200 loss_tea:2.0533191066299756  loss_houyan:237.49571228027344\n",
      "Number:2 epoch: 441/1200 loss_tea:2.079030049300936  loss_houyan:587.469482421875\n",
      "Number:2 epoch: 451/1200 loss_tea:2.0391274250914893  loss_houyan:258.22100830078125\n",
      "Number:2 epoch: 461/1200 loss_tea:2.0977927465339845  loss_houyan:249.9666290283203\n",
      "Number:2 epoch: 471/1200 loss_tea:2.086535981617172  loss_houyan:301.10162353515625\n",
      "Number:2 epoch: 481/1200 loss_tea:2.111415016568656  loss_houyan:261.1554870605469\n",
      "Number:2 epoch: 491/1200 loss_tea:2.100934518662291  loss_houyan:188.04747009277344\n",
      "Number:2 epoch: 501/1200 loss_tea:2.069178391162912  loss_houyan:220.29644775390625\n",
      "Number:2 epoch: 511/1200 loss_tea:2.0857516856754525  loss_houyan:208.9407501220703\n",
      "Number:2 epoch: 521/1200 loss_tea:2.0826552457462957  loss_houyan:229.3585662841797\n",
      "Number:2 epoch: 531/1200 loss_tea:2.0917721393611606  loss_houyan:461.57220458984375\n",
      "Number:2 epoch: 541/1200 loss_tea:2.0955863672144273  loss_houyan:219.79617309570312\n",
      "Number:2 epoch: 551/1200 loss_tea:2.107296108044555  loss_houyan:245.90699768066406\n",
      "Number:2 epoch: 561/1200 loss_tea:2.070238264951739  loss_houyan:447.3500671386719\n",
      "Number:2 epoch: 571/1200 loss_tea:2.0949840360859273  loss_houyan:215.09584045410156\n",
      "Number:2 epoch: 581/1200 loss_tea:2.087013600689317  loss_houyan:190.9694061279297\n",
      "Number:2 epoch: 591/1200 loss_tea:2.1072945819181554  loss_houyan:206.16220092773438\n",
      "Number:2 epoch: 601/1200 loss_tea:2.067065718264728  loss_houyan:291.3873596191406\n",
      "Number:2 epoch: 611/1200 loss_tea:2.1059954572301423  loss_houyan:217.02877807617188\n",
      "Number:2 epoch: 621/1200 loss_tea:2.077615528535678  loss_houyan:319.152587890625\n",
      "Number:2 epoch: 631/1200 loss_tea:2.0549693659515116  loss_houyan:240.46585083007812\n",
      "Number:2 epoch: 641/1200 loss_tea:2.098277813423051  loss_houyan:304.3175354003906\n",
      "Number:2 epoch: 651/1200 loss_tea:2.068269715507138  loss_houyan:329.5264892578125\n",
      "Number:2 epoch: 661/1200 loss_tea:2.1024490219499  loss_houyan:232.71786499023438\n",
      "Number:2 epoch: 671/1200 loss_tea:2.1008723498215724  loss_houyan:409.19281005859375\n",
      "Number:2 epoch: 681/1200 loss_tea:2.0558162057276004  loss_houyan:388.190673828125\n",
      "Number:2 epoch: 691/1200 loss_tea:2.076471969495595  loss_houyan:309.7420959472656\n",
      "Number:2 epoch: 701/1200 loss_tea:2.0894048591798566  loss_houyan:209.2379608154297\n",
      "Number:2 epoch: 711/1200 loss_tea:2.0614150553838604  loss_houyan:249.61502075195312\n",
      "Number:2 epoch: 721/1200 loss_tea:2.0752617248201863  loss_houyan:237.66236877441406\n",
      "Number:2 epoch: 731/1200 loss_tea:2.091471438011909  loss_houyan:295.25115966796875\n",
      "Number:2 epoch: 741/1200 loss_tea:2.1003608765486614  loss_houyan:224.43661499023438\n",
      "Number:2 epoch: 751/1200 loss_tea:2.077482541374682  loss_houyan:274.8827209472656\n",
      "Number:2 epoch: 761/1200 loss_tea:2.0962646973586825  loss_houyan:215.60946655273438\n",
      "Number:2 epoch: 771/1200 loss_tea:2.0877173575563  loss_houyan:241.60911560058594\n",
      "Number:2 epoch: 781/1200 loss_tea:2.088306623140421  loss_houyan:209.83673095703125\n",
      "Number:2 epoch: 791/1200 loss_tea:2.0815262867505164  loss_houyan:268.8020935058594\n",
      "Number:2 epoch: 801/1200 loss_tea:2.1079036036164704  loss_houyan:318.6690673828125\n",
      "Number:2 epoch: 811/1200 loss_tea:2.074517983301288  loss_houyan:238.59324645996094\n",
      "Number:2 epoch: 821/1200 loss_tea:2.1087136722353503  loss_houyan:200.5542755126953\n",
      "Number:2 epoch: 831/1200 loss_tea:2.093671833272624  loss_houyan:326.55938720703125\n",
      "Number:2 epoch: 841/1200 loss_tea:2.102822664947246  loss_houyan:204.85874938964844\n",
      "Number:2 epoch: 851/1200 loss_tea:2.095593483885267  loss_houyan:230.4715118408203\n",
      "Number:2 epoch: 861/1200 loss_tea:2.084830182316394  loss_houyan:231.37937927246094\n",
      "Number:2 epoch: 871/1200 loss_tea:2.1036907621733456  loss_houyan:255.91140747070312\n",
      "Number:2 epoch: 881/1200 loss_tea:2.06458538201441  loss_houyan:269.4623107910156\n",
      "Number:2 epoch: 891/1200 loss_tea:2.0811232246742115  loss_houyan:623.9459838867188\n",
      "Number:2 epoch: 901/1200 loss_tea:2.0989249441037954  loss_houyan:199.9086456298828\n",
      "Number:2 epoch: 911/1200 loss_tea:2.0850707273582274  loss_houyan:472.0242919921875\n",
      "Number:2 epoch: 921/1200 loss_tea:2.105406408738925  loss_houyan:416.88616943359375\n",
      "Number:2 epoch: 931/1200 loss_tea:2.091647363121534  loss_houyan:276.3559875488281\n",
      "Number:2 epoch: 941/1200 loss_tea:2.082954538714102  loss_houyan:273.500244140625\n",
      "Number:2 epoch: 951/1200 loss_tea:2.071959459699149  loss_houyan:254.10574340820312\n",
      "Number:2 epoch: 961/1200 loss_tea:2.0850344632014273  loss_houyan:211.32846069335938\n",
      "Number:2 epoch: 971/1200 loss_tea:2.0974457089051244  loss_houyan:382.15771484375\n",
      "Number:2 epoch: 981/1200 loss_tea:2.067191514341889  loss_houyan:443.6790466308594\n",
      "Number:2 epoch: 991/1200 loss_tea:2.0844049034646637  loss_houyan:259.2886657714844\n",
      "Number:2 epoch: 1001/1200 loss_tea:2.0824044146752274  loss_houyan:449.2625732421875\n",
      "Number:2 epoch: 1011/1200 loss_tea:2.103521536130806  loss_houyan:221.79148864746094\n",
      "Number:2 epoch: 1021/1200 loss_tea:2.087411053122946  loss_houyan:219.27210998535156\n",
      "Number:2 epoch: 1031/1200 loss_tea:2.0811425985349503  loss_houyan:301.07330322265625\n",
      "Number:2 epoch: 1041/1200 loss_tea:2.0699932696910053  loss_houyan:234.79220581054688\n",
      "Number:2 epoch: 1051/1200 loss_tea:2.0878506489691024  loss_houyan:292.57586669921875\n",
      "Number:2 epoch: 1061/1200 loss_tea:2.1021647271400505  loss_houyan:225.89939880371094\n",
      "Number:2 epoch: 1071/1200 loss_tea:2.0940812062639678  loss_houyan:243.66561889648438\n",
      "Number:2 epoch: 1081/1200 loss_tea:2.0967561537006736  loss_houyan:210.6656494140625\n",
      "Number:2 epoch: 1091/1200 loss_tea:2.069516047639418  loss_houyan:217.5995635986328\n",
      "Number:2 epoch: 1101/1200 loss_tea:2.1068445959511926  loss_houyan:230.4225311279297\n",
      "Number:2 epoch: 1111/1200 loss_tea:2.0691648021185687  loss_houyan:192.78683471679688\n",
      "Number:2 epoch: 1121/1200 loss_tea:2.0832697277663077  loss_houyan:403.925537109375\n",
      "Number:2 epoch: 1131/1200 loss_tea:2.082016507317038  loss_houyan:221.60549926757812\n",
      "Number:2 epoch: 1141/1200 loss_tea:2.0820840317485243  loss_houyan:207.9673309326172\n",
      "Number:2 epoch: 1151/1200 loss_tea:2.0691495826706343  loss_houyan:199.32383728027344\n",
      "Number:2 epoch: 1161/1200 loss_tea:2.078012567902931  loss_houyan:263.68939208984375\n",
      "Number:2 epoch: 1171/1200 loss_tea:2.0738456033299126  loss_houyan:196.46859741210938\n",
      "Number:2 epoch: 1181/1200 loss_tea:2.0824842730195465  loss_houyan:206.95851135253906\n",
      "Number:2 epoch: 1191/1200 loss_tea:2.0914527778806984  loss_houyan:225.72866821289062\n",
      "finished training number 2 techer!\n",
      "start training number 3 techer!\n",
      "Number:3 epoch: 1/1200 loss_tea:2.1932976351477285  loss_houyan:212.4386749267578\n",
      "Number:3 epoch: 11/1200 loss_tea:2.1276652519265675  loss_houyan:243.74757385253906\n",
      "Number:3 epoch: 21/1200 loss_tea:2.150957615895255  loss_houyan:207.19932556152344\n",
      "Number:3 epoch: 31/1200 loss_tea:2.099081870016342  loss_houyan:249.3310546875\n",
      "Number:3 epoch: 41/1200 loss_tea:2.1199132759265833  loss_houyan:374.84136962890625\n",
      "Number:3 epoch: 51/1200 loss_tea:2.098211538915403  loss_houyan:402.5702209472656\n",
      "Number:3 epoch: 61/1200 loss_tea:2.1327163416209105  loss_houyan:242.15135192871094\n",
      "Number:3 epoch: 71/1200 loss_tea:2.1446187083283923  loss_houyan:229.5210418701172\n",
      "Number:3 epoch: 81/1200 loss_tea:2.1473936954584087  loss_houyan:310.56976318359375\n",
      "Number:3 epoch: 91/1200 loss_tea:2.151642337224888  loss_houyan:371.3668212890625\n",
      "Number:3 epoch: 101/1200 loss_tea:2.1537008585814372  loss_houyan:238.7919158935547\n",
      "Number:3 epoch: 111/1200 loss_tea:2.1643698606524087  loss_houyan:241.28701782226562\n",
      "Number:3 epoch: 121/1200 loss_tea:2.128277223250445  loss_houyan:412.1737365722656\n",
      "Number:3 epoch: 131/1200 loss_tea:2.1563465059009803  loss_houyan:242.6527862548828\n",
      "Number:3 epoch: 141/1200 loss_tea:2.1265891120508056  loss_houyan:329.37457275390625\n",
      "Number:3 epoch: 151/1200 loss_tea:2.1482079581405875  loss_houyan:291.3368225097656\n",
      "Number:3 epoch: 161/1200 loss_tea:2.152381480523872  loss_houyan:249.36083984375\n",
      "Number:3 epoch: 171/1200 loss_tea:2.127260258618523  loss_houyan:424.2061767578125\n",
      "Number:3 epoch: 181/1200 loss_tea:2.137487733157861  loss_houyan:228.36026000976562\n",
      "Number:3 epoch: 191/1200 loss_tea:2.15911266449001  loss_houyan:229.69882202148438\n",
      "Number:3 epoch: 201/1200 loss_tea:2.1739078921017763  loss_houyan:240.68307495117188\n",
      "Number:3 epoch: 211/1200 loss_tea:2.140665092641507  loss_houyan:215.17398071289062\n",
      "Number:3 epoch: 221/1200 loss_tea:2.1519151035064645  loss_houyan:357.7469787597656\n",
      "Number:3 epoch: 231/1200 loss_tea:2.188770131029472  loss_houyan:226.4974365234375\n",
      "Number:3 epoch: 241/1200 loss_tea:2.160204514500179  loss_houyan:383.2382507324219\n",
      "Number:3 epoch: 251/1200 loss_tea:2.1652462118637192  loss_houyan:215.54359436035156\n",
      "Number:3 epoch: 261/1200 loss_tea:2.1028790161271407  loss_houyan:207.13516235351562\n",
      "Number:3 epoch: 271/1200 loss_tea:2.133733389864331  loss_houyan:228.55853271484375\n",
      "Number:3 epoch: 281/1200 loss_tea:2.1280913756380446  loss_houyan:370.9481506347656\n",
      "Number:3 epoch: 291/1200 loss_tea:2.1295097980532267  loss_houyan:206.08790588378906\n",
      "Number:3 epoch: 301/1200 loss_tea:2.104714776652907  loss_houyan:375.5399169921875\n",
      "Number:3 epoch: 311/1200 loss_tea:2.169298439949854  loss_houyan:218.87457275390625\n",
      "Number:3 epoch: 321/1200 loss_tea:2.1202582613407124  loss_houyan:214.1025848388672\n",
      "Number:3 epoch: 331/1200 loss_tea:2.1591907240527726  loss_houyan:205.3641357421875\n",
      "Number:3 epoch: 341/1200 loss_tea:2.1580612350087676  loss_houyan:264.6759338378906\n",
      "Number:3 epoch: 351/1200 loss_tea:2.1766888039334833  loss_houyan:204.7314453125\n",
      "Number:3 epoch: 361/1200 loss_tea:2.105334738917829  loss_houyan:259.47967529296875\n",
      "Number:3 epoch: 371/1200 loss_tea:2.1281370374983157  loss_houyan:300.5426330566406\n",
      "Number:3 epoch: 381/1200 loss_tea:2.1339125376671655  loss_houyan:276.12750244140625\n",
      "Number:3 epoch: 391/1200 loss_tea:2.154300970437205  loss_houyan:366.51727294921875\n",
      "Number:3 epoch: 401/1200 loss_tea:2.115058206026942  loss_houyan:246.8428955078125\n",
      "Number:3 epoch: 411/1200 loss_tea:2.15179443656367  loss_houyan:256.7242126464844\n",
      "Number:3 epoch: 421/1200 loss_tea:2.1604084337458893  loss_houyan:196.32789611816406\n",
      "Number:3 epoch: 431/1200 loss_tea:2.14267634157491  loss_houyan:254.8197021484375\n",
      "Number:3 epoch: 441/1200 loss_tea:2.1670140160821303  loss_houyan:219.44622802734375\n",
      "Number:3 epoch: 451/1200 loss_tea:2.1162968096023613  loss_houyan:271.50213623046875\n",
      "Number:3 epoch: 461/1200 loss_tea:2.147499367728778  loss_houyan:208.02391052246094\n",
      "Number:3 epoch: 471/1200 loss_tea:2.110526126726276  loss_houyan:224.8968963623047\n",
      "Number:3 epoch: 481/1200 loss_tea:2.186394643948565  loss_houyan:245.60671997070312\n",
      "Number:3 epoch: 491/1200 loss_tea:2.1380893586828633  loss_houyan:359.9286804199219\n",
      "Number:3 epoch: 501/1200 loss_tea:2.1207067091572243  loss_houyan:247.15380859375\n",
      "Number:3 epoch: 511/1200 loss_tea:2.1099318118656383  loss_houyan:235.46920776367188\n",
      "Number:3 epoch: 521/1200 loss_tea:2.1368052360508267  loss_houyan:261.0927734375\n",
      "Number:3 epoch: 531/1200 loss_tea:2.127147471739759  loss_houyan:271.8487854003906\n",
      "Number:3 epoch: 541/1200 loss_tea:2.1742135141960066  loss_houyan:234.9873046875\n",
      "Number:3 epoch: 551/1200 loss_tea:2.1208627570459173  loss_houyan:205.63577270507812\n",
      "Number:3 epoch: 561/1200 loss_tea:2.1795069813315844  loss_houyan:497.26861572265625\n",
      "Number:3 epoch: 571/1200 loss_tea:2.131253697055434  loss_houyan:250.3070526123047\n",
      "Number:3 epoch: 581/1200 loss_tea:2.105718105474558  loss_houyan:459.7605285644531\n",
      "Number:3 epoch: 591/1200 loss_tea:2.1646150433893436  loss_houyan:332.2615051269531\n",
      "Number:3 epoch: 601/1200 loss_tea:2.1571419674632457  loss_houyan:299.7038879394531\n",
      "Number:3 epoch: 611/1200 loss_tea:2.159766984274643  loss_houyan:250.08755493164062\n",
      "Number:3 epoch: 621/1200 loss_tea:2.165860715003162  loss_houyan:189.89378356933594\n",
      "Number:3 epoch: 631/1200 loss_tea:2.1609882037119883  loss_houyan:216.3253631591797\n",
      "Number:3 epoch: 641/1200 loss_tea:2.179155162378991  loss_houyan:203.87210083007812\n",
      "Number:3 epoch: 651/1200 loss_tea:2.165506007349615  loss_houyan:252.26132202148438\n",
      "Number:3 epoch: 661/1200 loss_tea:2.1224033900198225  loss_houyan:271.7558288574219\n",
      "Number:3 epoch: 671/1200 loss_tea:2.134385559055632  loss_houyan:286.51202392578125\n",
      "Number:3 epoch: 681/1200 loss_tea:2.1324202018213105  loss_houyan:273.803466796875\n",
      "Number:3 epoch: 691/1200 loss_tea:2.1333967104915104  loss_houyan:567.523193359375\n",
      "Number:3 epoch: 701/1200 loss_tea:2.123473755051108  loss_houyan:257.2060852050781\n",
      "Number:3 epoch: 711/1200 loss_tea:2.1067570740788866  loss_houyan:324.93939208984375\n",
      "Number:3 epoch: 721/1200 loss_tea:2.14285749762116  loss_houyan:338.6488037109375\n",
      "Number:3 epoch: 731/1200 loss_tea:2.1664357005518613  loss_houyan:341.57757568359375\n",
      "Number:3 epoch: 741/1200 loss_tea:2.127745707142312  loss_houyan:228.71487426757812\n",
      "Number:3 epoch: 751/1200 loss_tea:2.1160142330974856  loss_houyan:284.25006103515625\n",
      "Number:3 epoch: 761/1200 loss_tea:2.149725441536689  loss_houyan:216.04608154296875\n",
      "Number:3 epoch: 771/1200 loss_tea:2.1393986463959243  loss_houyan:218.9767608642578\n",
      "Number:3 epoch: 781/1200 loss_tea:2.1375607480639816  loss_houyan:208.80886840820312\n",
      "Number:3 epoch: 791/1200 loss_tea:2.147058202156146  loss_houyan:294.01318359375\n",
      "Number:3 epoch: 801/1200 loss_tea:2.1343239790840545  loss_houyan:235.73001098632812\n",
      "Number:3 epoch: 811/1200 loss_tea:2.1519695998063137  loss_houyan:353.5439758300781\n",
      "Number:3 epoch: 821/1200 loss_tea:2.1336951229399053  loss_houyan:250.01412963867188\n",
      "Number:3 epoch: 831/1200 loss_tea:2.1237157589011537  loss_houyan:233.74169921875\n",
      "Number:3 epoch: 841/1200 loss_tea:2.097928662366108  loss_houyan:296.280029296875\n",
      "Number:3 epoch: 851/1200 loss_tea:2.1381273759690123  loss_houyan:230.80361938476562\n",
      "Number:3 epoch: 861/1200 loss_tea:2.1782173478892104  loss_houyan:266.242919921875\n",
      "Number:3 epoch: 871/1200 loss_tea:2.141784366686864  loss_houyan:273.65911865234375\n",
      "Number:3 epoch: 881/1200 loss_tea:2.1199802364032574  loss_houyan:362.9164123535156\n",
      "Number:3 epoch: 891/1200 loss_tea:2.119458743940175  loss_houyan:199.9646453857422\n",
      "Number:3 epoch: 901/1200 loss_tea:2.1530705763806934  loss_houyan:195.6071319580078\n",
      "Number:3 epoch: 911/1200 loss_tea:2.1379378381484933  loss_houyan:635.1578369140625\n",
      "Number:3 epoch: 921/1200 loss_tea:2.132585626489976  loss_houyan:339.37109375\n",
      "Number:3 epoch: 931/1200 loss_tea:2.1568788421195273  loss_houyan:420.2400207519531\n",
      "Number:3 epoch: 941/1200 loss_tea:2.1668060651699976  loss_houyan:244.76400756835938\n",
      "Number:3 epoch: 951/1200 loss_tea:2.1274335692910586  loss_houyan:267.0461120605469\n",
      "Number:3 epoch: 961/1200 loss_tea:2.176143900642758  loss_houyan:233.19398498535156\n",
      "Number:3 epoch: 971/1200 loss_tea:2.145160043033349  loss_houyan:288.25762939453125\n",
      "Number:3 epoch: 981/1200 loss_tea:2.1360921546249654  loss_houyan:572.58154296875\n",
      "Number:3 epoch: 991/1200 loss_tea:2.130091131434721  loss_houyan:757.923583984375\n",
      "Number:3 epoch: 1001/1200 loss_tea:2.134547124189489  loss_houyan:222.44931030273438\n",
      "Number:3 epoch: 1011/1200 loss_tea:2.130983223634608  loss_houyan:347.07012939453125\n",
      "Number:3 epoch: 1021/1200 loss_tea:2.1612038538117724  loss_houyan:229.3971710205078\n",
      "Number:3 epoch: 1031/1200 loss_tea:2.1421226901991557  loss_houyan:218.76214599609375\n",
      "Number:3 epoch: 1041/1200 loss_tea:2.143476403460783  loss_houyan:234.53933715820312\n",
      "Number:3 epoch: 1051/1200 loss_tea:2.165153329141412  loss_houyan:217.414306640625\n",
      "Number:3 epoch: 1061/1200 loss_tea:2.1417672783445734  loss_houyan:195.9267120361328\n",
      "Number:3 epoch: 1071/1200 loss_tea:2.174467262759753  loss_houyan:222.15863037109375\n",
      "Number:3 epoch: 1081/1200 loss_tea:2.1454057049998774  loss_houyan:418.42449951171875\n",
      "Number:3 epoch: 1091/1200 loss_tea:2.1519163179356333  loss_houyan:242.28311157226562\n",
      "Number:3 epoch: 1101/1200 loss_tea:2.1406581809363976  loss_houyan:197.2477569580078\n",
      "Number:3 epoch: 1111/1200 loss_tea:2.1379382550922643  loss_houyan:249.6289825439453\n",
      "Number:3 epoch: 1121/1200 loss_tea:2.155747245834773  loss_houyan:216.77903747558594\n",
      "Number:3 epoch: 1131/1200 loss_tea:2.1620567116357874  loss_houyan:210.21969604492188\n",
      "Number:3 epoch: 1141/1200 loss_tea:2.1246991401725164  loss_houyan:273.6485595703125\n",
      "Number:3 epoch: 1151/1200 loss_tea:2.144062291214623  loss_houyan:306.15863037109375\n",
      "Number:3 epoch: 1161/1200 loss_tea:2.145248479537898  loss_houyan:252.01162719726562\n",
      "Number:3 epoch: 1171/1200 loss_tea:2.1107999672320474  loss_houyan:209.55426025390625\n",
      "Number:3 epoch: 1181/1200 loss_tea:2.164012985905974  loss_houyan:289.4002685546875\n",
      "Number:3 epoch: 1191/1200 loss_tea:2.1195855294131905  loss_houyan:784.435546875\n",
      "finished training number 3 techer!\n",
      "start training number 4 techer!\n",
      "Number:4 epoch: 1/1200 loss_tea:2.352352777938117  loss_houyan:230.36447143554688\n",
      "Number:4 epoch: 11/1200 loss_tea:2.442079243445479  loss_houyan:246.3977813720703\n",
      "Number:4 epoch: 21/1200 loss_tea:2.432492640356704  loss_houyan:360.5242004394531\n",
      "Number:4 epoch: 31/1200 loss_tea:2.4497989732088925  loss_houyan:203.44473266601562\n",
      "Number:4 epoch: 41/1200 loss_tea:2.4008764441038086  loss_houyan:226.10409545898438\n",
      "Number:4 epoch: 51/1200 loss_tea:2.4131762705872215  loss_houyan:891.4952392578125\n",
      "Number:4 epoch: 61/1200 loss_tea:2.409136796946344  loss_houyan:207.03973388671875\n",
      "Number:4 epoch: 71/1200 loss_tea:2.3701651327337774  loss_houyan:251.7464599609375\n",
      "Number:4 epoch: 81/1200 loss_tea:2.4498117107421056  loss_houyan:234.1337127685547\n",
      "Number:4 epoch: 91/1200 loss_tea:2.41788804473349  loss_houyan:210.72055053710938\n",
      "Number:4 epoch: 101/1200 loss_tea:2.4280532968910506  loss_houyan:262.06378173828125\n",
      "Number:4 epoch: 111/1200 loss_tea:2.441160122993496  loss_houyan:255.01205444335938\n",
      "Number:4 epoch: 121/1200 loss_tea:2.4182650409355295  loss_houyan:362.4353332519531\n",
      "Number:4 epoch: 131/1200 loss_tea:2.4304669513834387  loss_houyan:218.06594848632812\n",
      "Number:4 epoch: 141/1200 loss_tea:2.4470718223743373  loss_houyan:198.41563415527344\n",
      "Number:4 epoch: 151/1200 loss_tea:2.436630965351646  loss_houyan:237.55584716796875\n",
      "Number:4 epoch: 161/1200 loss_tea:2.4162683587585767  loss_houyan:243.05038452148438\n",
      "Number:4 epoch: 171/1200 loss_tea:2.4408297197216524  loss_houyan:400.67755126953125\n",
      "Number:4 epoch: 181/1200 loss_tea:2.4240703080352204  loss_houyan:365.12701416015625\n",
      "Number:4 epoch: 191/1200 loss_tea:2.4642664529873013  loss_houyan:399.834716796875\n",
      "Number:4 epoch: 201/1200 loss_tea:2.4278854486439054  loss_houyan:217.6326141357422\n",
      "Number:4 epoch: 211/1200 loss_tea:2.445906127530398  loss_houyan:197.39984130859375\n",
      "Number:4 epoch: 221/1200 loss_tea:2.434473370887004  loss_houyan:197.64012145996094\n",
      "Number:4 epoch: 231/1200 loss_tea:2.395629786991331  loss_houyan:268.47296142578125\n",
      "Number:4 epoch: 241/1200 loss_tea:2.451466945958385  loss_houyan:227.57923889160156\n",
      "Number:4 epoch: 251/1200 loss_tea:2.4582819649917442  loss_houyan:337.83807373046875\n",
      "Number:4 epoch: 261/1200 loss_tea:2.403009797751285  loss_houyan:201.4525604248047\n",
      "Number:4 epoch: 271/1200 loss_tea:2.4162436381343326  loss_houyan:251.72305297851562\n",
      "Number:4 epoch: 281/1200 loss_tea:2.4404973228085  loss_houyan:328.14825439453125\n",
      "Number:4 epoch: 291/1200 loss_tea:2.449493565113899  loss_houyan:200.7974395751953\n",
      "Number:4 epoch: 301/1200 loss_tea:2.425560251651751  loss_houyan:447.94085693359375\n",
      "Number:4 epoch: 311/1200 loss_tea:2.4467289748076335  loss_houyan:329.481201171875\n",
      "Number:4 epoch: 321/1200 loss_tea:2.437654412576484  loss_houyan:295.4717712402344\n",
      "Number:4 epoch: 331/1200 loss_tea:2.4321583452521724  loss_houyan:276.9834289550781\n",
      "Number:4 epoch: 341/1200 loss_tea:2.4466200383064245  loss_houyan:409.8569030761719\n",
      "Number:4 epoch: 351/1200 loss_tea:2.411611317763279  loss_houyan:199.3422088623047\n",
      "Number:4 epoch: 361/1200 loss_tea:2.456970301961404  loss_houyan:341.7897644042969\n",
      "Number:4 epoch: 371/1200 loss_tea:2.3904889119950132  loss_houyan:422.0552978515625\n",
      "Number:4 epoch: 381/1200 loss_tea:2.4547254381707795  loss_houyan:273.10748291015625\n",
      "Number:4 epoch: 391/1200 loss_tea:2.422456660938923  loss_houyan:331.8475341796875\n",
      "Number:4 epoch: 401/1200 loss_tea:2.435587835641881  loss_houyan:322.9114990234375\n",
      "Number:4 epoch: 411/1200 loss_tea:2.4306879035329323  loss_houyan:223.3562774658203\n",
      "Number:4 epoch: 421/1200 loss_tea:2.4241019768698404  loss_houyan:401.8843078613281\n",
      "Number:4 epoch: 431/1200 loss_tea:2.4192377461694106  loss_houyan:442.4868469238281\n",
      "Number:4 epoch: 441/1200 loss_tea:2.4545107092824363  loss_houyan:260.7681884765625\n",
      "Number:4 epoch: 451/1200 loss_tea:2.409578176023226  loss_houyan:250.73358154296875\n",
      "Number:4 epoch: 461/1200 loss_tea:2.431041762078097  loss_houyan:202.12379455566406\n",
      "Number:4 epoch: 471/1200 loss_tea:2.416041790902821  loss_houyan:264.84759521484375\n",
      "Number:4 epoch: 481/1200 loss_tea:2.407517677524923  loss_houyan:262.7306213378906\n",
      "Number:4 epoch: 491/1200 loss_tea:2.4136445284714747  loss_houyan:314.2298583984375\n",
      "Number:4 epoch: 501/1200 loss_tea:2.4658343019782465  loss_houyan:243.578369140625\n",
      "Number:4 epoch: 511/1200 loss_tea:2.4298102921680598  loss_houyan:251.89431762695312\n",
      "Number:4 epoch: 521/1200 loss_tea:2.445171539511235  loss_houyan:238.1355438232422\n",
      "Number:4 epoch: 531/1200 loss_tea:2.430572250887597  loss_houyan:281.91131591796875\n",
      "Number:4 epoch: 541/1200 loss_tea:2.426374530307562  loss_houyan:249.02578735351562\n",
      "Number:4 epoch: 551/1200 loss_tea:2.4131974189287653  loss_houyan:216.4725341796875\n",
      "Number:4 epoch: 561/1200 loss_tea:2.451116018806774  loss_houyan:276.29669189453125\n",
      "Number:4 epoch: 571/1200 loss_tea:2.4334286915802212  loss_houyan:255.37753295898438\n",
      "Number:4 epoch: 581/1200 loss_tea:2.4129227017861337  loss_houyan:422.328125\n",
      "Number:4 epoch: 591/1200 loss_tea:2.4322695085334116  loss_houyan:277.9451904296875\n",
      "Number:4 epoch: 601/1200 loss_tea:2.465805627152994  loss_houyan:250.61842346191406\n",
      "Number:4 epoch: 611/1200 loss_tea:2.4405264715834885  loss_houyan:199.38485717773438\n",
      "Number:4 epoch: 621/1200 loss_tea:2.4285745120791002  loss_houyan:209.92147827148438\n",
      "Number:4 epoch: 631/1200 loss_tea:2.4415072579284853  loss_houyan:215.84361267089844\n",
      "Number:4 epoch: 641/1200 loss_tea:2.4185901177383213  loss_houyan:202.8512725830078\n",
      "Number:4 epoch: 651/1200 loss_tea:2.4287216068964104  loss_houyan:203.8728790283203\n",
      "Number:4 epoch: 661/1200 loss_tea:2.4326905745535985  loss_houyan:245.9488525390625\n",
      "Number:4 epoch: 671/1200 loss_tea:2.4199207888342515  loss_houyan:492.5968933105469\n",
      "Number:4 epoch: 681/1200 loss_tea:2.447025278076581  loss_houyan:264.465087890625\n",
      "Number:4 epoch: 691/1200 loss_tea:2.4454471550185786  loss_houyan:295.8514099121094\n",
      "Number:4 epoch: 701/1200 loss_tea:2.4413322092340066  loss_houyan:209.8190460205078\n",
      "Number:4 epoch: 711/1200 loss_tea:2.450625537506024  loss_houyan:247.79449462890625\n",
      "Number:4 epoch: 721/1200 loss_tea:2.455304594745273  loss_houyan:263.9146423339844\n",
      "Number:4 epoch: 731/1200 loss_tea:2.4288680604584902  loss_houyan:410.54681396484375\n",
      "Number:4 epoch: 741/1200 loss_tea:2.4346304663943585  loss_houyan:270.9527587890625\n",
      "Number:4 epoch: 751/1200 loss_tea:2.41660856738635  loss_houyan:386.66510009765625\n",
      "Number:4 epoch: 761/1200 loss_tea:2.426263345896579  loss_houyan:228.547607421875\n",
      "Number:4 epoch: 771/1200 loss_tea:2.4144900958843296  loss_houyan:204.6346893310547\n",
      "Number:4 epoch: 781/1200 loss_tea:2.4309216863968794  loss_houyan:258.57861328125\n",
      "Number:4 epoch: 791/1200 loss_tea:2.433139800860395  loss_houyan:233.22610473632812\n",
      "Number:4 epoch: 801/1200 loss_tea:2.4531931423192206  loss_houyan:213.1631317138672\n",
      "Number:4 epoch: 811/1200 loss_tea:2.431058411655954  loss_houyan:236.47157287597656\n",
      "Number:4 epoch: 821/1200 loss_tea:2.410465899800759  loss_houyan:243.0309600830078\n",
      "Number:4 epoch: 831/1200 loss_tea:2.434928986522978  loss_houyan:230.46389770507812\n",
      "Number:4 epoch: 841/1200 loss_tea:2.4287223999888012  loss_houyan:296.48388671875\n",
      "Number:4 epoch: 851/1200 loss_tea:2.425761753011327  loss_houyan:318.9041442871094\n",
      "Number:4 epoch: 861/1200 loss_tea:2.425595703421992  loss_houyan:198.35122680664062\n",
      "Number:4 epoch: 871/1200 loss_tea:2.422002064068012  loss_houyan:705.001220703125\n",
      "Number:4 epoch: 881/1200 loss_tea:2.4280208866488975  loss_houyan:328.65185546875\n",
      "Number:4 epoch: 891/1200 loss_tea:2.4224774447279405  loss_houyan:198.74681091308594\n",
      "Number:4 epoch: 901/1200 loss_tea:2.3860252250849583  loss_houyan:214.0784149169922\n",
      "Number:4 epoch: 911/1200 loss_tea:2.4412638266606312  loss_houyan:245.0620574951172\n",
      "Number:4 epoch: 921/1200 loss_tea:2.4211618966297297  loss_houyan:216.88430786132812\n",
      "Number:4 epoch: 931/1200 loss_tea:2.425348318159374  loss_houyan:201.90345764160156\n",
      "Number:4 epoch: 941/1200 loss_tea:2.4277703125996575  loss_houyan:236.38165283203125\n",
      "Number:4 epoch: 951/1200 loss_tea:2.459150591358594  loss_houyan:301.1466064453125\n",
      "Number:4 epoch: 961/1200 loss_tea:2.424591784328738  loss_houyan:440.2378234863281\n",
      "Number:4 epoch: 971/1200 loss_tea:2.426739711827473  loss_houyan:256.9432067871094\n",
      "Number:4 epoch: 981/1200 loss_tea:2.4041743788752177  loss_houyan:206.4414825439453\n",
      "Number:4 epoch: 991/1200 loss_tea:2.380827369821938  loss_houyan:712.7952270507812\n",
      "Number:4 epoch: 1001/1200 loss_tea:2.446785705229815  loss_houyan:203.4123077392578\n",
      "Number:4 epoch: 1011/1200 loss_tea:2.40829020503483  loss_houyan:270.6983642578125\n",
      "Number:4 epoch: 1021/1200 loss_tea:2.4498497662659746  loss_houyan:233.96559143066406\n",
      "Number:4 epoch: 1031/1200 loss_tea:2.369306038160225  loss_houyan:341.4801330566406\n",
      "Number:4 epoch: 1041/1200 loss_tea:2.4082871770363776  loss_houyan:239.55601501464844\n",
      "Number:4 epoch: 1051/1200 loss_tea:2.4338377415927637  loss_houyan:235.1821746826172\n",
      "Number:4 epoch: 1061/1200 loss_tea:2.4132897848076476  loss_houyan:192.18997192382812\n",
      "Number:4 epoch: 1071/1200 loss_tea:2.4500412660899045  loss_houyan:243.51405334472656\n",
      "Number:4 epoch: 1081/1200 loss_tea:2.4410611133261946  loss_houyan:408.36871337890625\n",
      "Number:4 epoch: 1091/1200 loss_tea:2.4072623535011055  loss_houyan:315.841552734375\n",
      "Number:4 epoch: 1101/1200 loss_tea:2.429936223459079  loss_houyan:327.1755065917969\n",
      "Number:4 epoch: 1111/1200 loss_tea:2.421371294181652  loss_houyan:225.95394897460938\n",
      "Number:4 epoch: 1121/1200 loss_tea:2.4151965227094077  loss_houyan:242.1350555419922\n",
      "Number:4 epoch: 1131/1200 loss_tea:2.4571111262463368  loss_houyan:210.103271484375\n",
      "Number:4 epoch: 1141/1200 loss_tea:2.44899762320271  loss_houyan:195.37283325195312\n",
      "Number:4 epoch: 1151/1200 loss_tea:2.3863349380377668  loss_houyan:234.63882446289062\n",
      "Number:4 epoch: 1161/1200 loss_tea:2.4238451999776505  loss_houyan:195.98936462402344\n",
      "Number:4 epoch: 1171/1200 loss_tea:2.4175147639013903  loss_houyan:424.5585021972656\n",
      "Number:4 epoch: 1181/1200 loss_tea:2.4471946379717657  loss_houyan:217.1581573486328\n",
      "Number:4 epoch: 1191/1200 loss_tea:2.4216117848986984  loss_houyan:954.6534423828125\n",
      "finished training number 4 techer!\n",
      "start training number 5 techer!\n",
      "Number:5 epoch: 1/1200 loss_tea:2.1593424266184904  loss_houyan:244.30674743652344\n",
      "Number:5 epoch: 11/1200 loss_tea:2.156504178624665  loss_houyan:707.7335205078125\n",
      "Number:5 epoch: 21/1200 loss_tea:2.140633704918066  loss_houyan:204.54359436035156\n",
      "Number:5 epoch: 31/1200 loss_tea:2.147302131133096  loss_houyan:250.16058349609375\n",
      "Number:5 epoch: 41/1200 loss_tea:2.1440683620611276  loss_houyan:235.56964111328125\n",
      "Number:5 epoch: 51/1200 loss_tea:2.1040839510392977  loss_houyan:180.68125915527344\n",
      "Number:5 epoch: 61/1200 loss_tea:2.118595624630014  loss_houyan:193.4144744873047\n",
      "Number:5 epoch: 71/1200 loss_tea:2.1241476251179785  loss_houyan:380.16839599609375\n",
      "Number:5 epoch: 81/1200 loss_tea:2.121218916949104  loss_houyan:310.60882568359375\n",
      "Number:5 epoch: 91/1200 loss_tea:2.137310469604281  loss_houyan:295.89141845703125\n",
      "Number:5 epoch: 101/1200 loss_tea:2.1601971743428585  loss_houyan:907.4155883789062\n",
      "Number:5 epoch: 111/1200 loss_tea:2.127439367234913  loss_houyan:215.45498657226562\n",
      "Number:5 epoch: 121/1200 loss_tea:2.1407498650072356  loss_houyan:193.55906677246094\n",
      "Number:5 epoch: 131/1200 loss_tea:2.1396252419709336  loss_houyan:339.1014404296875\n",
      "Number:5 epoch: 141/1200 loss_tea:2.1443367758424223  loss_houyan:200.9065399169922\n",
      "Number:5 epoch: 151/1200 loss_tea:2.1384489202994375  loss_houyan:199.34695434570312\n",
      "Number:5 epoch: 161/1200 loss_tea:2.1460858711321875  loss_houyan:205.06015014648438\n",
      "Number:5 epoch: 171/1200 loss_tea:2.0931873466729294  loss_houyan:227.89859008789062\n",
      "Number:5 epoch: 181/1200 loss_tea:2.0889795957552106  loss_houyan:217.65403747558594\n",
      "Number:5 epoch: 191/1200 loss_tea:2.1213587481257825  loss_houyan:239.492919921875\n",
      "Number:5 epoch: 201/1200 loss_tea:2.126688267285436  loss_houyan:209.9403533935547\n",
      "Number:5 epoch: 211/1200 loss_tea:2.121305682708648  loss_houyan:348.2379150390625\n",
      "Number:5 epoch: 221/1200 loss_tea:2.083760781106652  loss_houyan:251.83419799804688\n",
      "Number:5 epoch: 231/1200 loss_tea:2.120058692037853  loss_houyan:216.29156494140625\n",
      "Number:5 epoch: 241/1200 loss_tea:2.115332094525796  loss_houyan:208.53414916992188\n",
      "Number:5 epoch: 251/1200 loss_tea:2.099897239447465  loss_houyan:264.5003356933594\n",
      "Number:5 epoch: 261/1200 loss_tea:2.1466598192300763  loss_houyan:226.03085327148438\n",
      "Number:5 epoch: 271/1200 loss_tea:2.1353309981138238  loss_houyan:200.4668731689453\n",
      "Number:5 epoch: 281/1200 loss_tea:2.1275287476790403  loss_houyan:301.91064453125\n",
      "Number:5 epoch: 291/1200 loss_tea:2.1260001189980953  loss_houyan:186.97232055664062\n",
      "Number:5 epoch: 301/1200 loss_tea:2.1571594317479117  loss_houyan:336.59136962890625\n",
      "Number:5 epoch: 311/1200 loss_tea:2.1354182248297033  loss_houyan:268.6842346191406\n",
      "Number:5 epoch: 321/1200 loss_tea:2.081793884056456  loss_houyan:187.99713134765625\n",
      "Number:5 epoch: 331/1200 loss_tea:2.109788252325619  loss_houyan:279.4219970703125\n",
      "Number:5 epoch: 341/1200 loss_tea:2.123471822169413  loss_houyan:206.9375762939453\n",
      "Number:5 epoch: 351/1200 loss_tea:2.1448427102557517  loss_houyan:260.4301452636719\n",
      "Number:5 epoch: 361/1200 loss_tea:2.134814046575949  loss_houyan:200.81211853027344\n",
      "Number:5 epoch: 371/1200 loss_tea:2.0896830600850724  loss_houyan:231.3721160888672\n",
      "Number:5 epoch: 381/1200 loss_tea:2.1050559520721435  loss_houyan:195.3262481689453\n",
      "Number:5 epoch: 391/1200 loss_tea:2.1021331648513106  loss_houyan:266.0402526855469\n",
      "Number:5 epoch: 401/1200 loss_tea:2.1164585625836594  loss_houyan:545.3133544921875\n",
      "Number:5 epoch: 411/1200 loss_tea:2.1324143309081713  loss_houyan:230.90208435058594\n",
      "Number:5 epoch: 421/1200 loss_tea:2.0985321730065922  loss_houyan:463.18975830078125\n",
      "Number:5 epoch: 431/1200 loss_tea:2.1275567946962006  loss_houyan:197.22390747070312\n",
      "Number:5 epoch: 441/1200 loss_tea:2.124854166367475  loss_houyan:435.39117431640625\n",
      "Number:5 epoch: 451/1200 loss_tea:2.133265274247496  loss_houyan:213.47975158691406\n",
      "Number:5 epoch: 461/1200 loss_tea:2.129121058374953  loss_houyan:272.1495056152344\n",
      "Number:5 epoch: 471/1200 loss_tea:2.110295055069313  loss_houyan:278.53857421875\n",
      "Number:5 epoch: 481/1200 loss_tea:2.1258590703192053  loss_houyan:216.5411834716797\n",
      "Number:5 epoch: 491/1200 loss_tea:2.1120804416267105  loss_houyan:234.04086303710938\n",
      "Number:5 epoch: 501/1200 loss_tea:2.1303586553124823  loss_houyan:218.9385986328125\n",
      "Number:5 epoch: 511/1200 loss_tea:2.124630356494943  loss_houyan:320.11419677734375\n",
      "Number:5 epoch: 521/1200 loss_tea:2.1236791010133946  loss_houyan:219.1275177001953\n",
      "Number:5 epoch: 531/1200 loss_tea:2.1424420912785513  loss_houyan:403.381103515625\n",
      "Number:5 epoch: 541/1200 loss_tea:2.117044677206389  loss_houyan:198.79017639160156\n",
      "Number:5 epoch: 551/1200 loss_tea:2.128813698877513  loss_houyan:323.0988464355469\n",
      "Number:5 epoch: 561/1200 loss_tea:2.1451207820106957  loss_houyan:231.35980224609375\n",
      "Number:5 epoch: 571/1200 loss_tea:2.1094309493332175  loss_houyan:269.20013427734375\n",
      "Number:5 epoch: 581/1200 loss_tea:2.0882959718935217  loss_houyan:318.7362365722656\n",
      "Number:5 epoch: 591/1200 loss_tea:2.1032839341147134  loss_houyan:197.41529846191406\n",
      "Number:5 epoch: 601/1200 loss_tea:2.1611538291389967  loss_houyan:273.0022277832031\n",
      "Number:5 epoch: 611/1200 loss_tea:2.1096265726848458  loss_houyan:388.0550842285156\n",
      "Number:5 epoch: 621/1200 loss_tea:2.0935971616461204  loss_houyan:266.4263916015625\n",
      "Number:5 epoch: 631/1200 loss_tea:2.0878273528339952  loss_houyan:192.1540069580078\n",
      "Number:5 epoch: 641/1200 loss_tea:2.132512499923112  loss_houyan:260.1663818359375\n",
      "Number:5 epoch: 651/1200 loss_tea:2.1141560089216926  loss_houyan:236.16262817382812\n",
      "Number:5 epoch: 661/1200 loss_tea:2.1388801296895763  loss_houyan:194.0657196044922\n",
      "Number:5 epoch: 671/1200 loss_tea:2.095720206494975  loss_houyan:201.7587890625\n",
      "Number:5 epoch: 681/1200 loss_tea:2.1536682828074922  loss_houyan:253.4174346923828\n",
      "Number:5 epoch: 691/1200 loss_tea:2.135005050134494  loss_houyan:260.6080017089844\n",
      "Number:5 epoch: 701/1200 loss_tea:2.124269572715033  loss_houyan:239.15634155273438\n",
      "Number:5 epoch: 711/1200 loss_tea:2.123171400384507  loss_houyan:189.7017364501953\n",
      "Number:5 epoch: 721/1200 loss_tea:2.104722036374894  loss_houyan:189.1695098876953\n",
      "Number:5 epoch: 731/1200 loss_tea:2.0893111447561985  loss_houyan:246.63694763183594\n",
      "Number:5 epoch: 741/1200 loss_tea:2.1045297868524044  loss_houyan:217.46351623535156\n",
      "Number:5 epoch: 751/1200 loss_tea:2.1167800358422486  loss_houyan:223.70614624023438\n",
      "Number:5 epoch: 761/1200 loss_tea:2.1090832326651445  loss_houyan:228.8777618408203\n",
      "Number:5 epoch: 771/1200 loss_tea:2.157347967962905  loss_houyan:196.66514587402344\n",
      "Number:5 epoch: 781/1200 loss_tea:2.0737028171859397  loss_houyan:349.2073059082031\n",
      "Number:5 epoch: 791/1200 loss_tea:2.1179445304672613  loss_houyan:252.17510986328125\n",
      "Number:5 epoch: 801/1200 loss_tea:2.1502493828638203  loss_houyan:259.5303955078125\n",
      "Number:5 epoch: 811/1200 loss_tea:2.1113838027505314  loss_houyan:207.7896270751953\n",
      "Number:5 epoch: 821/1200 loss_tea:2.128172066714937  loss_houyan:226.88462829589844\n",
      "Number:5 epoch: 831/1200 loss_tea:2.1240932736223543  loss_houyan:233.16302490234375\n",
      "Number:5 epoch: 841/1200 loss_tea:2.118908842047193  loss_houyan:446.037109375\n",
      "Number:5 epoch: 851/1200 loss_tea:2.1490354876204756  loss_houyan:248.09019470214844\n",
      "Number:5 epoch: 861/1200 loss_tea:2.10106527520711  loss_houyan:294.8455810546875\n",
      "Number:5 epoch: 871/1200 loss_tea:2.1100783283735227  loss_houyan:333.3526611328125\n",
      "Number:5 epoch: 881/1200 loss_tea:2.128440527684961  loss_houyan:252.11460876464844\n",
      "Number:5 epoch: 891/1200 loss_tea:2.1146607521908507  loss_houyan:216.05426025390625\n",
      "Number:5 epoch: 901/1200 loss_tea:2.131869037351394  loss_houyan:208.3199462890625\n",
      "Number:5 epoch: 911/1200 loss_tea:2.0789866262653707  loss_houyan:267.9077453613281\n",
      "Number:5 epoch: 921/1200 loss_tea:2.1439316742972934  loss_houyan:290.7179870605469\n",
      "Number:5 epoch: 931/1200 loss_tea:2.1169845087511727  loss_houyan:272.8984375\n",
      "Number:5 epoch: 941/1200 loss_tea:2.1392193140868083  loss_houyan:188.65835571289062\n",
      "Number:5 epoch: 951/1200 loss_tea:2.097936007555793  loss_houyan:358.69744873046875\n",
      "Number:5 epoch: 961/1200 loss_tea:2.112208695972667  loss_houyan:227.85708618164062\n",
      "Number:5 epoch: 971/1200 loss_tea:2.0955504785978256  loss_houyan:206.64866638183594\n",
      "Number:5 epoch: 981/1200 loss_tea:2.1258580829032976  loss_houyan:331.5257263183594\n",
      "Number:5 epoch: 991/1200 loss_tea:2.1395358631767616  loss_houyan:351.01123046875\n",
      "Number:5 epoch: 1001/1200 loss_tea:2.1290977398829476  loss_houyan:225.70486450195312\n",
      "Number:5 epoch: 1011/1200 loss_tea:2.1082799933360934  loss_houyan:226.71556091308594\n",
      "Number:5 epoch: 1021/1200 loss_tea:2.108155022834824  loss_houyan:260.6129455566406\n",
      "Number:5 epoch: 1031/1200 loss_tea:2.1131968009843134  loss_houyan:316.07562255859375\n",
      "Number:5 epoch: 1041/1200 loss_tea:2.11315323229892  loss_houyan:236.54351806640625\n",
      "Number:5 epoch: 1051/1200 loss_tea:2.115447628993064  loss_houyan:208.41700744628906\n",
      "Number:5 epoch: 1061/1200 loss_tea:2.1179885877663702  loss_houyan:197.13555908203125\n",
      "Number:5 epoch: 1071/1200 loss_tea:2.1289301084399637  loss_houyan:207.71548461914062\n",
      "Number:5 epoch: 1081/1200 loss_tea:2.10943246234247  loss_houyan:203.0034637451172\n",
      "Number:5 epoch: 1091/1200 loss_tea:2.135018866883017  loss_houyan:224.88238525390625\n",
      "Number:5 epoch: 1101/1200 loss_tea:2.1192513097941257  loss_houyan:192.45654296875\n",
      "Number:5 epoch: 1111/1200 loss_tea:2.0859357312476345  loss_houyan:189.9466094970703\n",
      "Number:5 epoch: 1121/1200 loss_tea:2.1407842950424936  loss_houyan:231.8497314453125\n",
      "Number:5 epoch: 1131/1200 loss_tea:2.1585275742421928  loss_houyan:351.06365966796875\n",
      "Number:5 epoch: 1141/1200 loss_tea:2.1206549558672525  loss_houyan:233.7168731689453\n",
      "Number:5 epoch: 1151/1200 loss_tea:2.1178791063467113  loss_houyan:488.24517822265625\n",
      "Number:5 epoch: 1161/1200 loss_tea:2.1130383873480825  loss_houyan:242.85153198242188\n",
      "Number:5 epoch: 1171/1200 loss_tea:2.0890498839447655  loss_houyan:290.37066650390625\n",
      "Number:5 epoch: 1181/1200 loss_tea:2.1154648181888884  loss_houyan:288.32122802734375\n",
      "Number:5 epoch: 1191/1200 loss_tea:2.1409827059115507  loss_houyan:203.31919860839844\n",
      "finished training number 5 techer!\n",
      "start training number 6 techer!\n",
      "Number:6 epoch: 1/1200 loss_tea:2.2063387245867903  loss_houyan:260.61480712890625\n",
      "Number:6 epoch: 11/1200 loss_tea:2.2890228525577534  loss_houyan:398.7122497558594\n",
      "Number:6 epoch: 21/1200 loss_tea:2.2758569105273714  loss_houyan:187.21603393554688\n",
      "Number:6 epoch: 31/1200 loss_tea:2.2905688483409814  loss_houyan:257.697265625\n",
      "Number:6 epoch: 41/1200 loss_tea:2.259099500979519  loss_houyan:302.87420654296875\n",
      "Number:6 epoch: 51/1200 loss_tea:2.287276195902313  loss_houyan:276.6578369140625\n",
      "Number:6 epoch: 61/1200 loss_tea:2.2783060989990367  loss_houyan:281.4413146972656\n",
      "Number:6 epoch: 71/1200 loss_tea:2.3025268712258256  loss_houyan:210.51300048828125\n",
      "Number:6 epoch: 81/1200 loss_tea:2.3121752590456635  loss_houyan:253.66290283203125\n",
      "Number:6 epoch: 91/1200 loss_tea:2.2800205263712003  loss_houyan:213.4818115234375\n",
      "Number:6 epoch: 101/1200 loss_tea:2.3139270927254305  loss_houyan:215.4882049560547\n",
      "Number:6 epoch: 111/1200 loss_tea:2.310254369062536  loss_houyan:206.1612548828125\n",
      "Number:6 epoch: 121/1200 loss_tea:2.306087775234533  loss_houyan:191.0769805908203\n",
      "Number:6 epoch: 131/1200 loss_tea:2.285848678519569  loss_houyan:241.76988220214844\n",
      "Number:6 epoch: 141/1200 loss_tea:2.3251554600507744  loss_houyan:461.3299865722656\n",
      "Number:6 epoch: 151/1200 loss_tea:2.315947823045988  loss_houyan:579.3074951171875\n",
      "Number:6 epoch: 161/1200 loss_tea:2.3087021451920373  loss_houyan:213.79876708984375\n",
      "Number:6 epoch: 171/1200 loss_tea:2.271851890887356  loss_houyan:228.63803100585938\n",
      "Number:6 epoch: 181/1200 loss_tea:2.2634038862060097  loss_houyan:192.16632080078125\n",
      "Number:6 epoch: 191/1200 loss_tea:2.309096873961518  loss_houyan:319.4266052246094\n",
      "Number:6 epoch: 201/1200 loss_tea:2.278488502040454  loss_houyan:210.20254516601562\n",
      "Number:6 epoch: 211/1200 loss_tea:2.2885054843648494  loss_houyan:243.50987243652344\n",
      "Number:6 epoch: 221/1200 loss_tea:2.313726731526398  loss_houyan:258.9574279785156\n",
      "Number:6 epoch: 231/1200 loss_tea:2.307732969749345  loss_houyan:355.63079833984375\n",
      "Number:6 epoch: 241/1200 loss_tea:2.2821371828396013  loss_houyan:223.8609619140625\n",
      "Number:6 epoch: 251/1200 loss_tea:2.2804756980247562  loss_houyan:239.00830078125\n",
      "Number:6 epoch: 261/1200 loss_tea:2.324160875787372  loss_houyan:224.81484985351562\n",
      "Number:6 epoch: 271/1200 loss_tea:2.2949106699867645  loss_houyan:294.1510925292969\n",
      "Number:6 epoch: 281/1200 loss_tea:2.3121809612920954  loss_houyan:205.818603515625\n",
      "Number:6 epoch: 291/1200 loss_tea:2.287130932824422  loss_houyan:273.5531005859375\n",
      "Number:6 epoch: 301/1200 loss_tea:2.3032298467563512  loss_houyan:323.22637939453125\n",
      "Number:6 epoch: 311/1200 loss_tea:2.3159996944521537  loss_houyan:211.91041564941406\n",
      "Number:6 epoch: 321/1200 loss_tea:2.294129199552701  loss_houyan:232.17910766601562\n",
      "Number:6 epoch: 331/1200 loss_tea:2.324428240898159  loss_houyan:236.017578125\n",
      "Number:6 epoch: 341/1200 loss_tea:2.2773317780564812  loss_houyan:241.88023376464844\n",
      "Number:6 epoch: 351/1200 loss_tea:2.3095060910320613  loss_houyan:334.6424560546875\n",
      "Number:6 epoch: 361/1200 loss_tea:2.2895262896395883  loss_houyan:253.0840606689453\n",
      "Number:6 epoch: 371/1200 loss_tea:2.307216387904639  loss_houyan:224.16558837890625\n",
      "Number:6 epoch: 381/1200 loss_tea:2.2721485799159145  loss_houyan:241.47886657714844\n",
      "Number:6 epoch: 391/1200 loss_tea:2.289587804437921  loss_houyan:218.06842041015625\n",
      "Number:6 epoch: 401/1200 loss_tea:2.2991602677787464  loss_houyan:309.02264404296875\n",
      "Number:6 epoch: 411/1200 loss_tea:2.314102490633004  loss_houyan:234.0967559814453\n",
      "Number:6 epoch: 421/1200 loss_tea:2.323631546984082  loss_houyan:247.57174682617188\n",
      "Number:6 epoch: 431/1200 loss_tea:2.3083278186593503  loss_houyan:227.15821838378906\n",
      "Number:6 epoch: 441/1200 loss_tea:2.3116559628796827  loss_houyan:201.7813262939453\n",
      "Number:6 epoch: 451/1200 loss_tea:2.3176432535310103  loss_houyan:273.3437805175781\n",
      "Number:6 epoch: 461/1200 loss_tea:2.3242651294259464  loss_houyan:214.97900390625\n",
      "Number:6 epoch: 471/1200 loss_tea:2.316728057828329  loss_houyan:365.4080810546875\n",
      "Number:6 epoch: 481/1200 loss_tea:2.280172193627869  loss_houyan:195.79061889648438\n",
      "Number:6 epoch: 491/1200 loss_tea:2.299702873444475  loss_houyan:360.9462890625\n",
      "Number:6 epoch: 501/1200 loss_tea:2.284066426341509  loss_houyan:207.378662109375\n",
      "Number:6 epoch: 511/1200 loss_tea:2.2913163485411543  loss_houyan:210.99378967285156\n",
      "Number:6 epoch: 521/1200 loss_tea:2.272992211312159  loss_houyan:309.79254150390625\n",
      "Number:6 epoch: 531/1200 loss_tea:2.264642444036411  loss_houyan:257.9071960449219\n",
      "Number:6 epoch: 541/1200 loss_tea:2.30567611468292  loss_houyan:208.38230895996094\n",
      "Number:6 epoch: 551/1200 loss_tea:2.298103958017686  loss_houyan:261.4601135253906\n",
      "Number:6 epoch: 561/1200 loss_tea:2.3156831653060386  loss_houyan:301.9889221191406\n",
      "Number:6 epoch: 571/1200 loss_tea:2.2785978540004743  loss_houyan:525.8350219726562\n",
      "Number:6 epoch: 581/1200 loss_tea:2.269796982603502  loss_houyan:240.2298583984375\n",
      "Number:6 epoch: 591/1200 loss_tea:2.2885116399778216  loss_houyan:365.4986877441406\n",
      "Number:6 epoch: 601/1200 loss_tea:2.313193231552942  loss_houyan:250.14881896972656\n",
      "Number:6 epoch: 611/1200 loss_tea:2.2694374397964214  loss_houyan:379.97528076171875\n",
      "Number:6 epoch: 621/1200 loss_tea:2.267285381963921  loss_houyan:263.716796875\n",
      "Number:6 epoch: 631/1200 loss_tea:2.3580395609449765  loss_houyan:397.88763427734375\n",
      "Number:6 epoch: 641/1200 loss_tea:2.279932955227096  loss_houyan:210.58209228515625\n",
      "Number:6 epoch: 651/1200 loss_tea:2.3313767032227304  loss_houyan:228.230712890625\n",
      "Number:6 epoch: 661/1200 loss_tea:2.290815162823687  loss_houyan:232.52169799804688\n",
      "Number:6 epoch: 671/1200 loss_tea:2.2881173577688143  loss_houyan:224.05235290527344\n",
      "Number:6 epoch: 681/1200 loss_tea:2.282354152738842  loss_houyan:224.1357421875\n",
      "Number:6 epoch: 691/1200 loss_tea:2.3125823717216307  loss_houyan:477.6199951171875\n",
      "Number:6 epoch: 701/1200 loss_tea:2.313231916699855  loss_houyan:342.529296875\n",
      "Number:6 epoch: 711/1200 loss_tea:2.2740999706888694  loss_houyan:366.4366455078125\n",
      "Number:6 epoch: 721/1200 loss_tea:2.3215443965885467  loss_houyan:216.23016357421875\n",
      "Number:6 epoch: 731/1200 loss_tea:2.284467102169578  loss_houyan:195.75001525878906\n",
      "Number:6 epoch: 741/1200 loss_tea:2.2665920522798717  loss_houyan:249.6730194091797\n",
      "Number:6 epoch: 751/1200 loss_tea:2.2875335812156177  loss_houyan:217.1778564453125\n",
      "Number:6 epoch: 761/1200 loss_tea:2.2768927372863135  loss_houyan:219.1846466064453\n",
      "Number:6 epoch: 771/1200 loss_tea:2.280275587350852  loss_houyan:218.7302703857422\n",
      "Number:6 epoch: 781/1200 loss_tea:2.3051088599597707  loss_houyan:253.41265869140625\n",
      "Number:6 epoch: 791/1200 loss_tea:2.3208949527113494  loss_houyan:233.4285430908203\n",
      "Number:6 epoch: 801/1200 loss_tea:2.273539253419658  loss_houyan:285.3252258300781\n",
      "Number:6 epoch: 811/1200 loss_tea:2.2894248456690964  loss_houyan:312.3304138183594\n",
      "Number:6 epoch: 821/1200 loss_tea:2.287053393657645  loss_houyan:193.03932189941406\n",
      "Number:6 epoch: 831/1200 loss_tea:2.3018961381747234  loss_houyan:174.1770782470703\n",
      "Number:6 epoch: 841/1200 loss_tea:2.2828327890498414  loss_houyan:206.5996551513672\n",
      "Number:6 epoch: 851/1200 loss_tea:2.302802546049072  loss_houyan:437.523681640625\n",
      "Number:6 epoch: 861/1200 loss_tea:2.3034776024339934  loss_houyan:312.61676025390625\n",
      "Number:6 epoch: 871/1200 loss_tea:2.305033767594598  loss_houyan:266.2933349609375\n",
      "Number:6 epoch: 881/1200 loss_tea:2.3309103087982916  loss_houyan:245.04556274414062\n",
      "Number:6 epoch: 891/1200 loss_tea:2.3157896060844605  loss_houyan:200.9150390625\n",
      "Number:6 epoch: 901/1200 loss_tea:2.265425820004156  loss_houyan:191.69735717773438\n",
      "Number:6 epoch: 911/1200 loss_tea:2.268744300146004  loss_houyan:223.75628662109375\n",
      "Number:6 epoch: 921/1200 loss_tea:2.299601754515229  loss_houyan:202.9507598876953\n",
      "Number:6 epoch: 931/1200 loss_tea:2.3208476427929625  loss_houyan:265.65185546875\n",
      "Number:6 epoch: 941/1200 loss_tea:2.283346912192638  loss_houyan:273.98626708984375\n",
      "Number:6 epoch: 951/1200 loss_tea:2.299741463553947  loss_houyan:233.77793884277344\n",
      "Number:6 epoch: 961/1200 loss_tea:2.3080689826226153  loss_houyan:252.89544677734375\n",
      "Number:6 epoch: 971/1200 loss_tea:2.3068527836288135  loss_houyan:226.87071228027344\n",
      "Number:6 epoch: 981/1200 loss_tea:2.270285151862768  loss_houyan:220.1598358154297\n",
      "Number:6 epoch: 991/1200 loss_tea:2.278245836310733  loss_houyan:186.70074462890625\n",
      "Number:6 epoch: 1001/1200 loss_tea:2.2972842916072858  loss_houyan:241.93099975585938\n",
      "Number:6 epoch: 1011/1200 loss_tea:2.314292320908147  loss_houyan:198.70785522460938\n",
      "Number:6 epoch: 1021/1200 loss_tea:2.2984806362732884  loss_houyan:188.65235900878906\n",
      "Number:6 epoch: 1031/1200 loss_tea:2.324537004484025  loss_houyan:233.42018127441406\n",
      "Number:6 epoch: 1041/1200 loss_tea:2.26207336174988  loss_houyan:341.0389404296875\n",
      "Number:6 epoch: 1051/1200 loss_tea:2.3139621592722963  loss_houyan:222.12615966796875\n",
      "Number:6 epoch: 1061/1200 loss_tea:2.320314726821279  loss_houyan:237.20956420898438\n",
      "Number:6 epoch: 1071/1200 loss_tea:2.3106595174664033  loss_houyan:211.90574645996094\n",
      "Number:6 epoch: 1081/1200 loss_tea:2.3221908589960383  loss_houyan:296.17608642578125\n",
      "Number:6 epoch: 1091/1200 loss_tea:2.2793387820563926  loss_houyan:346.6352233886719\n",
      "Number:6 epoch: 1101/1200 loss_tea:2.3073237795318287  loss_houyan:256.2745056152344\n",
      "Number:6 epoch: 1111/1200 loss_tea:2.2878134140093844  loss_houyan:321.5506286621094\n",
      "Number:6 epoch: 1121/1200 loss_tea:2.290578564310569  loss_houyan:249.46473693847656\n",
      "Number:6 epoch: 1131/1200 loss_tea:2.3367225829292746  loss_houyan:241.934326171875\n",
      "Number:6 epoch: 1141/1200 loss_tea:2.345220943553225  loss_houyan:260.6890869140625\n",
      "Number:6 epoch: 1151/1200 loss_tea:2.3075084643380452  loss_houyan:205.5395965576172\n",
      "Number:6 epoch: 1161/1200 loss_tea:2.269563873086421  loss_houyan:233.36184692382812\n",
      "Number:6 epoch: 1171/1200 loss_tea:2.2775496816140146  loss_houyan:293.462890625\n",
      "Number:6 epoch: 1181/1200 loss_tea:2.31894250757554  loss_houyan:217.9779052734375\n",
      "Number:6 epoch: 1191/1200 loss_tea:2.282820947112509  loss_houyan:241.42269897460938\n",
      "finished training number 6 techer!\n",
      "start training number 7 techer!\n",
      "Number:7 epoch: 1/1200 loss_tea:2.2041795087314395  loss_houyan:209.5635528564453\n",
      "Number:7 epoch: 11/1200 loss_tea:2.175560421267183  loss_houyan:190.64593505859375\n",
      "Number:7 epoch: 21/1200 loss_tea:2.157258581042702  loss_houyan:308.45391845703125\n",
      "Number:7 epoch: 31/1200 loss_tea:2.1728741807508634  loss_houyan:415.543701171875\n",
      "Number:7 epoch: 41/1200 loss_tea:2.1755567819601938  loss_houyan:354.165283203125\n",
      "Number:7 epoch: 51/1200 loss_tea:2.201182607795953  loss_houyan:366.08392333984375\n",
      "Number:7 epoch: 61/1200 loss_tea:2.1664371780870697  loss_houyan:411.02862548828125\n",
      "Number:7 epoch: 71/1200 loss_tea:2.1598039003391993  loss_houyan:299.80010986328125\n",
      "Number:7 epoch: 81/1200 loss_tea:2.15078219352709  loss_houyan:232.89706420898438\n",
      "Number:7 epoch: 91/1200 loss_tea:2.135615347660949  loss_houyan:647.428955078125\n",
      "Number:7 epoch: 101/1200 loss_tea:2.1660867156454437  loss_houyan:305.2207336425781\n",
      "Number:7 epoch: 111/1200 loss_tea:2.169450940846572  loss_houyan:273.7550354003906\n",
      "Number:7 epoch: 121/1200 loss_tea:2.1535329946597144  loss_houyan:266.19940185546875\n",
      "Number:7 epoch: 131/1200 loss_tea:2.1736794742333436  loss_houyan:267.8370666503906\n",
      "Number:7 epoch: 141/1200 loss_tea:2.1431696707814623  loss_houyan:275.78668212890625\n",
      "Number:7 epoch: 151/1200 loss_tea:2.153013963435348  loss_houyan:414.70709228515625\n",
      "Number:7 epoch: 161/1200 loss_tea:2.185041879823876  loss_houyan:304.9538879394531\n",
      "Number:7 epoch: 171/1200 loss_tea:2.153208778638741  loss_houyan:217.95501708984375\n",
      "Number:7 epoch: 181/1200 loss_tea:2.182919796115387  loss_houyan:438.6282958984375\n",
      "Number:7 epoch: 191/1200 loss_tea:2.1727274794479556  loss_houyan:275.6170959472656\n",
      "Number:7 epoch: 201/1200 loss_tea:2.1507163219385905  loss_houyan:278.3438720703125\n",
      "Number:7 epoch: 211/1200 loss_tea:2.1611393976376543  loss_houyan:278.6053466796875\n",
      "Number:7 epoch: 221/1200 loss_tea:2.1807464632608484  loss_houyan:210.4253692626953\n",
      "Number:7 epoch: 231/1200 loss_tea:2.159214503666109  loss_houyan:261.1666564941406\n",
      "Number:7 epoch: 241/1200 loss_tea:2.1725215707270746  loss_houyan:235.03485107421875\n",
      "Number:7 epoch: 251/1200 loss_tea:2.1629625185550703  loss_houyan:345.75177001953125\n",
      "Number:7 epoch: 261/1200 loss_tea:2.168325709512902  loss_houyan:334.3758544921875\n",
      "Number:7 epoch: 271/1200 loss_tea:2.168992906540735  loss_houyan:305.5626220703125\n",
      "Number:7 epoch: 281/1200 loss_tea:2.1680318975531105  loss_houyan:200.06924438476562\n",
      "Number:7 epoch: 291/1200 loss_tea:2.1908149367797747  loss_houyan:234.06851196289062\n",
      "Number:7 epoch: 301/1200 loss_tea:2.191543323028459  loss_houyan:260.89678955078125\n",
      "Number:7 epoch: 311/1200 loss_tea:2.1815518521107604  loss_houyan:306.6165466308594\n",
      "Number:7 epoch: 321/1200 loss_tea:2.190708505983584  loss_houyan:234.49679565429688\n",
      "Number:7 epoch: 331/1200 loss_tea:2.163945577878853  loss_houyan:249.7525634765625\n",
      "Number:7 epoch: 341/1200 loss_tea:2.1842143753408148  loss_houyan:233.80096435546875\n",
      "Number:7 epoch: 351/1200 loss_tea:2.1414804309297186  loss_houyan:204.81629943847656\n",
      "Number:7 epoch: 361/1200 loss_tea:2.1625596889987535  loss_houyan:240.06871032714844\n",
      "Number:7 epoch: 371/1200 loss_tea:2.196658388182366  loss_houyan:229.54986572265625\n",
      "Number:7 epoch: 381/1200 loss_tea:2.153130084503068  loss_houyan:219.15005493164062\n",
      "Number:7 epoch: 391/1200 loss_tea:2.1927896408886234  loss_houyan:254.09735107421875\n",
      "Number:7 epoch: 401/1200 loss_tea:2.1527596124728245  loss_houyan:308.487548828125\n",
      "Number:7 epoch: 411/1200 loss_tea:2.1854111633498774  loss_houyan:209.12542724609375\n",
      "Number:7 epoch: 421/1200 loss_tea:2.1829057261193086  loss_houyan:708.2060546875\n",
      "Number:7 epoch: 431/1200 loss_tea:2.1493077631227697  loss_houyan:187.2086639404297\n",
      "Number:7 epoch: 441/1200 loss_tea:2.156129950883067  loss_houyan:296.04833984375\n",
      "Number:7 epoch: 451/1200 loss_tea:2.203220280685227  loss_houyan:270.1919860839844\n",
      "Number:7 epoch: 461/1200 loss_tea:2.1777864852166093  loss_houyan:316.531982421875\n",
      "Number:7 epoch: 471/1200 loss_tea:2.1673467632808485  loss_houyan:549.4237670898438\n",
      "Number:7 epoch: 481/1200 loss_tea:2.1596137747220103  loss_houyan:215.1187744140625\n",
      "Number:7 epoch: 491/1200 loss_tea:2.1629913814340083  loss_houyan:239.70298767089844\n",
      "Number:7 epoch: 501/1200 loss_tea:2.1485048541560716  loss_houyan:321.0670166015625\n",
      "Number:7 epoch: 511/1200 loss_tea:2.187613052975348  loss_houyan:316.6044921875\n",
      "Number:7 epoch: 521/1200 loss_tea:2.144959656672494  loss_houyan:515.244384765625\n",
      "Number:7 epoch: 531/1200 loss_tea:2.146290485421679  loss_houyan:270.13885498046875\n",
      "Number:7 epoch: 541/1200 loss_tea:2.172250274102168  loss_houyan:221.18545532226562\n",
      "Number:7 epoch: 551/1200 loss_tea:2.19629307902808  loss_houyan:222.7053985595703\n",
      "Number:7 epoch: 561/1200 loss_tea:2.1666371421417976  loss_houyan:258.6568298339844\n",
      "Number:7 epoch: 571/1200 loss_tea:2.2006662461171924  loss_houyan:1032.7591552734375\n",
      "Number:7 epoch: 581/1200 loss_tea:2.162876063647155  loss_houyan:252.77207946777344\n",
      "Number:7 epoch: 591/1200 loss_tea:2.1668605992538295  loss_houyan:304.4002380371094\n",
      "Number:7 epoch: 601/1200 loss_tea:2.124900025397436  loss_houyan:283.7603454589844\n",
      "Number:7 epoch: 611/1200 loss_tea:2.1608507937110417  loss_houyan:216.1165771484375\n",
      "Number:7 epoch: 621/1200 loss_tea:2.1865753059981192  loss_houyan:280.29876708984375\n",
      "Number:7 epoch: 631/1200 loss_tea:2.173192088579224  loss_houyan:276.8756103515625\n",
      "Number:7 epoch: 641/1200 loss_tea:2.185521456403303  loss_houyan:206.5736541748047\n",
      "Number:7 epoch: 651/1200 loss_tea:2.1603986881183506  loss_houyan:239.88987731933594\n",
      "Number:7 epoch: 661/1200 loss_tea:2.1730785132279444  loss_houyan:1076.8212890625\n",
      "Number:7 epoch: 671/1200 loss_tea:2.1311356448797207  loss_houyan:185.31996154785156\n",
      "Number:7 epoch: 681/1200 loss_tea:2.165401732756605  loss_houyan:206.87930297851562\n",
      "Number:7 epoch: 691/1200 loss_tea:2.184034011603227  loss_houyan:346.32208251953125\n",
      "Number:7 epoch: 701/1200 loss_tea:2.173388703471649  loss_houyan:247.68045043945312\n",
      "Number:7 epoch: 711/1200 loss_tea:2.157268910259524  loss_houyan:217.20794677734375\n",
      "Number:7 epoch: 721/1200 loss_tea:2.1523119666997124  loss_houyan:280.5506591796875\n",
      "Number:7 epoch: 731/1200 loss_tea:2.149521796843585  loss_houyan:441.8090515136719\n",
      "Number:7 epoch: 741/1200 loss_tea:2.1662501932427958  loss_houyan:457.6816101074219\n",
      "Number:7 epoch: 751/1200 loss_tea:2.17550908091984  loss_houyan:432.69085693359375\n",
      "Number:7 epoch: 761/1200 loss_tea:2.1822275458735167  loss_houyan:417.1544494628906\n",
      "Number:7 epoch: 771/1200 loss_tea:2.183527097900021  loss_houyan:330.05926513671875\n",
      "Number:7 epoch: 781/1200 loss_tea:2.178377459700957  loss_houyan:347.0599365234375\n",
      "Number:7 epoch: 791/1200 loss_tea:2.1453101016245912  loss_houyan:239.56842041015625\n",
      "Number:7 epoch: 801/1200 loss_tea:2.1816195718969853  loss_houyan:656.5547485351562\n",
      "Number:7 epoch: 811/1200 loss_tea:2.1512734028707325  loss_houyan:630.2618408203125\n",
      "Number:7 epoch: 821/1200 loss_tea:2.175275733974153  loss_houyan:258.2503356933594\n",
      "Number:7 epoch: 831/1200 loss_tea:2.1578181212336136  loss_houyan:722.1781005859375\n",
      "Number:7 epoch: 841/1200 loss_tea:2.16417777224808  loss_houyan:209.71112060546875\n",
      "Number:7 epoch: 851/1200 loss_tea:2.1577024022600644  loss_houyan:406.11962890625\n",
      "Number:7 epoch: 861/1200 loss_tea:2.175517689068012  loss_houyan:206.2733917236328\n",
      "Number:7 epoch: 871/1200 loss_tea:2.1799737273615536  loss_houyan:273.69561767578125\n",
      "Number:7 epoch: 881/1200 loss_tea:2.1699974075733173  loss_houyan:301.9912109375\n",
      "Number:7 epoch: 891/1200 loss_tea:2.1522958578947917  loss_houyan:239.936767578125\n",
      "Number:7 epoch: 901/1200 loss_tea:2.191940805573777  loss_houyan:264.8624267578125\n",
      "Number:7 epoch: 911/1200 loss_tea:2.1836270088143004  loss_houyan:280.255126953125\n",
      "Number:7 epoch: 921/1200 loss_tea:2.1567841603269216  loss_houyan:280.410888671875\n",
      "Number:7 epoch: 931/1200 loss_tea:2.148852411149695  loss_houyan:243.4998779296875\n",
      "Number:7 epoch: 941/1200 loss_tea:2.201991117825558  loss_houyan:206.42515563964844\n",
      "Number:7 epoch: 951/1200 loss_tea:2.1811491646981156  loss_houyan:244.66015625\n",
      "Number:7 epoch: 961/1200 loss_tea:2.1834295957971195  loss_houyan:211.05703735351562\n",
      "Number:7 epoch: 971/1200 loss_tea:2.2009364915966576  loss_houyan:278.496826171875\n",
      "Number:7 epoch: 981/1200 loss_tea:2.18498093958132  loss_houyan:206.85130310058594\n",
      "Number:7 epoch: 991/1200 loss_tea:2.2109386749333577  loss_houyan:230.27438354492188\n",
      "Number:7 epoch: 1001/1200 loss_tea:2.174438257922763  loss_houyan:254.86114501953125\n",
      "Number:7 epoch: 1011/1200 loss_tea:2.193588872269363  loss_houyan:301.9126281738281\n",
      "Number:7 epoch: 1021/1200 loss_tea:2.1472789058223314  loss_houyan:256.78411865234375\n",
      "Number:7 epoch: 1031/1200 loss_tea:2.149089936917216  loss_houyan:237.64830017089844\n",
      "Number:7 epoch: 1041/1200 loss_tea:2.1673117378591256  loss_houyan:344.9831848144531\n",
      "Number:7 epoch: 1051/1200 loss_tea:2.1781058823773605  loss_houyan:269.0521240234375\n",
      "Number:7 epoch: 1061/1200 loss_tea:2.1857687255502984  loss_houyan:201.46286010742188\n",
      "Number:7 epoch: 1071/1200 loss_tea:2.1622198122182934  loss_houyan:250.7505340576172\n",
      "Number:7 epoch: 1081/1200 loss_tea:2.196215885815736  loss_houyan:193.759521484375\n",
      "Number:7 epoch: 1091/1200 loss_tea:2.193327768121211  loss_houyan:221.2900848388672\n",
      "Number:7 epoch: 1101/1200 loss_tea:2.1888836279047403  loss_houyan:271.1512145996094\n",
      "Number:7 epoch: 1111/1200 loss_tea:2.1664156532617587  loss_houyan:416.0577392578125\n",
      "Number:7 epoch: 1121/1200 loss_tea:2.1680931414699884  loss_houyan:631.2784423828125\n",
      "Number:7 epoch: 1131/1200 loss_tea:2.1840309601754053  loss_houyan:263.1669921875\n",
      "Number:7 epoch: 1141/1200 loss_tea:2.166261383942667  loss_houyan:350.42138671875\n",
      "Number:7 epoch: 1151/1200 loss_tea:2.1632614493782545  loss_houyan:247.43051147460938\n",
      "Number:7 epoch: 1161/1200 loss_tea:2.174111757476437  loss_houyan:449.3858337402344\n",
      "Number:7 epoch: 1171/1200 loss_tea:2.1476870851120733  loss_houyan:250.0162353515625\n",
      "Number:7 epoch: 1181/1200 loss_tea:2.16288684907669  loss_houyan:186.50567626953125\n",
      "Number:7 epoch: 1191/1200 loss_tea:2.1908814836125887  loss_houyan:222.60089111328125\n",
      "finished training number 7 techer!\n",
      "start training number 8 techer!\n",
      "Number:8 epoch: 1/1200 loss_tea:2.26957581950719  loss_houyan:280.74249267578125\n",
      "Number:8 epoch: 11/1200 loss_tea:2.2565244163196394  loss_houyan:218.46429443359375\n",
      "Number:8 epoch: 21/1200 loss_tea:2.2348391674794126  loss_houyan:195.1563720703125\n",
      "Number:8 epoch: 31/1200 loss_tea:2.2506785123818474  loss_houyan:344.2433776855469\n",
      "Number:8 epoch: 41/1200 loss_tea:2.236953613419846  loss_houyan:486.3419494628906\n",
      "Number:8 epoch: 51/1200 loss_tea:2.2439012157875773  loss_houyan:273.7049865722656\n",
      "Number:8 epoch: 61/1200 loss_tea:2.2274918782463535  loss_houyan:220.306884765625\n",
      "Number:8 epoch: 71/1200 loss_tea:2.256974860819566  loss_houyan:203.93209838867188\n",
      "Number:8 epoch: 81/1200 loss_tea:2.197168225483086  loss_houyan:209.7569580078125\n",
      "Number:8 epoch: 91/1200 loss_tea:2.2643764482649966  loss_houyan:221.5477294921875\n",
      "Number:8 epoch: 101/1200 loss_tea:2.2749908604011404  loss_houyan:203.8076171875\n",
      "Number:8 epoch: 111/1200 loss_tea:2.209767160283653  loss_houyan:260.71728515625\n",
      "Number:8 epoch: 121/1200 loss_tea:2.2307453282976644  loss_houyan:213.53799438476562\n",
      "Number:8 epoch: 131/1200 loss_tea:2.237584153921134  loss_houyan:248.62461853027344\n",
      "Number:8 epoch: 141/1200 loss_tea:2.23597490086275  loss_houyan:248.03785705566406\n",
      "Number:8 epoch: 151/1200 loss_tea:2.23588769382259  loss_houyan:223.5416259765625\n",
      "Number:8 epoch: 161/1200 loss_tea:2.1889549200510072  loss_houyan:207.73843383789062\n",
      "Number:8 epoch: 171/1200 loss_tea:2.2389150962697593  loss_houyan:339.9176940917969\n",
      "Number:8 epoch: 181/1200 loss_tea:2.2396476100472844  loss_houyan:213.2668914794922\n",
      "Number:8 epoch: 191/1200 loss_tea:2.2143521431820616  loss_houyan:247.66856384277344\n",
      "Number:8 epoch: 201/1200 loss_tea:2.1912347855039944  loss_houyan:209.7023468017578\n",
      "Number:8 epoch: 211/1200 loss_tea:2.265789161338938  loss_houyan:365.0286560058594\n",
      "Number:8 epoch: 221/1200 loss_tea:2.265944681118104  loss_houyan:202.85536193847656\n",
      "Number:8 epoch: 231/1200 loss_tea:2.2453262092951674  loss_houyan:196.9682159423828\n",
      "Number:8 epoch: 241/1200 loss_tea:2.2561738383811236  loss_houyan:250.51513671875\n",
      "Number:8 epoch: 251/1200 loss_tea:2.2424507787895864  loss_houyan:222.17709350585938\n",
      "Number:8 epoch: 261/1200 loss_tea:2.2238359745811014  loss_houyan:191.4753875732422\n",
      "Number:8 epoch: 271/1200 loss_tea:2.2347921717950627  loss_houyan:344.80462646484375\n",
      "Number:8 epoch: 281/1200 loss_tea:2.2559521179298216  loss_houyan:216.0343017578125\n",
      "Number:8 epoch: 291/1200 loss_tea:2.259916025379537  loss_houyan:371.97991943359375\n",
      "Number:8 epoch: 301/1200 loss_tea:2.2616744334309984  loss_houyan:251.41989135742188\n",
      "Number:8 epoch: 311/1200 loss_tea:2.228806242233329  loss_houyan:265.077392578125\n",
      "Number:8 epoch: 321/1200 loss_tea:2.227813169700464  loss_houyan:789.85009765625\n",
      "Number:8 epoch: 331/1200 loss_tea:2.227678010641085  loss_houyan:211.27777099609375\n",
      "Number:8 epoch: 341/1200 loss_tea:2.225130768960735  loss_houyan:222.75634765625\n",
      "Number:8 epoch: 351/1200 loss_tea:2.2437481534522297  loss_houyan:257.75067138671875\n",
      "Number:8 epoch: 361/1200 loss_tea:2.2473040984988626  loss_houyan:233.079345703125\n",
      "Number:8 epoch: 371/1200 loss_tea:2.2288981088717503  loss_houyan:300.5865783691406\n",
      "Number:8 epoch: 381/1200 loss_tea:2.23383092434761  loss_houyan:311.497314453125\n",
      "Number:8 epoch: 391/1200 loss_tea:2.2508743117837344  loss_houyan:234.8804168701172\n",
      "Number:8 epoch: 401/1200 loss_tea:2.19926396076242  loss_houyan:210.38735961914062\n",
      "Number:8 epoch: 411/1200 loss_tea:2.2299159198483793  loss_houyan:242.6089324951172\n",
      "Number:8 epoch: 421/1200 loss_tea:2.2344601669732262  loss_houyan:225.30152893066406\n",
      "Number:8 epoch: 431/1200 loss_tea:2.2559658275756043  loss_houyan:267.309326171875\n",
      "Number:8 epoch: 441/1200 loss_tea:2.263963791325843  loss_houyan:225.3179931640625\n",
      "Number:8 epoch: 451/1200 loss_tea:2.2447890118331646  loss_houyan:316.0257873535156\n",
      "Number:8 epoch: 461/1200 loss_tea:2.2629932142871474  loss_houyan:281.7019348144531\n",
      "Number:8 epoch: 471/1200 loss_tea:2.2367519555207354  loss_houyan:227.54075622558594\n",
      "Number:8 epoch: 481/1200 loss_tea:2.202789465448848  loss_houyan:365.6370849609375\n",
      "Number:8 epoch: 491/1200 loss_tea:2.2452380322255063  loss_houyan:204.56402587890625\n",
      "Number:8 epoch: 501/1200 loss_tea:2.2666884195020867  loss_houyan:233.57986450195312\n",
      "Number:8 epoch: 511/1200 loss_tea:2.253723582759448  loss_houyan:238.51040649414062\n",
      "Number:8 epoch: 521/1200 loss_tea:2.2317225786228905  loss_houyan:238.3506622314453\n",
      "Number:8 epoch: 531/1200 loss_tea:2.236064611710479  loss_houyan:308.816162109375\n",
      "Number:8 epoch: 541/1200 loss_tea:2.235827597614803  loss_houyan:265.8160400390625\n",
      "Number:8 epoch: 551/1200 loss_tea:2.21318619853485  loss_houyan:212.92666625976562\n",
      "Number:8 epoch: 561/1200 loss_tea:2.258944469009716  loss_houyan:257.56842041015625\n",
      "Number:8 epoch: 571/1200 loss_tea:2.2765088828789732  loss_houyan:829.5292358398438\n",
      "Number:8 epoch: 581/1200 loss_tea:2.223986720296338  loss_houyan:280.50225830078125\n",
      "Number:8 epoch: 591/1200 loss_tea:2.244488791549082  loss_houyan:249.2997589111328\n",
      "Number:8 epoch: 601/1200 loss_tea:2.2206981292645414  loss_houyan:224.4392547607422\n",
      "Number:8 epoch: 611/1200 loss_tea:2.255904914516066  loss_houyan:222.26821899414062\n",
      "Number:8 epoch: 621/1200 loss_tea:2.245652687178351  loss_houyan:321.55352783203125\n",
      "Number:8 epoch: 631/1200 loss_tea:2.2411563230633322  loss_houyan:194.77902221679688\n",
      "Number:8 epoch: 641/1200 loss_tea:2.2362759337293237  loss_houyan:203.37945556640625\n",
      "Number:8 epoch: 651/1200 loss_tea:2.230732582514674  loss_houyan:369.81903076171875\n",
      "Number:8 epoch: 661/1200 loss_tea:2.25432584920969  loss_houyan:276.74652099609375\n",
      "Number:8 epoch: 671/1200 loss_tea:2.2337086591959823  loss_houyan:179.66537475585938\n",
      "Number:8 epoch: 681/1200 loss_tea:2.2502134598662695  loss_houyan:322.2589111328125\n",
      "Number:8 epoch: 691/1200 loss_tea:2.2439379438809457  loss_houyan:373.3477478027344\n",
      "Number:8 epoch: 701/1200 loss_tea:2.233281258315776  loss_houyan:191.10824584960938\n",
      "Number:8 epoch: 711/1200 loss_tea:2.2402873671178587  loss_houyan:541.438720703125\n",
      "Number:8 epoch: 721/1200 loss_tea:2.2213256439948164  loss_houyan:224.92091369628906\n",
      "Number:8 epoch: 731/1200 loss_tea:2.262315161521047  loss_houyan:242.01319885253906\n",
      "Number:8 epoch: 741/1200 loss_tea:2.196694527447842  loss_houyan:208.13934326171875\n",
      "Number:8 epoch: 751/1200 loss_tea:2.246547844665686  loss_houyan:591.96923828125\n",
      "Number:8 epoch: 761/1200 loss_tea:2.232145419954016  loss_houyan:263.8357849121094\n",
      "Number:8 epoch: 771/1200 loss_tea:2.2512241007547478  loss_houyan:322.142333984375\n",
      "Number:8 epoch: 781/1200 loss_tea:2.2430628750151005  loss_houyan:225.74591064453125\n",
      "Number:8 epoch: 791/1200 loss_tea:2.246867652912866  loss_houyan:289.3997802734375\n",
      "Number:8 epoch: 801/1200 loss_tea:2.215437635789693  loss_houyan:302.73248291015625\n",
      "Number:8 epoch: 811/1200 loss_tea:2.239666129808525  loss_houyan:247.20193481445312\n",
      "Number:8 epoch: 821/1200 loss_tea:2.22908231420088  loss_houyan:295.83270263671875\n",
      "Number:8 epoch: 831/1200 loss_tea:2.212802648461814  loss_houyan:238.77566528320312\n",
      "Number:8 epoch: 841/1200 loss_tea:2.2400643557825717  loss_houyan:250.408447265625\n",
      "Number:8 epoch: 851/1200 loss_tea:2.2481957829122314  loss_houyan:242.14068603515625\n",
      "Number:8 epoch: 861/1200 loss_tea:2.2213359737891225  loss_houyan:274.17926025390625\n",
      "Number:8 epoch: 871/1200 loss_tea:2.2592530356146474  loss_houyan:300.884765625\n",
      "Number:8 epoch: 881/1200 loss_tea:2.217255910540122  loss_houyan:409.29168701171875\n",
      "Number:8 epoch: 891/1200 loss_tea:2.2542880315681644  loss_houyan:204.03738403320312\n",
      "Number:8 epoch: 901/1200 loss_tea:2.2748351778538582  loss_houyan:231.05165100097656\n",
      "Number:8 epoch: 911/1200 loss_tea:2.2503687284397005  loss_houyan:423.5598449707031\n",
      "Number:8 epoch: 921/1200 loss_tea:2.2555104818311116  loss_houyan:195.73348999023438\n",
      "Number:8 epoch: 931/1200 loss_tea:2.2576735645016996  loss_houyan:577.6214599609375\n",
      "Number:8 epoch: 941/1200 loss_tea:2.2130619076502778  loss_houyan:203.46368408203125\n",
      "Number:8 epoch: 951/1200 loss_tea:2.2461718027360713  loss_houyan:187.25100708007812\n",
      "Number:8 epoch: 961/1200 loss_tea:2.255427958313569  loss_houyan:254.29989624023438\n",
      "Number:8 epoch: 971/1200 loss_tea:2.238557944660781  loss_houyan:234.18075561523438\n",
      "Number:8 epoch: 981/1200 loss_tea:2.234390892536995  loss_houyan:217.84754943847656\n",
      "Number:8 epoch: 991/1200 loss_tea:2.258042078859666  loss_houyan:278.00860595703125\n",
      "Number:8 epoch: 1001/1200 loss_tea:2.2212803075882803  loss_houyan:234.40130615234375\n",
      "Number:8 epoch: 1011/1200 loss_tea:2.246795194454259  loss_houyan:230.18399047851562\n",
      "Number:8 epoch: 1021/1200 loss_tea:2.2522551076222457  loss_houyan:402.11505126953125\n",
      "Number:8 epoch: 1031/1200 loss_tea:2.257288787026719  loss_houyan:315.3880615234375\n",
      "Number:8 epoch: 1041/1200 loss_tea:2.2119439498775972  loss_houyan:257.65008544921875\n",
      "Number:8 epoch: 1051/1200 loss_tea:2.2516983225271363  loss_houyan:255.8957977294922\n",
      "Number:8 epoch: 1061/1200 loss_tea:2.2281204520624813  loss_houyan:775.9437255859375\n",
      "Number:8 epoch: 1071/1200 loss_tea:2.242092780340914  loss_houyan:236.21759033203125\n",
      "Number:8 epoch: 1081/1200 loss_tea:2.25675045635461  loss_houyan:300.17840576171875\n",
      "Number:8 epoch: 1091/1200 loss_tea:2.244248463950768  loss_houyan:299.00360107421875\n",
      "Number:8 epoch: 1101/1200 loss_tea:2.240163711940541  loss_houyan:340.4329528808594\n",
      "Number:8 epoch: 1111/1200 loss_tea:2.229812129723572  loss_houyan:236.00706481933594\n",
      "Number:8 epoch: 1121/1200 loss_tea:2.2548130364566528  loss_houyan:304.8079833984375\n",
      "Number:8 epoch: 1131/1200 loss_tea:2.239770003338586  loss_houyan:484.8968200683594\n",
      "Number:8 epoch: 1141/1200 loss_tea:2.231503793855027  loss_houyan:509.9529113769531\n",
      "Number:8 epoch: 1151/1200 loss_tea:2.239736264222221  loss_houyan:221.48558044433594\n",
      "Number:8 epoch: 1161/1200 loss_tea:2.252713023255028  loss_houyan:388.82196044921875\n",
      "Number:8 epoch: 1171/1200 loss_tea:2.2233255111634938  loss_houyan:522.3092651367188\n",
      "Number:8 epoch: 1181/1200 loss_tea:2.2535085600552676  loss_houyan:221.35081481933594\n",
      "Number:8 epoch: 1191/1200 loss_tea:2.2414712562693033  loss_houyan:443.08209228515625\n",
      "finished training number 8 techer!\n",
      "start training number 9 techer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ysy/anaconda3/envs/bnn/lib/python3.9/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number:9 epoch: 1/1200 loss_tea:2.2373183965682983  loss_houyan:282.894775390625\n",
      "Number:9 epoch: 11/1200 loss_tea:2.2233966269678156  loss_houyan:198.7427215576172\n",
      "Number:9 epoch: 21/1200 loss_tea:2.2490472755577526  loss_houyan:217.35330200195312\n",
      "Number:9 epoch: 31/1200 loss_tea:2.247324367488166  loss_houyan:254.30862426757812\n",
      "Number:9 epoch: 41/1200 loss_tea:2.2325457204817405  loss_houyan:191.72555541992188\n",
      "Number:9 epoch: 51/1200 loss_tea:2.2534933800674195  loss_houyan:274.36578369140625\n",
      "Number:9 epoch: 61/1200 loss_tea:2.2519494051414486  loss_houyan:357.6236572265625\n",
      "Number:9 epoch: 71/1200 loss_tea:2.2606455225623985  loss_houyan:218.90286254882812\n",
      "Number:9 epoch: 81/1200 loss_tea:2.2754194251547566  loss_houyan:207.6839599609375\n",
      "Number:9 epoch: 91/1200 loss_tea:2.280402369719781  loss_houyan:200.67031860351562\n",
      "Number:9 epoch: 101/1200 loss_tea:2.244479770998697  loss_houyan:220.83349609375\n",
      "Number:9 epoch: 111/1200 loss_tea:2.238097152029237  loss_houyan:389.77642822265625\n",
      "Number:9 epoch: 121/1200 loss_tea:2.2822396006752697  loss_houyan:213.12176513671875\n",
      "Number:9 epoch: 131/1200 loss_tea:2.264538927782102  loss_houyan:222.8048553466797\n",
      "Number:9 epoch: 141/1200 loss_tea:2.2364446151735713  loss_houyan:266.2341003417969\n",
      "Number:9 epoch: 151/1200 loss_tea:2.2458480640326783  loss_houyan:366.47137451171875\n",
      "Number:9 epoch: 161/1200 loss_tea:2.2627092043889205  loss_houyan:675.7657470703125\n",
      "Number:9 epoch: 171/1200 loss_tea:2.256577481991371  loss_houyan:202.82000732421875\n",
      "Number:9 epoch: 181/1200 loss_tea:2.2525209355027993  loss_houyan:193.96755981445312\n",
      "Number:9 epoch: 191/1200 loss_tea:2.2759130810666894  loss_houyan:223.14578247070312\n",
      "Number:9 epoch: 201/1200 loss_tea:2.282992678678292  loss_houyan:200.89349365234375\n",
      "Number:9 epoch: 211/1200 loss_tea:2.2464398012670146  loss_houyan:255.18821716308594\n",
      "Number:9 epoch: 221/1200 loss_tea:2.2449138073646933  loss_houyan:216.76185607910156\n",
      "Number:9 epoch: 231/1200 loss_tea:2.2743818032518495  loss_houyan:213.10235595703125\n",
      "Number:9 epoch: 241/1200 loss_tea:2.2693321347567084  loss_houyan:247.697998046875\n",
      "Number:9 epoch: 251/1200 loss_tea:2.25100271052688  loss_houyan:213.0598907470703\n",
      "Number:9 epoch: 261/1200 loss_tea:2.2424861100590734  loss_houyan:180.44549560546875\n",
      "Number:9 epoch: 271/1200 loss_tea:2.2877904772345423  loss_houyan:218.68890380859375\n",
      "Number:9 epoch: 281/1200 loss_tea:2.2274189180412476  loss_houyan:221.86183166503906\n",
      "Number:9 epoch: 291/1200 loss_tea:2.2849433546213334  loss_houyan:187.55369567871094\n",
      "Number:9 epoch: 301/1200 loss_tea:2.2778592382713234  loss_houyan:234.63211059570312\n",
      "Number:9 epoch: 311/1200 loss_tea:2.2771363104703273  loss_houyan:263.71209716796875\n",
      "Number:9 epoch: 321/1200 loss_tea:2.275879994416848  loss_houyan:182.68495178222656\n",
      "Number:9 epoch: 331/1200 loss_tea:2.2226781195658245  loss_houyan:259.75811767578125\n",
      "Number:9 epoch: 341/1200 loss_tea:2.2322135277283497  loss_houyan:227.27146911621094\n",
      "Number:9 epoch: 351/1200 loss_tea:2.2603737151796257  loss_houyan:203.4035186767578\n",
      "Number:9 epoch: 361/1200 loss_tea:2.253998676274935  loss_houyan:220.28724670410156\n",
      "Number:9 epoch: 371/1200 loss_tea:2.2412567501752143  loss_houyan:254.5089569091797\n",
      "Number:9 epoch: 381/1200 loss_tea:2.2635643255851043  loss_houyan:220.7476043701172\n",
      "Number:9 epoch: 391/1200 loss_tea:2.2691718264743015  loss_houyan:238.43478393554688\n",
      "Number:9 epoch: 401/1200 loss_tea:2.2617982799859817  loss_houyan:279.7839660644531\n",
      "Number:9 epoch: 411/1200 loss_tea:2.262235271872866  loss_houyan:214.14942932128906\n",
      "Number:9 epoch: 421/1200 loss_tea:2.2199986397868963  loss_houyan:190.09414672851562\n",
      "Number:9 epoch: 431/1200 loss_tea:2.2637999297344926  loss_houyan:198.13209533691406\n",
      "Number:9 epoch: 441/1200 loss_tea:2.252778052292346  loss_houyan:198.3997039794922\n",
      "Number:9 epoch: 451/1200 loss_tea:2.248210607430874  loss_houyan:308.08966064453125\n",
      "Number:9 epoch: 461/1200 loss_tea:2.242591802807872  loss_houyan:362.46417236328125\n",
      "Number:9 epoch: 471/1200 loss_tea:2.2711112431495124  loss_houyan:196.8496551513672\n",
      "Number:9 epoch: 481/1200 loss_tea:2.2804251545762653  loss_houyan:248.2579803466797\n",
      "Number:9 epoch: 491/1200 loss_tea:2.2654661829076703  loss_houyan:199.8188018798828\n",
      "Number:9 epoch: 501/1200 loss_tea:2.255688847061337  loss_houyan:221.19696044921875\n",
      "Number:9 epoch: 511/1200 loss_tea:2.2598560661056846  loss_houyan:304.08819580078125\n",
      "Number:9 epoch: 521/1200 loss_tea:2.253424206873814  loss_houyan:357.67218017578125\n",
      "Number:9 epoch: 531/1200 loss_tea:2.275407479855226  loss_houyan:390.88165283203125\n",
      "Number:9 epoch: 541/1200 loss_tea:2.2709076318241865  loss_houyan:240.89784240722656\n",
      "Number:9 epoch: 551/1200 loss_tea:2.2460450586228426  loss_houyan:199.33253479003906\n",
      "Number:9 epoch: 561/1200 loss_tea:2.2640434628196333  loss_houyan:213.51837158203125\n",
      "Number:9 epoch: 571/1200 loss_tea:2.235566938555414  loss_houyan:208.87661743164062\n",
      "Number:9 epoch: 581/1200 loss_tea:2.2759730964176206  loss_houyan:262.0782470703125\n",
      "Number:9 epoch: 591/1200 loss_tea:2.2627567477467485  loss_houyan:363.10626220703125\n",
      "Number:9 epoch: 601/1200 loss_tea:2.224871535311122  loss_houyan:267.3872985839844\n",
      "Number:9 epoch: 611/1200 loss_tea:2.27187394426667  loss_houyan:213.305908203125\n",
      "Number:9 epoch: 621/1200 loss_tea:2.2576372413195758  loss_houyan:196.98658752441406\n",
      "Number:9 epoch: 631/1200 loss_tea:2.2394708273822617  loss_houyan:200.143798828125\n",
      "Number:9 epoch: 641/1200 loss_tea:2.2607096254887553  loss_houyan:245.59576416015625\n",
      "Number:9 epoch: 651/1200 loss_tea:2.2740743094158105  loss_houyan:399.0547180175781\n",
      "Number:9 epoch: 661/1200 loss_tea:2.2732201593664065  loss_houyan:350.29815673828125\n",
      "Number:9 epoch: 671/1200 loss_tea:2.240756703183127  loss_houyan:530.88427734375\n",
      "Number:9 epoch: 681/1200 loss_tea:2.2494239199103703  loss_houyan:200.8468475341797\n",
      "Number:9 epoch: 691/1200 loss_tea:2.26728749721313  loss_houyan:274.42266845703125\n",
      "Number:9 epoch: 701/1200 loss_tea:2.248506957594091  loss_houyan:194.72164916992188\n",
      "Number:9 epoch: 711/1200 loss_tea:2.2629669020759176  loss_houyan:207.0635528564453\n",
      "Number:9 epoch: 721/1200 loss_tea:2.2633525269029873  loss_houyan:278.04705810546875\n",
      "Number:9 epoch: 731/1200 loss_tea:2.201596185389444  loss_houyan:238.4335479736328\n",
      "Number:9 epoch: 741/1200 loss_tea:2.2553659328262756  loss_houyan:274.9053649902344\n",
      "Number:9 epoch: 751/1200 loss_tea:2.2514609245742183  loss_houyan:197.57223510742188\n",
      "Number:9 epoch: 761/1200 loss_tea:2.290627661878172  loss_houyan:266.72625732421875\n",
      "Number:9 epoch: 771/1200 loss_tea:2.2289967265545454  loss_houyan:217.31771850585938\n",
      "Number:9 epoch: 781/1200 loss_tea:2.2370084950433204  loss_houyan:217.46917724609375\n",
      "Number:9 epoch: 791/1200 loss_tea:2.255266688573055  loss_houyan:293.285400390625\n",
      "Number:9 epoch: 801/1200 loss_tea:2.270838701716745  loss_houyan:215.5181427001953\n",
      "Number:9 epoch: 811/1200 loss_tea:2.2527173891384735  loss_houyan:246.58969116210938\n",
      "Number:9 epoch: 821/1200 loss_tea:2.264302633491061  loss_houyan:201.8854217529297\n",
      "Number:9 epoch: 831/1200 loss_tea:2.244031636399357  loss_houyan:384.77423095703125\n",
      "Number:9 epoch: 841/1200 loss_tea:2.28025557295944  loss_houyan:205.19866943359375\n",
      "Number:9 epoch: 851/1200 loss_tea:2.232673252074981  loss_houyan:205.5288848876953\n",
      "Number:9 epoch: 861/1200 loss_tea:2.263465949492329  loss_houyan:239.408203125\n",
      "Number:9 epoch: 871/1200 loss_tea:2.2201565393373857  loss_houyan:243.80711364746094\n",
      "Number:9 epoch: 881/1200 loss_tea:2.2464734468704615  loss_houyan:347.78765869140625\n",
      "Number:9 epoch: 891/1200 loss_tea:2.2487416180239235  loss_houyan:211.09902954101562\n",
      "Number:9 epoch: 901/1200 loss_tea:2.2580210094392426  loss_houyan:276.60052490234375\n",
      "Number:9 epoch: 911/1200 loss_tea:2.2481954578240546  loss_houyan:224.95835876464844\n",
      "Number:9 epoch: 921/1200 loss_tea:2.269667911265182  loss_houyan:301.57159423828125\n",
      "Number:9 epoch: 931/1200 loss_tea:2.245209920348513  loss_houyan:326.991455078125\n",
      "Number:9 epoch: 941/1200 loss_tea:2.24793893520153  loss_houyan:218.7936248779297\n",
      "Number:9 epoch: 951/1200 loss_tea:2.242474815495214  loss_houyan:223.2106475830078\n",
      "Number:9 epoch: 961/1200 loss_tea:2.2489181513267513  loss_houyan:235.99281311035156\n",
      "Number:9 epoch: 971/1200 loss_tea:2.245351318799202  loss_houyan:192.3346405029297\n",
      "Number:9 epoch: 981/1200 loss_tea:2.259824475509709  loss_houyan:241.29736328125\n",
      "Number:9 epoch: 991/1200 loss_tea:2.292979291657648  loss_houyan:228.9481201171875\n",
      "Number:9 epoch: 1001/1200 loss_tea:2.2094847869930874  loss_houyan:218.18394470214844\n",
      "Number:9 epoch: 1011/1200 loss_tea:2.264961917399947  loss_houyan:230.13433837890625\n",
      "Number:9 epoch: 1021/1200 loss_tea:2.258762673090831  loss_houyan:230.57948303222656\n",
      "Number:9 epoch: 1031/1200 loss_tea:2.2728841945849108  loss_houyan:252.70892333984375\n",
      "Number:9 epoch: 1041/1200 loss_tea:2.2272273972888663  loss_houyan:338.2378845214844\n",
      "Number:9 epoch: 1051/1200 loss_tea:2.2401550861174915  loss_houyan:230.64747619628906\n",
      "Number:9 epoch: 1061/1200 loss_tea:2.2668639804060393  loss_houyan:202.94595336914062\n",
      "Number:9 epoch: 1071/1200 loss_tea:2.252180347401114  loss_houyan:201.17318725585938\n",
      "Number:9 epoch: 1081/1200 loss_tea:2.272497656191709  loss_houyan:226.4902801513672\n",
      "Number:9 epoch: 1091/1200 loss_tea:2.259214915629484  loss_houyan:214.56243896484375\n",
      "Number:9 epoch: 1101/1200 loss_tea:2.26400097095545  loss_houyan:216.18601989746094\n",
      "Number:9 epoch: 1111/1200 loss_tea:2.266137114103189  loss_houyan:307.7354736328125\n",
      "Number:9 epoch: 1121/1200 loss_tea:2.269134072074441  loss_houyan:244.5533447265625\n",
      "Number:9 epoch: 1131/1200 loss_tea:2.26080936751098  loss_houyan:617.24169921875\n",
      "Number:9 epoch: 1141/1200 loss_tea:2.2572656455175464  loss_houyan:206.3301544189453\n",
      "Number:9 epoch: 1151/1200 loss_tea:2.263006771883036  loss_houyan:222.59689331054688\n",
      "Number:9 epoch: 1161/1200 loss_tea:2.2550327258438845  loss_houyan:217.73550415039062\n",
      "Number:9 epoch: 1171/1200 loss_tea:2.2837987816837706  loss_houyan:376.96728515625\n",
      "Number:9 epoch: 1181/1200 loss_tea:2.2654285927613578  loss_houyan:242.6549835205078\n",
      "Number:9 epoch: 1191/1200 loss_tea:2.2487047464171797  loss_houyan:248.9780731201172\n",
      "finished training number 9 techer!\n"
     ]
    }
   ],
   "source": [
    "#training teacher models\n",
    "loss_func=nn.MSELoss()\n",
    "train_loss_all=[]\n",
    "for tea_num in range(n_teachers):\n",
    "    print(f'start training number {tea_num} techer!')\n",
    "    minloss =float ('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        train_num=0\n",
    "        teachers_model[tea_num].train()\n",
    "        for b_x,b_y in teacher_data_loader[tea_num]:\n",
    "            teacher_optimizers[tea_num].zero_grad()\n",
    "            output=teachers_model[tea_num](b_x.to(device))\n",
    "            loss=loss_func(output,b_y)\n",
    "            loss.backward()\n",
    "            teacher_optimizers[tea_num].step()\n",
    "            \n",
    "            teacher_optimizers[tea_num].zero_grad()\n",
    "            loss_houyan = teachers_model[tea_num].sample_elbo(b_x.to(device),b_y.to(device),1)\n",
    "            loss_houyan.backward()\n",
    "            teacher_optimizers[tea_num].step()\n",
    "\n",
    "            train_loss+=loss.item() * b_x.size(0)\n",
    "            train_num += b_x.size(0)\n",
    "            loss_tea = train_loss/train_num\n",
    "            \n",
    "            if loss_tea< minloss:\n",
    "                minloss = loss_tea\n",
    "                if os.path.exists(f'/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_8_gauss/teacher{tea_num}/best.pth'):\n",
    "                    os.remove(f'/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_8_gauss/teacher{tea_num}/best.pth')\n",
    "                torch.save(teachers_model[tea_num], f'/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_8_gauss/teacher{tea_num}/best.pth')\n",
    "        if epoch%10== 0:\n",
    "            print(f'Number:{tea_num}','epoch: {}/{}'.format(epoch+1,num_epochs),f'loss_tea:{loss_tea}', f' loss_houyan:{loss_houyan}')\n",
    "        train_loss_all.append(loss) \n",
    "    print(f'finished training number {tea_num} techer!')          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    \n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "train_loss_all  = torch.tensor(train_loss_all,device='cpu')\n",
    "plt.plot(train_loss_all ,\"ro-\",label=\"Train loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3096\n",
      "[0.51445022] [5.11542988]\n",
      "3059 0.011950904392764805 37\n",
      "tensor([0.5145], device='cuda:1', dtype=torch.float64)\n",
      "tensor([5.1154], device='cuda:1', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(samples)\n",
    "y_samp = np.zeros((samples,1))\n",
    "for s in range(samples):\n",
    "    for b_x,b_y  in student_train_loader:\n",
    "        y = net (b_x[s]).cpu().detach().numpy()\n",
    "        y_samp[s] = y.reshape(-1)\n",
    "\n",
    "lower = np.percentile(y_samp, 2.5, axis = 0)\n",
    "upper = np.percentile(y_samp, 97.5, axis = 0)\n",
    "print(lower,upper)\n",
    "count = 0 \n",
    "for i in range(samples):\n",
    "    for b_x,b_y in student_train_loader:\n",
    "        if lower<=b_y[i].item()<=upper:\n",
    "            count+=1\n",
    "per = 1-count/samples\n",
    "print (count,per,samples-count)\n",
    "#\n",
    "upper=torch.tensor(upper).to(device)\n",
    "lower= torch.tensor(lower).to(device)\n",
    "print(lower)\n",
    "print(upper)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 2.8388,  ..., 0.0000, 2.7433, 0.0000],\n",
      "        [0.0000, 0.0000, 1.5391,  ..., 3.1841, 1.9641, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0721,  ..., 3.2811, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 5.0702,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 3.7439,  ..., 4.0381, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 4.2896,  ..., 4.1645, 0.5253, 0.0000]],\n",
      "       device='cuda:1') torch.Size([3096, 10]) 0.5458979328165374\n",
      "tensor([[2.1128],\n",
      "        [2.3143],\n",
      "        [2.3275],\n",
      "        ...,\n",
      "        [2.6863],\n",
      "        [2.6864],\n",
      "        [2.6862]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "#certainty aware\n",
    "import random\n",
    "import numpy as np\n",
    "mu = 0\n",
    "# sigma = 1\n",
    "# sigma =2\n",
    "# sigma=3\n",
    "sigma=3.028\n",
    "\n",
    "#teahcer labeling\n",
    "teacher_best_models =[torch.load(f'/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_8_gauss/teacher{tea_num}/best.pth')  for  tea_num in  range(n_teachers)]\n",
    "\n",
    "\n",
    "pos= torch.zeros(samples,n_teachers).to(device)\n",
    "#0 ln ln0\n",
    "#    \n",
    "ccc = 0\n",
    "sss=0# ;  %=ccc/(sss+ccc)\n",
    "for tea in range(n_teachers):\n",
    "    for x,y in student_train_loader:\n",
    "            teacher_best_models[tea].eval().to(device)\n",
    "            pred=teacher_best_models[tea].forward(x)\n",
    "\n",
    "            pred+=random.gauss(mu,sigma)\n",
    "            for i in range(samples):\n",
    "                if lower.item()<= pred[i].item() <= upper.item():\n",
    "                    # \n",
    "                    pos[i][tea]+=pred[i].item()\n",
    "                    sss+=1\n",
    "                else:\n",
    "                    ccc+=1\n",
    "print(pos,pos.shape,ccc/(sss+ccc))            \n",
    "\n",
    "#pos0     \n",
    "res= torch.zeros(samples,1).to(device)\n",
    "count_data=0\n",
    "count_sum=0\n",
    "for i in range(samples):\n",
    "    for j in range(n_teachers):\n",
    "        if pos[i][j]!=0:\n",
    "            count_data+=1\n",
    "            count_sum+=pos[i][j]\n",
    "\n",
    "    if count_data==0:#n//0//\n",
    "        print('False')\n",
    "    cur=count_sum/count_data #data1 +....datan//K\n",
    "    res[i]+=cur#\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2769242/2143468446.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  student_trainset =TensorDataset(torch.tensor(student_x[0],device=device,dtype=torch.float),torch.tensor(res,device=device,dtype=torch.float))\n"
     ]
    }
   ],
   "source": [
    "#student_label\n",
    "student_trainset =TensorDataset(torch.tensor(student_x[0],device=device,dtype=torch.float),torch.tensor(res,device=device,dtype=torch.float))\n",
    "student_train_loader1 = DataLoader(student_trainset,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating student  folders\n",
    "def  mkdir_if_missing(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "mkdir_if_missing(f'/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_8_gauss/stu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden1): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=256, bias=True)\n",
      "    (1): Dropout(p=0.7, inplace=False)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (hidden2): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (1): Dropout(p=0.7, inplace=False)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (hidden3): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=16, bias=True)\n",
      "    (1): Dropout(p=0.5, inplace=False)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (predict): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# neural network model mlp\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden1 = nn.Sequential(nn.Linear(in_features=8, out_features=256, bias=True),\n",
    "                                  nn.Dropout(0.7),\n",
    "                                  nn.ReLU())\n",
    "        \n",
    "        self.hidden2 = nn.Sequential(nn.Linear(in_features=256, out_features=64, bias=True),\n",
    "                                  nn.Dropout(0.7),\n",
    "                                  nn.ReLU())\n",
    "        \n",
    "        self.hidden3 = nn.Sequential(nn.Linear(in_features=64, out_features=16, bias=True),\n",
    "                                  nn.Dropout(0.5),\n",
    "                                  nn.ReLU()\n",
    "                                  )\n",
    "        \n",
    "        self.predict = nn.Sequential(nn.Linear(in_features=16, out_features=1, bias=True),\n",
    "                                        nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.hidden3(x)\n",
    "        output = self.predict(x)\n",
    "        return output[:, 0]\n",
    "\n",
    "\n",
    "mlpreg=MLP().to(device)\n",
    "print(mlpreg)\n",
    "#initialzing student model\n",
    "student_model=mlpreg\n",
    "student_optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ysy/anaconda3/envs/bnn/lib/python3.9/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/ysy/anaconda3/envs/bnn/lib/python3.9/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([24, 1])) that is different to the input size (torch.Size([24])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student train: Epo:0  Loss_stu:6.474905026359459\n",
      "student train: Epo:1  Loss_stu:4.859234451323516\n",
      "student train: Epo:2  Loss_stu:3.5882337758707448\n",
      "student train: Epo:3  Loss_stu:3.247581213943718\n",
      "student train: Epo:4  Loss_stu:3.711885659885653\n",
      "student train: Epo:5  Loss_stu:3.7548939873697837\n",
      "student train: Epo:6  Loss_stu:2.475243135324128\n",
      "student train: Epo:7  Loss_stu:2.906385845300147\n",
      "student train: Epo:8  Loss_stu:2.4886246574633497\n",
      "student train: Epo:9  Loss_stu:2.424465664905479\n",
      "student train: Epo:10  Loss_stu:2.29482523569457\n",
      "student train: Epo:11  Loss_stu:2.021989417014504\n",
      "student train: Epo:12  Loss_stu:2.1961490060623916\n",
      "student train: Epo:13  Loss_stu:2.637243472328482\n",
      "student train: Epo:14  Loss_stu:1.9850547563198\n",
      "student train: Epo:15  Loss_stu:1.9833903827100454\n",
      "student train: Epo:16  Loss_stu:1.8072168753128643\n",
      "student train: Epo:17  Loss_stu:1.6676025313616416\n",
      "student train: Epo:18  Loss_stu:1.9223097852342197\n",
      "student train: Epo:19  Loss_stu:1.5584235915225915\n",
      "student train: Epo:20  Loss_stu:1.5867253615874652\n",
      "student train: Epo:21  Loss_stu:1.6977737088536107\n",
      "student train: Epo:22  Loss_stu:1.5106474734091944\n",
      "student train: Epo:23  Loss_stu:1.3620983069565253\n",
      "student train: Epo:24  Loss_stu:1.4816535760573946\n",
      "student train: Epo:25  Loss_stu:1.5242871646733247\n",
      "student train: Epo:26  Loss_stu:1.326780559172618\n",
      "student train: Epo:27  Loss_stu:1.772488994807852\n",
      "student train: Epo:28  Loss_stu:1.4794779901972728\n",
      "student train: Epo:29  Loss_stu:1.3778171634797285\n",
      "student train: Epo:30  Loss_stu:1.2281729826939507\n",
      "student train: Epo:31  Loss_stu:1.3009577619937038\n",
      "student train: Epo:32  Loss_stu:1.3438652538205917\n",
      "student train: Epo:33  Loss_stu:1.2805187830013207\n",
      "student train: Epo:34  Loss_stu:1.228219491258764\n",
      "student train: Epo:35  Loss_stu:1.334668241730032\n",
      "student train: Epo:36  Loss_stu:1.3053310020641455\n",
      "student train: Epo:37  Loss_stu:1.2127264984510358\n",
      "student train: Epo:38  Loss_stu:1.1035055324088696\n",
      "student train: Epo:39  Loss_stu:1.139542085871832\n",
      "student train: Epo:40  Loss_stu:1.3664284654982022\n",
      "student train: Epo:41  Loss_stu:1.334065445941856\n",
      "student train: Epo:42  Loss_stu:1.0405287051077654\n",
      "student train: Epo:43  Loss_stu:1.1081193919637715\n",
      "student train: Epo:44  Loss_stu:1.0407544066431602\n",
      "student train: Epo:45  Loss_stu:1.0482820631305685\n",
      "student train: Epo:46  Loss_stu:1.0616133305453515\n",
      "student train: Epo:47  Loss_stu:1.0271294067688383\n",
      "student train: Epo:48  Loss_stu:0.9353779705303892\n",
      "student train: Epo:49  Loss_stu:0.9832839308166996\n",
      "student train: Epo:50  Loss_stu:0.9215694120688032\n",
      "student train: Epo:51  Loss_stu:0.9360946267458203\n",
      "student train: Epo:52  Loss_stu:0.8674758487893629\n",
      "student train: Epo:53  Loss_stu:0.9182241914808288\n",
      "student train: Epo:54  Loss_stu:0.9022771110830381\n",
      "student train: Epo:55  Loss_stu:0.9658145688916978\n",
      "student train: Epo:56  Loss_stu:0.9489929916322694\n",
      "student train: Epo:57  Loss_stu:0.9142389189673332\n",
      "student train: Epo:58  Loss_stu:0.8658736081702456\n",
      "student train: Epo:59  Loss_stu:0.8406845078603857\n",
      "student train: Epo:60  Loss_stu:0.8755399225910196\n",
      "student train: Epo:61  Loss_stu:0.8227908163107642\n",
      "student train: Epo:62  Loss_stu:0.8311771192907979\n",
      "student train: Epo:63  Loss_stu:0.7838290831223322\n",
      "student train: Epo:64  Loss_stu:0.7761413496594096\n",
      "student train: Epo:65  Loss_stu:0.8202248582236219\n",
      "student train: Epo:66  Loss_stu:0.7936477439348087\n",
      "student train: Epo:67  Loss_stu:0.7925651586332987\n",
      "student train: Epo:68  Loss_stu:0.747803614275092\n",
      "student train: Epo:69  Loss_stu:0.7027652074200238\n",
      "student train: Epo:70  Loss_stu:0.7253442576073245\n",
      "student train: Epo:71  Loss_stu:0.7185518886383806\n",
      "student train: Epo:72  Loss_stu:0.6884611621691583\n",
      "student train: Epo:73  Loss_stu:0.7098703101007822\n",
      "student train: Epo:74  Loss_stu:0.6841543908267058\n",
      "student train: Epo:75  Loss_stu:0.6914780595043833\n",
      "student train: Epo:76  Loss_stu:0.6814766538543603\n",
      "student train: Epo:77  Loss_stu:0.659050756492664\n",
      "student train: Epo:78  Loss_stu:0.6872099835127207\n",
      "student train: Epo:79  Loss_stu:0.6765124850186883\n",
      "student train: Epo:80  Loss_stu:0.6166696255829291\n",
      "student train: Epo:81  Loss_stu:0.6029948127054121\n",
      "student train: Epo:82  Loss_stu:0.6219058104571754\n",
      "student train: Epo:83  Loss_stu:0.598141302973109\n",
      "student train: Epo:84  Loss_stu:0.596337732418563\n",
      "student train: Epo:85  Loss_stu:0.5887773337290269\n",
      "student train: Epo:86  Loss_stu:0.6036989710435695\n",
      "student train: Epo:87  Loss_stu:0.595139547065863\n",
      "student train: Epo:88  Loss_stu:0.58524469919599\n",
      "student train: Epo:89  Loss_stu:0.580830258146429\n",
      "student train: Epo:90  Loss_stu:0.5574980698506653\n",
      "student train: Epo:91  Loss_stu:0.5397347324270303\n",
      "student train: Epo:92  Loss_stu:0.5658492488300462\n",
      "student train: Epo:93  Loss_stu:0.5670209870473975\n",
      "student train: Epo:94  Loss_stu:0.5243702677947298\n",
      "student train: Epo:95  Loss_stu:0.5021588200130512\n",
      "student train: Epo:96  Loss_stu:0.5381322349072735\n",
      "student train: Epo:97  Loss_stu:0.511499592038088\n",
      "student train: Epo:98  Loss_stu:0.5119851329375915\n",
      "student train: Epo:99  Loss_stu:0.4813870491907578\n",
      "student train: Epo:100  Loss_stu:0.474574634579109\n",
      "student train: Epo:101  Loss_stu:0.4860112695576917\n",
      "student train: Epo:102  Loss_stu:0.46215226838138984\n",
      "student train: Epo:103  Loss_stu:0.46892941352316886\n",
      "student train: Epo:104  Loss_stu:0.4795084642655474\n",
      "student train: Epo:105  Loss_stu:0.45950927213796966\n",
      "student train: Epo:106  Loss_stu:0.4382213482715055\n",
      "student train: Epo:107  Loss_stu:0.4524548534275025\n",
      "student train: Epo:108  Loss_stu:0.44271668837976086\n",
      "student train: Epo:109  Loss_stu:0.43749287770700085\n",
      "student train: Epo:110  Loss_stu:0.45005997306924767\n",
      "student train: Epo:111  Loss_stu:0.43448726482477606\n",
      "student train: Epo:112  Loss_stu:0.40441636229054256\n",
      "student train: Epo:113  Loss_stu:0.401085918456393\n",
      "student train: Epo:114  Loss_stu:0.3790080028294901\n",
      "student train: Epo:115  Loss_stu:0.40525384512982626\n",
      "student train: Epo:116  Loss_stu:0.3766693405864775\n",
      "student train: Epo:117  Loss_stu:0.37865440258684085\n",
      "student train: Epo:118  Loss_stu:0.37841532666245786\n",
      "student train: Epo:119  Loss_stu:0.37379021599927303\n",
      "student train: Epo:120  Loss_stu:0.3687542015421914\n",
      "student train: Epo:121  Loss_stu:0.3786546028091618\n",
      "student train: Epo:122  Loss_stu:0.3557379485254756\n",
      "student train: Epo:123  Loss_stu:0.3543985717056334\n",
      "student train: Epo:124  Loss_stu:0.35777513532675514\n",
      "student train: Epo:125  Loss_stu:0.358746215547086\n",
      "student train: Epo:126  Loss_stu:0.3383683277346983\n",
      "student train: Epo:127  Loss_stu:0.35219009956032116\n",
      "student train: Epo:128  Loss_stu:0.3343477376979759\n",
      "student train: Epo:129  Loss_stu:0.3209235725168726\n",
      "student train: Epo:130  Loss_stu:0.32486506002817966\n",
      "student train: Epo:131  Loss_stu:0.32124881072869904\n",
      "student train: Epo:132  Loss_stu:0.3154268290183341\n",
      "student train: Epo:133  Loss_stu:0.30523634495969276\n",
      "student train: Epo:134  Loss_stu:0.30073215179049073\n",
      "student train: Epo:135  Loss_stu:0.3051921835780452\n",
      "student train: Epo:136  Loss_stu:0.3004896891055489\n",
      "student train: Epo:137  Loss_stu:0.2970064048696242\n",
      "student train: Epo:138  Loss_stu:0.29309894510325846\n",
      "student train: Epo:139  Loss_stu:0.285924428578187\n",
      "student train: Epo:140  Loss_stu:0.2789317402189708\n",
      "student train: Epo:141  Loss_stu:0.27955564355973433\n",
      "student train: Epo:142  Loss_stu:0.26349827747320326\n",
      "student train: Epo:143  Loss_stu:0.27021497857663057\n",
      "student train: Epo:144  Loss_stu:0.27059685144313544\n",
      "student train: Epo:145  Loss_stu:0.24910990851486067\n",
      "student train: Epo:146  Loss_stu:0.24178322726133875\n",
      "student train: Epo:147  Loss_stu:0.2548762791301545\n",
      "student train: Epo:148  Loss_stu:0.23452683628683557\n",
      "student train: Epo:149  Loss_stu:0.2301545140817184\n",
      "student train: Epo:150  Loss_stu:0.23507641286658995\n",
      "student train: Epo:151  Loss_stu:0.22478675395635364\n",
      "student train: Epo:152  Loss_stu:0.23048566162432196\n",
      "student train: Epo:153  Loss_stu:0.217157994784434\n",
      "student train: Epo:154  Loss_stu:0.2127543804488441\n",
      "student train: Epo:155  Loss_stu:0.21655887334537752\n",
      "student train: Epo:156  Loss_stu:0.21617368148586855\n",
      "student train: Epo:157  Loss_stu:0.2127553479462015\n",
      "student train: Epo:158  Loss_stu:0.2070260194998995\n",
      "student train: Epo:159  Loss_stu:0.20110924318625328\n",
      "student train: Epo:160  Loss_stu:0.19489677265787\n",
      "student train: Epo:161  Loss_stu:0.2011738286307924\n",
      "student train: Epo:162  Loss_stu:0.1894895665597854\n",
      "student train: Epo:163  Loss_stu:0.1960315593453341\n",
      "student train: Epo:164  Loss_stu:0.18168176487280724\n",
      "student train: Epo:165  Loss_stu:0.1902335120571984\n",
      "student train: Epo:166  Loss_stu:0.19117126211485505\n",
      "student train: Epo:167  Loss_stu:0.17863870467942505\n",
      "student train: Epo:168  Loss_stu:0.17463885324715953\n",
      "student train: Epo:169  Loss_stu:0.17363229463266772\n",
      "student train: Epo:170  Loss_stu:0.17571374689269745\n",
      "student train: Epo:171  Loss_stu:0.16915812341433778\n",
      "student train: Epo:172  Loss_stu:0.1707413985516674\n",
      "student train: Epo:173  Loss_stu:0.1664641715913472\n",
      "student train: Epo:174  Loss_stu:0.16298850169477536\n",
      "student train: Epo:175  Loss_stu:0.17240997623334559\n",
      "student train: Epo:176  Loss_stu:0.16319260051238135\n",
      "student train: Epo:177  Loss_stu:0.1550796602200476\n",
      "student train: Epo:178  Loss_stu:0.1529956252075905\n",
      "student train: Epo:179  Loss_stu:0.1508876489576443\n",
      "student train: Epo:180  Loss_stu:0.15577746130699335\n",
      "student train: Epo:181  Loss_stu:0.1475185570367239\n",
      "student train: Epo:182  Loss_stu:0.1467308619610715\n",
      "student train: Epo:183  Loss_stu:0.14671828799931577\n",
      "student train: Epo:184  Loss_stu:0.14777081338472145\n",
      "student train: Epo:185  Loss_stu:0.14989127967631785\n",
      "student train: Epo:186  Loss_stu:0.15031470104243405\n",
      "student train: Epo:187  Loss_stu:0.14145923238392025\n",
      "student train: Epo:188  Loss_stu:0.1388909254964316\n",
      "student train: Epo:189  Loss_stu:0.1316411166574604\n",
      "student train: Epo:190  Loss_stu:0.13539259022135453\n",
      "student train: Epo:191  Loss_stu:0.135184620509468\n",
      "student train: Epo:192  Loss_stu:0.13170955728652867\n",
      "student train: Epo:193  Loss_stu:0.12453756845274637\n",
      "student train: Epo:194  Loss_stu:0.12673605579969494\n",
      "student train: Epo:195  Loss_stu:0.12403530113456786\n",
      "student train: Epo:196  Loss_stu:0.127617762782623\n",
      "student train: Epo:197  Loss_stu:0.12619355193866316\n",
      "student train: Epo:198  Loss_stu:0.12149594487376915\n",
      "student train: Epo:199  Loss_stu:0.11848965056182802\n",
      "student train: Epo:200  Loss_stu:0.1199758264407015\n",
      "student train: Epo:201  Loss_stu:0.11989375205773099\n",
      "student train: Epo:202  Loss_stu:0.11410412071056145\n",
      "student train: Epo:203  Loss_stu:0.1117404842754051\n",
      "student train: Epo:204  Loss_stu:0.11723313944592341\n",
      "student train: Epo:205  Loss_stu:0.10627895605194476\n",
      "student train: Epo:206  Loss_stu:0.10934434559687163\n",
      "student train: Epo:207  Loss_stu:0.11016434404123045\n",
      "student train: Epo:208  Loss_stu:0.10839801689836098\n",
      "student train: Epo:209  Loss_stu:0.10471677271894707\n",
      "student train: Epo:210  Loss_stu:0.1067829790407219\n",
      "student train: Epo:211  Loss_stu:0.10287796634728286\n",
      "student train: Epo:212  Loss_stu:0.1064993612641512\n",
      "student train: Epo:213  Loss_stu:0.10206280427308662\n",
      "student train: Epo:214  Loss_stu:0.09876430841302379\n",
      "student train: Epo:215  Loss_stu:0.09715742192373103\n",
      "student train: Epo:216  Loss_stu:0.09729839597638572\n",
      "student train: Epo:217  Loss_stu:0.09723172773840508\n",
      "student train: Epo:218  Loss_stu:0.0931974733368679\n",
      "student train: Epo:219  Loss_stu:0.09339936289368365\n",
      "student train: Epo:220  Loss_stu:0.08708076219457064\n",
      "student train: Epo:221  Loss_stu:0.08929615067342148\n",
      "student train: Epo:222  Loss_stu:0.0904225741130437\n",
      "student train: Epo:223  Loss_stu:0.08804870955164734\n",
      "student train: Epo:224  Loss_stu:0.08321914384839454\n",
      "student train: Epo:225  Loss_stu:0.08264298346230534\n",
      "student train: Epo:226  Loss_stu:0.08122997465290764\n",
      "student train: Epo:227  Loss_stu:0.07929288334085652\n",
      "student train: Epo:228  Loss_stu:0.0809820558712156\n",
      "student train: Epo:229  Loss_stu:0.07759311727852168\n",
      "student train: Epo:230  Loss_stu:0.075214362142545\n",
      "student train: Epo:231  Loss_stu:0.07612225954307758\n",
      "student train: Epo:232  Loss_stu:0.07434940860830536\n",
      "student train: Epo:233  Loss_stu:0.07431044758752335\n",
      "student train: Epo:234  Loss_stu:0.07013891375496098\n",
      "student train: Epo:235  Loss_stu:0.06950737985491137\n",
      "student train: Epo:236  Loss_stu:0.07075767867863948\n",
      "student train: Epo:237  Loss_stu:0.07079924015419736\n",
      "student train: Epo:238  Loss_stu:0.07021492977629028\n",
      "student train: Epo:239  Loss_stu:0.06486904286136923\n",
      "student train: Epo:240  Loss_stu:0.06551836939283119\n",
      "student train: Epo:241  Loss_stu:0.06247734798248424\n",
      "student train: Epo:242  Loss_stu:0.06280944459814126\n",
      "student train: Epo:243  Loss_stu:0.06386378829100335\n",
      "student train: Epo:244  Loss_stu:0.06308513693030182\n",
      "student train: Epo:245  Loss_stu:0.062077939673988704\n",
      "student train: Epo:246  Loss_stu:0.05918383638757144\n",
      "student train: Epo:247  Loss_stu:0.057088234281355094\n",
      "student train: Epo:248  Loss_stu:0.05376339365923127\n",
      "student train: Epo:249  Loss_stu:0.05448356683748637\n",
      "student train: Epo:250  Loss_stu:0.05399998236996259\n",
      "student train: Epo:251  Loss_stu:0.05558253013232882\n",
      "student train: Epo:252  Loss_stu:0.05364061781332782\n",
      "student train: Epo:253  Loss_stu:0.05242905085208496\n",
      "student train: Epo:254  Loss_stu:0.05107535002151509\n",
      "student train: Epo:255  Loss_stu:0.050135866558366965\n",
      "student train: Epo:256  Loss_stu:0.05126925983324224\n",
      "student train: Epo:257  Loss_stu:0.04949682799844163\n",
      "student train: Epo:258  Loss_stu:0.0465211916223977\n",
      "student train: Epo:259  Loss_stu:0.048526060962399774\n",
      "student train: Epo:260  Loss_stu:0.046755273896209336\n",
      "student train: Epo:261  Loss_stu:0.04630925479400374\n",
      "student train: Epo:262  Loss_stu:0.044199451380613854\n",
      "student train: Epo:263  Loss_stu:0.042894434520748544\n",
      "student train: Epo:264  Loss_stu:0.04185162427852012\n",
      "student train: Epo:265  Loss_stu:0.04343806407402344\n",
      "student train: Epo:266  Loss_stu:0.03979600210811279\n",
      "student train: Epo:267  Loss_stu:0.038611819938749306\n",
      "student train: Epo:268  Loss_stu:0.038242585186964474\n",
      "student train: Epo:269  Loss_stu:0.03754681131251406\n",
      "student train: Epo:270  Loss_stu:0.037883740216878646\n",
      "student train: Epo:271  Loss_stu:0.03667418886871301\n",
      "student train: Epo:272  Loss_stu:0.035989487558141235\n",
      "student train: Epo:273  Loss_stu:0.03597163280005295\n",
      "student train: Epo:274  Loss_stu:0.03437770172491554\n",
      "student train: Epo:275  Loss_stu:0.03446455250761306\n",
      "student train: Epo:276  Loss_stu:0.03358477333353625\n",
      "student train: Epo:277  Loss_stu:0.031809570657652476\n",
      "student train: Epo:278  Loss_stu:0.030538978146567208\n",
      "student train: Epo:279  Loss_stu:0.02993028567627419\n",
      "student train: Epo:280  Loss_stu:0.0292396224428748\n",
      "student train: Epo:281  Loss_stu:0.02937732115195395\n",
      "student train: Epo:282  Loss_stu:0.028929039008087583\n",
      "student train: Epo:283  Loss_stu:0.028271013736070282\n",
      "student train: Epo:284  Loss_stu:0.026213433800472155\n",
      "student train: Epo:285  Loss_stu:0.027128931914651117\n",
      "student train: Epo:286  Loss_stu:0.024964314459483754\n",
      "student train: Epo:287  Loss_stu:0.025136708640505486\n",
      "student train: Epo:288  Loss_stu:0.025180341237891243\n",
      "student train: Epo:289  Loss_stu:0.02460775415121893\n",
      "student train: Epo:290  Loss_stu:0.02503870170351686\n",
      "student train: Epo:291  Loss_stu:0.02288411239532691\n",
      "student train: Epo:292  Loss_stu:0.02314099063013875\n",
      "student train: Epo:293  Loss_stu:0.02140405832835562\n",
      "student train: Epo:294  Loss_stu:0.021921647086893557\n",
      "student train: Epo:295  Loss_stu:0.02181708128299824\n",
      "student train: Epo:296  Loss_stu:0.020567292674836878\n",
      "student train: Epo:297  Loss_stu:0.019460568483678255\n",
      "student train: Epo:298  Loss_stu:0.019002609539690407\n",
      "student train: Epo:299  Loss_stu:0.018774459833996393\n",
      "student train: Epo:300  Loss_stu:0.01912323623481724\n",
      "student train: Epo:301  Loss_stu:0.0184187076851071\n",
      "student train: Epo:302  Loss_stu:0.017349611594791713\n",
      "student train: Epo:303  Loss_stu:0.016460739576693344\n",
      "student train: Epo:304  Loss_stu:0.016703964538333182\n",
      "student train: Epo:305  Loss_stu:0.015552315110153005\n",
      "student train: Epo:306  Loss_stu:0.015628722903032327\n",
      "student train: Epo:307  Loss_stu:0.014925083881893824\n",
      "student train: Epo:308  Loss_stu:0.014287739063225667\n",
      "student train: Epo:309  Loss_stu:0.014281554927313051\n",
      "student train: Epo:310  Loss_stu:0.013341132397414485\n",
      "student train: Epo:311  Loss_stu:0.013226048626217422\n",
      "student train: Epo:312  Loss_stu:0.012760773174782363\n",
      "student train: Epo:313  Loss_stu:0.012754811578037973\n",
      "student train: Epo:314  Loss_stu:0.012085828583600909\n",
      "student train: Epo:315  Loss_stu:0.011303142504886909\n",
      "student train: Epo:316  Loss_stu:0.01101423096927292\n",
      "student train: Epo:317  Loss_stu:0.010632296658340043\n",
      "student train: Epo:318  Loss_stu:0.01032180297946514\n",
      "student train: Epo:319  Loss_stu:0.01026416713376686\n",
      "student train: Epo:320  Loss_stu:0.010011382003914016\n",
      "student train: Epo:321  Loss_stu:0.00965413181183412\n",
      "student train: Epo:322  Loss_stu:0.008959551118025485\n",
      "student train: Epo:323  Loss_stu:0.008877944425037202\n",
      "student train: Epo:324  Loss_stu:0.008399764258770672\n",
      "student train: Epo:325  Loss_stu:0.008174736200752415\n",
      "student train: Epo:326  Loss_stu:0.008323048729959693\n",
      "student train: Epo:327  Loss_stu:0.007845859569124395\n",
      "student train: Epo:328  Loss_stu:0.006973778526960494\n",
      "student train: Epo:329  Loss_stu:0.0070527525334703215\n",
      "student train: Epo:330  Loss_stu:0.0069725186171765785\n",
      "student train: Epo:331  Loss_stu:0.0065468197197530624\n",
      "student train: Epo:332  Loss_stu:0.006577037236333201\n",
      "student train: Epo:333  Loss_stu:0.0062349742309741425\n",
      "student train: Epo:334  Loss_stu:0.005941893288129291\n",
      "student train: Epo:335  Loss_stu:0.005909220138636901\n",
      "student train: Epo:336  Loss_stu:0.0054455924326751275\n",
      "student train: Epo:337  Loss_stu:0.005248726452688761\n",
      "student train: Epo:338  Loss_stu:0.005238919130235295\n",
      "student train: Epo:339  Loss_stu:0.0048296149321940054\n",
      "student train: Epo:340  Loss_stu:0.0046470192404228046\n",
      "student train: Epo:341  Loss_stu:0.004459153729050543\n",
      "student train: Epo:342  Loss_stu:0.004412367084431787\n",
      "student train: Epo:343  Loss_stu:0.004131310604974743\n",
      "student train: Epo:344  Loss_stu:0.003978363347771633\n",
      "student train: Epo:345  Loss_stu:0.003890716275248378\n",
      "student train: Epo:346  Loss_stu:0.0036072758828393286\n",
      "student train: Epo:347  Loss_stu:0.0035934878593835333\n",
      "student train: Epo:348  Loss_stu:0.003338165963261126\n",
      "student train: Epo:349  Loss_stu:0.003264948485349365\n",
      "student train: Epo:350  Loss_stu:0.0029882520205011588\n",
      "student train: Epo:351  Loss_stu:0.0030164698419192426\n",
      "student train: Epo:352  Loss_stu:0.002828487362929208\n",
      "student train: Epo:353  Loss_stu:0.0027811034439367455\n",
      "student train: Epo:354  Loss_stu:0.0026224930710532254\n",
      "student train: Epo:355  Loss_stu:0.002521075083614262\n",
      "student train: Epo:356  Loss_stu:0.0024053329633168625\n",
      "student train: Epo:357  Loss_stu:0.0023139872553924372\n",
      "student train: Epo:358  Loss_stu:0.0022274534652991468\n",
      "student train: Epo:359  Loss_stu:0.0021495732803075776\n",
      "student train: Epo:360  Loss_stu:0.002038006438544015\n",
      "student train: Epo:361  Loss_stu:0.0019663735741065956\n",
      "student train: Epo:362  Loss_stu:0.0018497569031065248\n",
      "student train: Epo:363  Loss_stu:0.0017903230726978806\n",
      "student train: Epo:364  Loss_stu:0.0017607537734753017\n",
      "student train: Epo:365  Loss_stu:0.001691285277733661\n",
      "student train: Epo:366  Loss_stu:0.0016276050540702826\n",
      "student train: Epo:367  Loss_stu:0.0015149415644831494\n",
      "student train: Epo:368  Loss_stu:0.001443243418725145\n",
      "student train: Epo:369  Loss_stu:0.001445409110103534\n",
      "student train: Epo:370  Loss_stu:0.001422666712785597\n",
      "student train: Epo:371  Loss_stu:0.001294645599633436\n",
      "student train: Epo:372  Loss_stu:0.001276626886657947\n",
      "student train: Epo:373  Loss_stu:0.0012597768613075938\n",
      "student train: Epo:374  Loss_stu:0.0011940239409005235\n",
      "student train: Epo:375  Loss_stu:0.001197029643879183\n",
      "student train: Epo:376  Loss_stu:0.0011207091643713242\n",
      "student train: Epo:377  Loss_stu:0.00109372351310175\n",
      "student train: Epo:378  Loss_stu:0.0010611616730514326\n",
      "student train: Epo:379  Loss_stu:0.0010361576570018655\n",
      "student train: Epo:380  Loss_stu:0.0009866414739595707\n",
      "student train: Epo:381  Loss_stu:0.0009603452392269012\n",
      "student train: Epo:382  Loss_stu:0.0009291192583014193\n",
      "student train: Epo:383  Loss_stu:0.000900324573168815\n",
      "student train: Epo:384  Loss_stu:0.0008930956083139313\n",
      "student train: Epo:385  Loss_stu:0.0008752805944766791\n",
      "student train: Epo:386  Loss_stu:0.0008637001159635808\n",
      "student train: Epo:387  Loss_stu:0.000828300146886884\n",
      "student train: Epo:388  Loss_stu:0.0007997917124411351\n",
      "student train: Epo:389  Loss_stu:0.0008455398066486147\n",
      "student train: Epo:390  Loss_stu:0.000781240311173153\n",
      "student train: Epo:391  Loss_stu:0.0007719505277915933\n",
      "student train: Epo:392  Loss_stu:0.0007538554006035365\n",
      "student train: Epo:393  Loss_stu:0.0007382931227193247\n",
      "student train: Epo:394  Loss_stu:0.0007520347957117892\n",
      "student train: Epo:395  Loss_stu:0.0007183316553612263\n",
      "student train: Epo:396  Loss_stu:0.0007142807614097061\n",
      "student train: Epo:397  Loss_stu:0.0007125617827132985\n",
      "student train: Epo:398  Loss_stu:0.0007027889582258107\n",
      "student train: Epo:399  Loss_stu:0.0006847465207590601\n",
      "student train: Epo:400  Loss_stu:0.0006779224619583572\n",
      "student train: Epo:401  Loss_stu:0.0006658012624767588\n",
      "student train: Epo:402  Loss_stu:0.0006642301297255969\n",
      "student train: Epo:403  Loss_stu:0.0006601052708048897\n",
      "student train: Epo:404  Loss_stu:0.0006462654366179908\n",
      "student train: Epo:405  Loss_stu:0.0006475686248412473\n",
      "student train: Epo:406  Loss_stu:0.0006401532254405834\n",
      "student train: Epo:407  Loss_stu:0.0006245623395391843\n",
      "student train: Epo:408  Loss_stu:0.0006193595656314269\n",
      "student train: Epo:409  Loss_stu:0.0006240806738571586\n",
      "student train: Epo:410  Loss_stu:0.0006293391191388706\n",
      "student train: Epo:411  Loss_stu:0.0006191923669361911\n",
      "student train: Epo:412  Loss_stu:0.0006145285903846181\n",
      "student train: Epo:413  Loss_stu:0.0006071755872040359\n",
      "student train: Epo:414  Loss_stu:0.0006098191159685896\n",
      "student train: Epo:415  Loss_stu:0.0006007981784359352\n",
      "student train: Epo:416  Loss_stu:0.0005938297741259334\n",
      "student train: Epo:417  Loss_stu:0.0005970833182133128\n",
      "student train: Epo:418  Loss_stu:0.0005951164227532667\n",
      "student train: Epo:419  Loss_stu:0.0005986397775247734\n",
      "student train: Epo:420  Loss_stu:0.0005814014454419981\n",
      "student train: Epo:421  Loss_stu:0.0005934108199366851\n",
      "student train: Epo:422  Loss_stu:0.0005892738037697477\n",
      "student train: Epo:423  Loss_stu:0.0005888593306068423\n",
      "student train: Epo:424  Loss_stu:0.0005838627072193077\n",
      "student train: Epo:425  Loss_stu:0.0005761177957636871\n",
      "student train: Epo:426  Loss_stu:0.0005765855796095188\n",
      "student train: Epo:427  Loss_stu:0.0005793477522446329\n",
      "student train: Epo:428  Loss_stu:0.0005715176604246873\n",
      "student train: Epo:429  Loss_stu:0.0005722590224295181\n",
      "student train: Epo:430  Loss_stu:0.0005738333663785107\n",
      "student train: Epo:431  Loss_stu:0.0005709263541599139\n",
      "student train: Epo:432  Loss_stu:0.0005646033042732587\n",
      "student train: Epo:433  Loss_stu:0.0005627362164844715\n",
      "student train: Epo:434  Loss_stu:0.0005589758223978543\n",
      "student train: Epo:435  Loss_stu:0.0005572582023220229\n",
      "student train: Epo:436  Loss_stu:0.0005533134022952317\n",
      "student train: Epo:437  Loss_stu:0.0005492169588185469\n",
      "student train: Epo:438  Loss_stu:0.0005473668591875297\n",
      "student train: Epo:439  Loss_stu:0.0005399544517706221\n",
      "student train: Epo:440  Loss_stu:0.0005386635552061022\n",
      "student train: Epo:441  Loss_stu:0.0005354786489071782\n",
      "student train: Epo:442  Loss_stu:0.0005308207265309387\n",
      "student train: Epo:443  Loss_stu:0.0005267964011898688\n",
      "student train: Epo:444  Loss_stu:0.0005258771357606635\n",
      "student train: Epo:445  Loss_stu:0.0005246138636664895\n",
      "student train: Epo:446  Loss_stu:0.0005221353805317982\n",
      "student train: Epo:447  Loss_stu:0.0005202080594564378\n",
      "student train: Epo:448  Loss_stu:0.0005199003073771364\n",
      "student train: Epo:449  Loss_stu:0.0005186285315850548\n",
      "student train: Epo:450  Loss_stu:0.0005170089241669573\n",
      "student train: Epo:451  Loss_stu:0.0005169556441459879\n",
      "student train: Epo:452  Loss_stu:0.0005166888032323252\n",
      "student train: Epo:453  Loss_stu:0.0005158726558306332\n",
      "student train: Epo:454  Loss_stu:0.0005160825981376726\n",
      "student train: Epo:455  Loss_stu:0.0005157700641220635\n",
      "student train: Epo:456  Loss_stu:0.0005154269034411002\n",
      "student train: Epo:457  Loss_stu:0.0005154905845703852\n",
      "student train: Epo:458  Loss_stu:0.0005150074138567621\n",
      "student train: Epo:459  Loss_stu:0.0005146493578522918\n",
      "student train: Epo:460  Loss_stu:0.000514420159571844\n",
      "student train: Epo:461  Loss_stu:0.0005146052475331643\n",
      "student train: Epo:462  Loss_stu:0.0005145509945373482\n",
      "student train: Epo:463  Loss_stu:0.0005145947032828383\n",
      "student train: Epo:464  Loss_stu:0.000514266472027908\n",
      "student train: Epo:465  Loss_stu:0.0005144024083303934\n",
      "student train: Epo:466  Loss_stu:0.0005142284581046441\n",
      "student train: Epo:467  Loss_stu:0.0005145075398240689\n",
      "student train: Epo:468  Loss_stu:0.00051428676652676\n",
      "student train: Epo:469  Loss_stu:0.0005140514149377013\n",
      "student train: Epo:470  Loss_stu:0.0005202313803882144\n",
      "student train: Epo:471  Loss_stu:0.0005139039918393385\n",
      "student train: Epo:472  Loss_stu:0.0005142711601111779\n",
      "student train: Epo:473  Loss_stu:0.0005139351074673754\n",
      "student train: Epo:474  Loss_stu:0.0005140535878827018\n",
      "student train: Epo:475  Loss_stu:0.0005141100281122209\n",
      "student train: Epo:476  Loss_stu:0.0005142273925552222\n",
      "student train: Epo:477  Loss_stu:0.0005139956850987481\n",
      "student train: Epo:478  Loss_stu:0.0005139895632828601\n",
      "student train: Epo:479  Loss_stu:0.0005140045456311443\n",
      "student train: Epo:480  Loss_stu:0.0005140445916041087\n",
      "student train: Epo:481  Loss_stu:0.0005139259604711744\n",
      "student train: Epo:482  Loss_stu:0.0005140522674961547\n",
      "student train: Epo:483  Loss_stu:0.0005142242523485057\n",
      "student train: Epo:484  Loss_stu:0.0005139491102017364\n",
      "student train: Epo:485  Loss_stu:0.0005140849025126446\n",
      "student train: Epo:486  Loss_stu:0.0005139109126416218\n",
      "student train: Epo:487  Loss_stu:0.000514396806483437\n",
      "student train: Epo:488  Loss_stu:0.0005142757450326023\n",
      "student train: Epo:489  Loss_stu:0.0005138157258415042\n",
      "student train: Epo:490  Loss_stu:0.0005139889098602693\n",
      "student train: Epo:491  Loss_stu:0.0005141240387300444\n",
      "student train: Epo:492  Loss_stu:0.0005138603889724274\n",
      "student train: Epo:493  Loss_stu:0.0005137830118893345\n",
      "student train: Epo:494  Loss_stu:0.0005141375505640527\n",
      "student train: Epo:495  Loss_stu:0.0005137139530259832\n",
      "student train: Epo:496  Loss_stu:0.0005139267923709374\n",
      "student train: Epo:497  Loss_stu:0.0005140346649441621\n",
      "student train: Epo:498  Loss_stu:0.0005138451966624736\n",
      "student train: Epo:499  Loss_stu:0.0005141296549362066\n"
     ]
    }
   ],
   "source": [
    "#train student model\n",
    "loss_func=nn.MSELoss()\n",
    "print('start training')\n",
    "train_loss_all_stu=[]\n",
    "minloss=float('inf')\n",
    "for epoch in  range(500):\n",
    "    train_loss = 0\n",
    "    train_num=0\n",
    "    student_model.train()\n",
    "    for b_x,b_y in  student_train_loader1:\n",
    "        student_optimizer.zero_grad()\n",
    "        output=student_model(b_x.to(device))\n",
    "        loss=loss_func(output,b_y)\n",
    "        loss.backward()\n",
    "        student_optimizer.step()\n",
    "        train_loss+=loss.item() * b_x.size(0)\n",
    "        train_num += b_x.size(0)\n",
    "        loss_stu = train_loss/train_num\n",
    "        if loss_stu < minloss:\n",
    "            minloss = loss_stu\n",
    "            if os.path.exists(f'/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_8_gauss/stu/best.pth'):\n",
    "                os.remove(f'/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_8_gauss/stu/best.pth')\n",
    "            torch.save(student_model, f'/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_8_gauss/stu/best.pth')\n",
    "    # if epoch%1 == 0:\n",
    "    print(f'student train: Epo:{epoch}  Loss_stu:{loss_stu}')\n",
    "    train_loss_all_stu.append(loss_stu)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "import numpy as np\n",
    "b=np.array(train_loss_all_stu)\n",
    "\n",
    "np.save('/home/ysy/ysy/Fed-ReKD/california_our_e8_loss.npy',b)   # .npy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student test Loss:1.718182921409607\n"
     ]
    }
   ],
   "source": [
    "#eval  student model\n",
    "student_best_model =torch.load('/home/ysy/ysy/Fed-ReKD-dirs/BNN_MLP_8_gauss/stu/best.pth')\n",
    "\n",
    "for x,y in  student_test_loader:\n",
    "    student_best_model.eval().to(device)\n",
    "    pred=student_best_model(x)\n",
    "    loss_test=loss_func(pred,y) \n",
    "print(f'student test Loss:{loss_test}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('bnn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be5910e786292a3dd74c1ecc18ef09548ddc10597f54e3d847c6ad036c3e5961"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
