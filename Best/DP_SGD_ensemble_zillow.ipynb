{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79637, 81), (79637,), (8849, 81), (8849,), 0.4134, -0.3901, 0.4121, -0.3857)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    " \n",
    "process_dataset_x= pd.read_csv('/home/ysy/ysy/Fed-ReKD-dirs/process_dataset_copy_x.csv',index_col=0)\n",
    "process_dataset_y= pd.read_csv('/home/ysy/ysy/Fed-ReKD-dirs/process_dataset_copy_y.csv',index_col=0)\n",
    "x = process_dataset_x\n",
    "y = process_dataset_y['logerror']\n",
    "train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.1,random_state=42)\n",
    " \n",
    "scale=StandardScaler()\n",
    "train_x=scale.fit_transform(train_x)\n",
    "test_x=scale.fit_transform(test_x)\n",
    " \n",
    "del x,y\n",
    "train_x.shape,train_y.shape,test_x.shape,test_y.shape,max(train_y),min(train_y),max(test_y),min(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=np.array(train_x)\n",
    "train_y=np.array(train_y)\n",
    "test_x=np.array(test_x)\n",
    "test_y=np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "def split_list_n_list(origin_list, n):\n",
    "    if len(origin_list) % n == 0:\n",
    "        cnt = len(origin_list) // n\n",
    "    else:\n",
    "        cnt = len(origin_list) // n + 1\n",
    " \n",
    "    for i in range(0, n):\n",
    "        yield origin_list[i*cnt:(i+1)*cnt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (15928, 81) (15928,)\n",
      "1 (15928, 81) (15928,)\n",
      "2 (15928, 81) (15928,)\n",
      "3 (15928, 81) (15928,)\n",
      "4 (15925, 81) (15925,)\n",
      "0 (4425, 81) (4425,)\n",
      "1 (4424, 81) (4424,)\n"
     ]
    }
   ],
   "source": [
    "#preparing teacher's datasets\n",
    "n_teachers=5\n",
    "\n",
    "teacher_x,teacher_y = [],[]\n",
    "teacher_datasets = []\n",
    "teacher_data_loader = []\n",
    "\n",
    "\n",
    "\n",
    "teacher_x_loder = split_list_n_list(train_x,n_teachers)\n",
    "teacher_y_loder = split_list_n_list(train_y,n_teachers)\n",
    "\n",
    "teacher_x.extend(iter(teacher_x_loder))\n",
    "teacher_y.extend(iter(teacher_y_loder))\n",
    "\n",
    "for i in range(n_teachers):\n",
    "    print(i,teacher_x[i].shape,teacher_y[i].shape)\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "teacher_datasets.extend(TensorDataset(torch.tensor(teacher_x[u],device=device,dtype=torch.float),\n",
    "                                       torch.tensor(teacher_y[u],device=device,dtype=torch.float))\n",
    "                        for u in  range(n_teachers))\n",
    "\n",
    "teacher_data_loader.extend(DataLoader(teacher_datasets[i],batch_size=128,shuffle=False)\n",
    "                           for i in range(n_teachers))\n",
    "\n",
    "\n",
    "student_x,student_y = [] ,[]\n",
    "student_datasets= []\n",
    " \n",
    "\n",
    "student_x_loder =  split_list_n_list(test_x,2)\n",
    "student_y_loder =  split_list_n_list(test_y,2)\n",
    "student_x.extend(iter(student_x_loder))\n",
    "student_y.extend(iter(student_y_loder))\n",
    "\n",
    "for i in range(2):\n",
    "    print(i,student_x[i].shape,student_y[i].shape)\n",
    "\n",
    "student_datasets.extend(\n",
    "                        TensorDataset(torch.tensor(student_x[u],device=device,dtype=torch.float),\n",
    "                                      torch.tensor(student_y[u],device=device,dtype=torch.float))\n",
    "                        for u in range(2)\n",
    ")\n",
    "\n",
    "student_train_loader = DataLoader(student_datasets[0], batch_size=len(student_datasets[0]),shuffle=True)\n",
    "student_test_loader = DataLoader(student_datasets[1], batch_size=len(student_datasets[1]),shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    " \n",
    "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
    "\n",
    "train_x=np.array(train_x)\n",
    "train_y=np.array(train_y)\n",
    "test_x=np.array(test_x)\n",
    "test_y=np.array(test_y)\n",
    "\n",
    "\n",
    "train_dataset=TensorDataset(torch.tensor(train_x,dtype=torch.float),torch.tensor(train_y,dtype=torch.float))\n",
    "train_loader=DataLoader(train_dataset,num_workers=1, batch_size=128,shuffle=True)\n",
    "\n",
    "test_dataset=TensorDataset(torch.tensor(test_x,dtype=torch.float)\n",
    "                    ,torch.tensor(test_y,dtype=torch.float))\n",
    "test_loader=DataLoader(train_dataset,num_workers=1, batch_size=128,shuffle=True)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickletools import optimize\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden1 = nn.Sequential(nn.Linear(in_features=81, out_features=810, bias=True),\n",
    "                                  nn.Dropout(0.7),\n",
    "                                  nn.Tanh()\n",
    "                                  )\n",
    "        \n",
    "        self.hidden2 = nn.Sequential(nn.Linear(in_features=810, out_features=162, bias=True),\n",
    "                                  nn.Dropout(0.7),\n",
    "                                  nn.Tanh()\n",
    "                                  )\n",
    "        \n",
    "        self.hidden3 = nn.Sequential(nn.Linear(in_features=162, out_features=54, bias=True),\n",
    "                                  nn.Dropout(0.6),\n",
    "                                  nn.Tanh()\n",
    "                                  )\n",
    "        \n",
    "        self.predict = nn.Sequential(nn.Linear(in_features=54, out_features=1, bias=True),\n",
    "                                        nn.Tanh())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.hidden3(x)\n",
    "        output = self.predict(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "model=MLP().to(device)\n",
    "teachers_models = []\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "for i in range(n_teachers):\n",
    "    net = model\n",
    "    teachers_models.append(net)\n",
    " \n",
    "teacher_optimizers = [torch.optim.SGD(teachers_models[i].parameters(), lr=1e-3) for i in range(n_teachers)]\n",
    "#creating teachers folders\n",
    "def  mkdir_if_missing(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "for i in  range(n_teachers):\n",
    "    mkdir_if_missing(f'/home/ysy/ysy/Fed-ReKD-dirs/MLP_DP_SGD/teacher{i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "#training configs\n",
    "num_epochs=20  #10*3000epoch\n",
    "batch_size =128\n",
    "lr =1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training number 0 techer!\n",
      "Number:0 epoch: 1/20 \t loss_tea:0.02730928361415863\n",
      "Number:0 epoch: 2/20 \t loss_tea:0.019977986812591553\n",
      "Number:0 epoch: 3/20 \t loss_tea:0.014993282966315746\n",
      "Number:0 epoch: 4/20 \t loss_tea:0.016108613461256027\n",
      "Number:0 epoch: 5/20 \t loss_tea:0.023659877479076385\n",
      "Number:0 epoch: 6/20 \t loss_tea:0.018528813496232033\n",
      "Number:0 epoch: 7/20 \t loss_tea:0.014356362633407116\n",
      "Number:0 epoch: 8/20 \t loss_tea:0.017576448619365692\n",
      "Number:0 epoch: 9/20 \t loss_tea:0.011448530480265617\n",
      "Number:0 epoch: 10/20 \t loss_tea:0.019804460927844048\n",
      "Number:0 epoch: 11/20 \t loss_tea:0.018628835678100586\n",
      "Number:0 epoch: 12/20 \t loss_tea:0.023576494306325912\n",
      "Number:0 epoch: 13/20 \t loss_tea:0.014419329352676868\n",
      "Number:0 epoch: 14/20 \t loss_tea:0.013067604973912239\n",
      "Number:0 epoch: 15/20 \t loss_tea:0.01613209955394268\n",
      "Number:0 epoch: 16/20 \t loss_tea:0.01458014640957117\n",
      "Number:0 epoch: 17/20 \t loss_tea:0.0138615183532238\n",
      "Number:0 epoch: 18/20 \t loss_tea:0.014844628982245922\n",
      "Number:0 epoch: 19/20 \t loss_tea:0.010161791928112507\n",
      "Number:0 epoch: 20/20 \t loss_tea:0.01548598799854517\n",
      "finished training number 0 techer!\n",
      "start training number 1 techer!\n",
      "Number:1 epoch: 1/20 \t loss_tea:0.011989753693342209\n",
      "Number:1 epoch: 2/20 \t loss_tea:0.01755089871585369\n",
      "Number:1 epoch: 3/20 \t loss_tea:0.013581197708845139\n",
      "Number:1 epoch: 4/20 \t loss_tea:0.012096052058041096\n",
      "Number:1 epoch: 5/20 \t loss_tea:0.011594313196837902\n",
      "Number:1 epoch: 6/20 \t loss_tea:0.013936452567577362\n",
      "Number:1 epoch: 7/20 \t loss_tea:0.012710011564195156\n",
      "Number:1 epoch: 8/20 \t loss_tea:0.01248158235102892\n",
      "Number:1 epoch: 9/20 \t loss_tea:0.01427820511162281\n",
      "Number:1 epoch: 10/20 \t loss_tea:0.012667661532759666\n",
      "Number:1 epoch: 11/20 \t loss_tea:0.010973247699439526\n",
      "Number:1 epoch: 12/20 \t loss_tea:0.01206292025744915\n",
      "Number:1 epoch: 13/20 \t loss_tea:0.011545654386281967\n",
      "Number:1 epoch: 14/20 \t loss_tea:0.012199411168694496\n",
      "Number:1 epoch: 15/20 \t loss_tea:0.01197130512446165\n",
      "Number:1 epoch: 16/20 \t loss_tea:0.011818676255643368\n",
      "Number:1 epoch: 17/20 \t loss_tea:0.013114784844219685\n",
      "Number:1 epoch: 18/20 \t loss_tea:0.012809849344193935\n",
      "Number:1 epoch: 19/20 \t loss_tea:0.008522333577275276\n",
      "Number:1 epoch: 20/20 \t loss_tea:0.011754537001252174\n",
      "finished training number 1 techer!\n",
      "start training number 2 techer!\n",
      "Number:2 epoch: 1/20 \t loss_tea:0.017622707411646843\n",
      "Number:2 epoch: 2/20 \t loss_tea:0.0198845025151968\n",
      "Number:2 epoch: 3/20 \t loss_tea:0.017895661294460297\n",
      "Number:2 epoch: 4/20 \t loss_tea:0.019036084413528442\n",
      "Number:2 epoch: 5/20 \t loss_tea:0.014111954718828201\n",
      "Number:2 epoch: 6/20 \t loss_tea:0.019187236204743385\n",
      "Number:2 epoch: 7/20 \t loss_tea:0.014997772872447968\n",
      "Number:2 epoch: 8/20 \t loss_tea:0.019269604235887527\n",
      "Number:2 epoch: 9/20 \t loss_tea:0.014715264551341534\n",
      "Number:2 epoch: 10/20 \t loss_tea:0.02486596070230007\n",
      "Number:2 epoch: 11/20 \t loss_tea:0.0173068568110466\n",
      "Number:2 epoch: 12/20 \t loss_tea:0.018392246216535568\n",
      "Number:2 epoch: 13/20 \t loss_tea:0.01955174282193184\n",
      "Number:2 epoch: 14/20 \t loss_tea:0.017213808372616768\n",
      "Number:2 epoch: 15/20 \t loss_tea:0.021068966016173363\n",
      "Number:2 epoch: 16/20 \t loss_tea:0.017270373180508614\n",
      "Number:2 epoch: 17/20 \t loss_tea:0.016229651868343353\n",
      "Number:2 epoch: 18/20 \t loss_tea:0.014641881920397282\n",
      "Number:2 epoch: 19/20 \t loss_tea:0.01832805946469307\n",
      "Number:2 epoch: 20/20 \t loss_tea:0.012738775461912155\n",
      "finished training number 2 techer!\n",
      "start training number 3 techer!\n",
      "Number:3 epoch: 1/20 \t loss_tea:0.008585727773606777\n",
      "Number:3 epoch: 2/20 \t loss_tea:0.007836150005459785\n",
      "Number:3 epoch: 3/20 \t loss_tea:0.008394962176680565\n",
      "Number:3 epoch: 4/20 \t loss_tea:0.006627557333558798\n",
      "Number:3 epoch: 5/20 \t loss_tea:0.008816687390208244\n",
      "Number:3 epoch: 6/20 \t loss_tea:0.0077582248486578465\n",
      "Number:3 epoch: 7/20 \t loss_tea:0.007664764299988747\n",
      "Number:3 epoch: 8/20 \t loss_tea:0.005258011631667614\n",
      "Number:3 epoch: 9/20 \t loss_tea:0.006510979495942593\n",
      "Number:3 epoch: 10/20 \t loss_tea:0.009336021728813648\n",
      "Number:3 epoch: 11/20 \t loss_tea:0.007615360431373119\n",
      "Number:3 epoch: 12/20 \t loss_tea:0.006201011594384909\n",
      "Number:3 epoch: 13/20 \t loss_tea:0.008175776340067387\n",
      "Number:3 epoch: 14/20 \t loss_tea:0.008319658227264881\n",
      "Number:3 epoch: 15/20 \t loss_tea:0.006173949223011732\n",
      "Number:3 epoch: 16/20 \t loss_tea:0.006333921570330858\n",
      "Number:3 epoch: 17/20 \t loss_tea:0.00870258454233408\n",
      "Number:3 epoch: 18/20 \t loss_tea:0.007009351160377264\n",
      "Number:3 epoch: 19/20 \t loss_tea:0.007170882076025009\n",
      "Number:3 epoch: 20/20 \t loss_tea:0.007835661992430687\n",
      "finished training number 3 techer!\n",
      "start training number 4 techer!\n",
      "Number:4 epoch: 1/20 \t loss_tea:0.010324161499738693\n",
      "Number:4 epoch: 2/20 \t loss_tea:0.0119814807549119\n",
      "Number:4 epoch: 3/20 \t loss_tea:0.010299946181476116\n",
      "Number:4 epoch: 4/20 \t loss_tea:0.011276272125542164\n",
      "Number:4 epoch: 5/20 \t loss_tea:0.009907972067594528\n",
      "Number:4 epoch: 6/20 \t loss_tea:0.009809527546167374\n",
      "Number:4 epoch: 7/20 \t loss_tea:0.011291146278381348\n",
      "Number:4 epoch: 8/20 \t loss_tea:0.009219667874276638\n",
      "Number:4 epoch: 9/20 \t loss_tea:0.011456306092441082\n",
      "Number:4 epoch: 10/20 \t loss_tea:0.01104869693517685\n",
      "Number:4 epoch: 11/20 \t loss_tea:0.010408841073513031\n",
      "Number:4 epoch: 12/20 \t loss_tea:0.00944454688578844\n",
      "Number:4 epoch: 13/20 \t loss_tea:0.010004758834838867\n",
      "Number:4 epoch: 14/20 \t loss_tea:0.010239009745419025\n",
      "Number:4 epoch: 15/20 \t loss_tea:0.007883167825639248\n",
      "Number:4 epoch: 16/20 \t loss_tea:0.010175890289247036\n",
      "Number:4 epoch: 17/20 \t loss_tea:0.009313398040831089\n",
      "Number:4 epoch: 18/20 \t loss_tea:0.011093594133853912\n",
      "Number:4 epoch: 19/20 \t loss_tea:0.009467938914895058\n",
      "Number:4 epoch: 20/20 \t loss_tea:0.010207847692072392\n",
      "finished training number 4 techer!\n"
     ]
    }
   ],
   "source": [
    "#training teacher models\n",
    "loss_func=nn.MSELoss()\n",
    "train_loss_all=[]\n",
    "for tea_num in range(n_teachers):\n",
    "    print(f'start training number {tea_num} techer!')\n",
    "    minloss =float ('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        train_num=0\n",
    "        teachers_models[tea_num].train()\n",
    "        for b_x,b_y in teacher_data_loader[tea_num]:\n",
    "            teacher_optimizers[tea_num].zero_grad()\n",
    "            output=teachers_models[tea_num].forward(b_x.to(device))\n",
    "            output=output.squeeze(-1)\n",
    "            loss=loss_func(output,b_y.to(device))\n",
    "            loss.backward()\n",
    "            teacher_optimizers[tea_num].step()\n",
    "            if loss< minloss:\n",
    "                minloss = loss\n",
    "                if os.path.exists(f'/home/ysy/ysy/Fed-ReKD-dirs/MLP_DP_SGD/teacher{tea_num}/best.pth'):\n",
    "                    os.remove(f'/home/ysy/ysy/Fed-ReKD-dirs/MLP_DP_SGD/teacher{tea_num}/best.pth')\n",
    "                torch.save(teachers_models[tea_num], f'/home/ysy/ysy/Fed-ReKD-dirs/MLP_DP_SGD/teacher{tea_num}/best.pth')\n",
    "        if epoch%1== 0:\n",
    "            print(f'Number:{tea_num}','epoch: {}/{} \\t'.format(epoch+1,num_epochs),f'loss_tea:{loss}')\n",
    "        train_loss_all.append(loss)\n",
    "    print(f'finished training number {tea_num} techer!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('bnn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be5910e786292a3dd74c1ecc18ef09548ddc10597f54e3d847c6ad036c3e5961"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
